{"instruction": "What is the project name (a lot of people get it wrong)?\u00b6", "context": "sklearn", "output": "\nscikit-learn, but not scikit or SciKit nor sci-kit learn.\nAlso not scikits.learn or scikits-learn, which were previously used.\n\n\n"}
{"instruction": "How do you pronounce the project name?\u00b6", "context": "sklearn", "output": "\nsy-kit learn. sci stands for science!\n\n\n"}
{"instruction": "Why scikit?\u00b6", "context": "sklearn", "output": "\nThere are multiple scikits, which are scientific toolboxes built around SciPy.\nApart from scikit-learn, another popular one is [scikit-image](https://scikit-image.org/).\n\n\n"}
{"instruction": "Do you support PyPy?\u00b6", "context": "sklearn", "output": "\nscikit-learn is regularly tested and maintained to work with\n[PyPy](https://pypy.org/) (an alternative Python implementation with\na built-in just-in-time compiler).\n\n\nNote however that this support is still considered experimental and specific\ncomponents might behave slightly differently. Please refer to the test\nsuite of the specific module of interest for more details.\n\n\n"}
{"instruction": "Why is there no support for deep or reinforcement learning / Will there be support for deep or reinforcement learning in scikit-learn?\u00b6", "context": "sklearn", "output": "\nDeep learning and reinforcement learning both require a rich vocabulary to\ndefine an architecture, with deep learning additionally requiring\nGPUs for efficient computing. However, neither of these fit within\nthe design constraints of scikit-learn; as a result, deep learning\nand reinforcement learning are currently out of scope for what\nscikit-learn seeks to achieve.\n\n\nYou can find more information about addition of gpu support at\n[Will you add GPU support?](#will-you-add-gpu-support).\n\n\nNote that scikit-learn currently implements a simple multilayer perceptron\nin [`sklearn.neural\\_network`](modules/classes.html#module-sklearn.neural_network \"sklearn.neural_network\"). We will only accept bug fixes for this module.\nIf you want to implement more complex deep learning models, please turn to\npopular deep learning frameworks such as\n[tensorflow](https://www.tensorflow.org/),\n[keras](https://keras.io/)\nand [pytorch](https://pytorch.org/).\n\n\n"}
{"instruction": "Will you add graphical models or sequence prediction to scikit-learn?\u00b6", "context": "sklearn", "output": "\nNot in the foreseeable future.\nscikit-learn tries to provide a unified API for the basic tasks in machine\nlearning, with pipelines and meta-algorithms like grid search to tie\neverything together. The required concepts, APIs, algorithms and\nexpertise required for structured learning are different from what\nscikit-learn has to offer. If we started doing arbitrary structured\nlearning, we\u2019d need to redesign the whole package and the project\nwould likely collapse under its own weight.\n\n\nThere are two projects with API similar to scikit-learn that\ndo structured prediction:\n\n\n* [pystruct](https://pystruct.github.io/) handles general structured\nlearning (focuses on SSVMs on arbitrary graph structures with\napproximate inference; defines the notion of sample as an instance of\nthe graph structure)\n* [seqlearn](https://larsmans.github.io/seqlearn/) handles sequences only\n(focuses on exact inference; has HMMs, but mostly for the sake of\ncompleteness; treats a feature vector as a sample and uses an offset encoding\nfor the dependencies between feature vectors)\n\n\n"}
{"instruction": "Why did you remove HMMs from scikit-learn?\u00b6", "context": "sklearn", "output": "\nSee [Will you add graphical models or sequence prediction to scikit-learn?](#adding-graphical-models).\n\n\n"}
{"instruction": "Will you add GPU support?\u00b6", "context": "sklearn", "output": "\nNo, or at least not in the near future. The main reason is that GPU support\nwill introduce many software dependencies and introduce platform specific\nissues. scikit-learn is designed to be easy to install on a wide variety of\nplatforms. Outside of neural networks, GPUs don\u2019t play a large role in machine\nlearning today, and much larger gains in speed can often be achieved by a\ncareful choice of algorithms.\n\n\n"}
{"instruction": "Why do categorical variables need preprocessing in scikit-learn, compared to other tools?\u00b6", "context": "sklearn", "output": "\nMost of scikit-learn assumes data is in NumPy arrays or SciPy sparse matrices\nof a single numeric dtype. These do not explicitly represent categorical\nvariables at present. Thus, unlike R\u2019s data.frames or pandas.DataFrame, we\nrequire explicit conversion of categorical features to numeric values, as\ndiscussed in [Encoding categorical features](modules/preprocessing.html#preprocessing-categorical-features).\nSee also [Column Transformer with Mixed Types](auto_examples/compose/plot_column_transformer_mixed_types.html#sphx-glr-auto-examples-compose-plot-column-transformer-mixed-types-py) for an\nexample of working with heterogeneous (e.g. categorical and numeric) data.\n\n\n"}
{"instruction": "Why does Scikit-learn not directly work with, for example, pandas.DataFrame?\u00b6", "context": "sklearn", "output": "\nThe homogeneous NumPy and SciPy data objects currently expected are most\nefficient to process for most operations. Extensive work would also be needed\nto support Pandas categorical types. Restricting input to homogeneous\ntypes therefore reduces maintenance cost and encourages usage of efficient\ndata structures.\n\n\nNote however that [`ColumnTransformer`](modules/generated/sklearn.compose.ColumnTransformer.html#sklearn.compose.ColumnTransformer \"sklearn.compose.ColumnTransformer\") makes it\nconvenient to handle heterogeneous pandas dataframes by mapping homogeneous subsets of\ndataframe columns selected by name or dtype to dedicated scikit-learn transformers.\n\n\nTherefore [`ColumnTransformer`](modules/generated/sklearn.compose.ColumnTransformer.html#sklearn.compose.ColumnTransformer \"sklearn.compose.ColumnTransformer\") are often used in the first\nstep of scikit-learn pipelines when dealing\nwith heterogeneous dataframes (see [Pipeline: chaining estimators](modules/compose.html#pipeline) for more details).\n\n\nSee also [Column Transformer with Mixed Types](auto_examples/compose/plot_column_transformer_mixed_types.html#sphx-glr-auto-examples-compose-plot-column-transformer-mixed-types-py)\nfor an example of working with heterogeneous (e.g. categorical and numeric) data.\n\n\n"}
{"instruction": "Do you plan to implement transform for target y in a pipeline?\u00b6", "context": "sklearn", "output": "\nCurrently transform only works for features X in a pipeline.\nThere\u2019s a long-standing discussion about\nnot being able to transform y in a pipeline.\nFollow on github issue\n[#4143](https://github.com/scikit-learn/scikit-learn/issues/4143).\nMeanwhile check out\n[`TransformedTargetRegressor`](modules/generated/sklearn.compose.TransformedTargetRegressor.html#sklearn.compose.TransformedTargetRegressor \"sklearn.compose.TransformedTargetRegressor\"),\n[pipegraph](https://github.com/mcasl/PipeGraph),\n[imbalanced-learn](https://github.com/scikit-learn-contrib/imbalanced-learn).\nNote that Scikit-learn solved for the case where y\nhas an invertible transformation applied before training\nand inverted after prediction. Scikit-learn intends to solve for\nuse cases where y should be transformed at training time\nand not at test time, for resampling and similar uses,\nlike at `imbalanced-learn`.\nIn general, these use cases can be solved\nwith a custom meta estimator rather than a Pipeline\n\n\n"}
{"instruction": "Why are there so many different estimators for linear models?\u00b6", "context": "sklearn", "output": "\nUsually, there is one classifier and one regressor per model type, e.g.\n[`GradientBoostingClassifier`](modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier \"sklearn.ensemble.GradientBoostingClassifier\") and\n[`GradientBoostingRegressor`](modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor \"sklearn.ensemble.GradientBoostingRegressor\"). Both have similar options and\nboth have the parameter `loss`, which is especially useful in the regression\ncase as it enables the estimation of conditional mean as well as conditional\nquantiles.\n\n\nFor linear models, there are many estimator classes which are very close to\neach other. Let us have a look at\n\n\n* [`LinearRegression`](modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression \"sklearn.linear_model.LinearRegression\"), no penalty\n* [`Ridge`](modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge \"sklearn.linear_model.Ridge\"), L2 penalty\n* [`Lasso`](modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso \"sklearn.linear_model.Lasso\"), L1 penalty (sparse models)\n* [`ElasticNet`](modules/generated/sklearn.linear_model.ElasticNet.html#sklearn.linear_model.ElasticNet \"sklearn.linear_model.ElasticNet\"), L1 + L2 penalty (less sparse models)\n* [`SGDRegressor`](modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor \"sklearn.linear_model.SGDRegressor\") with `loss='squared\\_loss'`\n\n\n**Maintainer perspective:**\nThey all do in principle the same and are different only by the penalty they\nimpose. This, however, has a large impact on the way the underlying\noptimization problem is solved. In the end, this amounts to usage of different\nmethods and tricks from linear algebra. A special case is `SGDRegressor` which\ncomprises all 4 previous models and is different by the optimization procedure.\nA further side effect is that the different estimators favor different data\nlayouts (`X` c-contiguous or f-contiguous, sparse csr or csc). This complexity\nof the seemingly simple linear models is the reason for having different\nestimator classes for different penalties.\n\n\n**User perspective:**\nFirst, the current design is inspired by the scientific literature where linear\nregression models with different regularization/penalty were given different\nnames, e.g. *ridge regression*. Having different model classes with according\nnames makes it easier for users to find those regression models.\nSecondly, if all the 5 above mentioned linear models were unified into a single\nclass, there would be parameters with a lot of options like the `solver`\nparameter. On top of that, there would be a lot of exclusive interactions\nbetween different parameters. For example, the possible options of the\nparameters `solver`, `precompute` and `selection` would depend on the\nchosen values of the penalty parameters `alpha` and `l1\\_ratio`.\n\n\n"}
{"instruction": "How can I contribute to scikit-learn?\u00b6", "context": "sklearn", "output": "\nSee [Contributing](developers/contributing.html#contributing). Before wanting to add a new algorithm, which is\nusually a major and lengthy undertaking, it is recommended to start with\n[known issues](developers/contributing.html#new-contributors). Please do not contact the contributors\nof scikit-learn directly regarding contributing to scikit-learn.\n\n\n"}
{"instruction": "Why is my pull request not getting any attention?\u00b6", "context": "sklearn", "output": "\nThe scikit-learn review process takes a significant amount of time, and\ncontributors should not be discouraged by a lack of activity or review on\ntheir pull request. We care a lot about getting things right\nthe first time, as maintenance and later change comes at a high cost.\nWe rarely release any \u201cexperimental\u201d code, so all of our contributions\nwill be subject to high use immediately and should be of the highest\nquality possible initially.\n\n\nBeyond that, scikit-learn is limited in its reviewing bandwidth; many of the\nreviewers and core developers are working on scikit-learn on their own time.\nIf a review of your pull request comes slowly, it is likely because the\nreviewers are busy. We ask for your understanding and request that you\nnot close your pull request or discontinue your work solely because of\nthis reason.\n\n\n"}
{"instruction": "What are the inclusion criteria for new algorithms ?\u00b6", "context": "sklearn", "output": "\nWe only consider well-established algorithms for inclusion. A rule of thumb is\nat least 3 years since publication, 200+ citations, and wide use and\nusefulness. A technique that provides a clear-cut improvement (e.g. an\nenhanced data structure or a more efficient approximation technique) on\na widely-used method will also be considered for inclusion.\n\n\nFrom the algorithms or techniques that meet the above criteria, only those\nwhich fit well within the current API of scikit-learn, that is a `fit`,\n`predict/transform` interface and ordinarily having input/output that is a\nnumpy array or sparse matrix, are accepted.\n\n\nThe contributor should support the importance of the proposed addition with\nresearch papers and/or implementations in other similar packages, demonstrate\nits usefulness via common use-cases/applications and corroborate performance\nimprovements, if any, with benchmarks and/or plots. It is expected that the\nproposed algorithm should outperform the methods that are already implemented\nin scikit-learn at least in some areas.\n\n\nInclusion of a new algorithm speeding up an existing model is easier if:\n\n\n* it does not introduce new hyper-parameters (as it makes the library\nmore future-proof),\n* it is easy to document clearly when the contribution improves the speed\nand when it does not, for instance \u201cwhen n\\_features >>\nn\\_samples\u201d,\n* benchmarks clearly show a speed up.\n\n\nAlso, note that your implementation need not be in scikit-learn to be used\ntogether with scikit-learn tools. You can implement your favorite algorithm\nin a scikit-learn compatible way, upload it to GitHub and let us know. We\nwill be happy to list it under [Related Projects](related_projects.html#related-projects). If you already have\na package on GitHub following the scikit-learn API, you may also be\ninterested to look at [scikit-learn-contrib](https://scikit-learn-contrib.github.io).\n\n\n"}
{"instruction": "Why are you so selective on what algorithms you include in scikit-learn?\u00b6", "context": "sklearn", "output": "\nCode comes with maintenance cost, and we need to balance the amount of\ncode we have with the size of the team (and add to this the fact that\ncomplexity scales non linearly with the number of features).\nThe package relies on core developers using their free time to\nfix bugs, maintain code and review contributions.\nAny algorithm that is added needs future attention by the developers,\nat which point the original author might long have lost interest.\nSee also [What are the inclusion criteria for new algorithms ?](#new-algorithms-inclusion-criteria). For a great read about\nlong-term maintenance issues in open-source software, look at\n[the Executive Summary of Roads and Bridges](https://www.fordfoundation.org/media/2976/roads-and-bridges-the-unseen-labor-behind-our-digital-infrastructure.pdf#page=8)\n\n\n"}
{"instruction": "What\u2019s the best way to get help on scikit-learn usage?\u00b6", "context": "sklearn", "output": "\n**For general machine learning questions**, please use\n[Cross Validated](https://stats.stackexchange.com/) with the `[machine-learning]` tag.\n\n\n**For scikit-learn usage questions**, please use [Stack Overflow](https://stackoverflow.com/questions/tagged/scikit-learn)\nwith the `[scikit-learn]` and `[python]` tags. You can alternatively use the [mailing list](https://mail.python.org/mailman/listinfo/scikit-learn).\n\n\nPlease make sure to include a minimal reproduction code snippet (ideally shorter\nthan 10 lines) that highlights your problem on a toy dataset (for instance from\n`sklearn.datasets` or randomly generated with functions of `numpy.random` with\na fixed random seed). Please remove any line of code that is not necessary to\nreproduce your problem.\n\n\nThe problem should be reproducible by simply copy-pasting your code snippet in a Python\nshell with scikit-learn installed. Do not forget to include the import statements.\n\n\nMore guidance to write good reproduction code snippets can be found at:\n\n\n<https://stackoverflow.com/help/mcve>\n\n\nIf your problem raises an exception that you do not understand (even after googling it),\nplease make sure to include the full traceback that you obtain when running the\nreproduction script.\n\n\nFor bug reports or feature requests, please make use of the\n[issue tracker on GitHub](https://github.com/scikit-learn/scikit-learn/issues).\n\n\nThere is also a [scikit-learn Gitter channel](https://gitter.im/scikit-learn/scikit-learn) where some users and developers\nmight be found.\n\n\n**Please do not email any authors directly to ask for assistance, report bugs,\nor for any other issue related to scikit-learn.**\n\n\n"}
{"instruction": "How should I save, export or deploy estimators for production?\u00b6", "context": "sklearn", "output": "\nSee [Model persistence](model_persistence.html#model-persistence).\n\n\n"}
{"instruction": "How can I create a bunch object?\u00b6", "context": "sklearn", "output": "\nBunch objects are sometimes used as an output for functions and methods. They\nextend dictionaries by enabling values to be accessed by key,\n`bunch[\"value\\_key\"]`, or by an attribute, `bunch.value\\_key`.\n\n\nThey should not be used as an input; therefore you almost never need to create\na `Bunch` object, unless you are extending the scikit-learn\u2019s API.\n\n\n"}
{"instruction": "How can I load my own datasets into a format usable by scikit-learn?\u00b6", "context": "sklearn", "output": "\nGenerally, scikit-learn works on any numeric data stored as numpy arrays\nor scipy sparse matrices. Other types that are convertible to numeric\narrays such as pandas DataFrame are also acceptable.\n\n\nFor more information on loading your data files into these usable data\nstructures, please refer to [loading external datasets](datasets/loading_other_datasets.html#external-datasets).\n\n\n"}
{"instruction": "How do I deal with string data (or trees, graphs\u2026)?\u00b6", "context": "sklearn", "output": "\nscikit-learn estimators assume you\u2019ll feed them real-valued feature vectors.\nThis assumption is hard-coded in pretty much all of the library.\nHowever, you can feed non-numerical inputs to estimators in several ways.\n\n\nIf you have text documents, you can use a term frequency features; see\n[Text feature extraction](modules/feature_extraction.html#text-feature-extraction) for the built-in *text vectorizers*.\nFor more general feature extraction from any kind of data, see\n[Loading features from dicts](modules/feature_extraction.html#dict-feature-extraction) and [Feature hashing](modules/feature_extraction.html#feature-hashing).\n\n\nAnother common case is when you have non-numerical data and a custom distance\n(or similarity) metric on these data. Examples include strings with edit\ndistance (aka. Levenshtein distance; e.g., DNA or RNA sequences). These can be\nencoded as numbers, but doing so is painful and error-prone. Working with\ndistance metrics on arbitrary data can be done in two ways.\n\n\nFirstly, many estimators take precomputed distance/similarity matrices, so if\nthe dataset is not too large, you can compute distances for all pairs of inputs.\nIf the dataset is large, you can use feature vectors with only one \u201cfeature\u201d,\nwhich is an index into a separate data structure, and supply a custom metric\nfunction that looks up the actual data in this data structure. E.g., to use\nDBSCAN with Levenshtein distances:\n\n\n\n```\n>>> from leven import levenshtein       \n>>> import numpy as np\n>>> from sklearn.cluster import dbscan\n>>> data = [\"ACCTCCTAGAAG\", \"ACCTACTAGAAGTT\", \"GAATATTAGGCCGA\"]\n>>> def lev\\_metric(x, y):\n...     i, j = int(x[0]), int(y[0])     # extract indices\n...     return levenshtein(data[i], data[j])\n...\n>>> X = np.arange(len(data)).reshape(-1, 1)\n>>> X\narray([[0],\n [1],\n [2]])\n>>> # We need to specify algoritum='brute' as the default assumes\n>>> # a continuous feature space.\n>>> dbscan(X, metric=lev\\_metric, eps=5, min\\_samples=2, algorithm='brute')\n... \n([0, 1], array([ 0, 0, -1]))\n\n```\n\n\n(This uses the third-party edit distance package `leven`.)\n\n\nSimilar tricks can be used, with some care, for tree kernels, graph kernels,\netc.\n\n\n"}
{"instruction": "Why do I sometime get a crash/freeze with n_jobs > 1 under OSX or Linux?\u00b6", "context": "sklearn", "output": "\nSeveral scikit-learn tools such as `GridSearchCV` and `cross\\_val\\_score`\nrely internally on Python\u2019s `multiprocessing` module to parallelize execution\nonto several Python processes by passing `n\\_jobs > 1` as an argument.\n\n\nThe problem is that Python `multiprocessing` does a `fork` system call\nwithout following it with an `exec` system call for performance reasons. Many\nlibraries like (some versions of) Accelerate / vecLib under OSX, (some versions\nof) MKL, the OpenMP runtime of GCC, nvidia\u2019s Cuda (and probably many others),\nmanage their own internal thread pool. Upon a call to `fork`, the thread pool\nstate in the child process is corrupted: the thread pool believes it has many\nthreads while only the main thread state has been forked. It is possible to\nchange the libraries to make them detect when a fork happens and reinitialize\nthe thread pool in that case: we did that for OpenBLAS (merged upstream in\nmain since 0.2.10) and we contributed a [patch](https://gcc.gnu.org/bugzilla/show_bug.cgi?id=60035) to GCC\u2019s OpenMP runtime\n(not yet reviewed).\n\n\nBut in the end the real culprit is Python\u2019s `multiprocessing` that does\n`fork` without `exec` to reduce the overhead of starting and using new\nPython processes for parallel computing. Unfortunately this is a violation of\nthe POSIX standard and therefore some software editors like Apple refuse to\nconsider the lack of fork-safety in Accelerate / vecLib as a bug.\n\n\nIn Python 3.4+ it is now possible to configure `multiprocessing` to\nuse the \u2018forkserver\u2019 or \u2018spawn\u2019 start methods (instead of the default\n\u2018fork\u2019) to manage the process pools. To work around this issue when\nusing scikit-learn, you can set the `JOBLIB\\_START\\_METHOD` environment\nvariable to \u2018forkserver\u2019. However the user should be aware that using\nthe \u2018forkserver\u2019 method prevents joblib.Parallel to call function\ninteractively defined in a shell session.\n\n\nIf you have custom code that uses `multiprocessing` directly instead of using\nit via joblib you can enable the \u2018forkserver\u2019 mode globally for your\nprogram: Insert the following instructions in your main script:\n\n\n\n```\nimport multiprocessing\n\n# other imports, custom code, load data, define model...\n\nif \\_\\_name\\_\\_ == '\\_\\_main\\_\\_':\n    multiprocessing.set\\_start\\_method('forkserver')\n\n    # call scikit-learn utils with n\\_jobs > 1 here\n\n```\n\n\nYou can find more default on the new start methods in the [multiprocessing\ndocumentation](https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods).\n\n\n"}
{"instruction": "Why does my job use more cores than specified with n_jobs?\u00b6", "context": "sklearn", "output": "\nThis is because `n\\_jobs` only controls the number of jobs for\nroutines that are parallelized with `joblib`, but parallel code can come\nfrom other sources:\n\n\n* some routines may be parallelized with OpenMP (for code written in C or\nCython).\n* scikit-learn relies a lot on numpy, which in turn may rely on numerical\nlibraries like MKL, OpenBLAS or BLIS which can provide parallel\nimplementations.\n\n\nFor more details, please refer to our [Parallelism notes](computing/parallelism.html#parallelism).\n\n\n"}
{"instruction": "How do I set a random_state for an entire execution?\u00b6", "context": "sklearn", "output": "\nPlease refer to [Controlling randomness](common_pitfalls.html#randomness).\n\n\n"}
{"instruction": "I\u2019ve installed seaborn, why can\u2019t I import it?#", "context": "seaborn", "output": "\n*It looks like you successfully installed seaborn by doing* `pip install seaborn` *but it cannot be imported. You get an error like \u201cModuleNotFoundError: No module named \u2018seaborn\u2019\u201d when you try.*\n\n\nThis is probably not a `seaborn` problem, *per se*. If you have multiple Python environments on your computer, it is possible that you did `pip install` in one environment and tried to import the library in another. On a unix system, you could check whether the terminal commands `which pip`, `which python`, and (if applicable) `which jupyter` point to the same `bin/` directory. If not, you\u2019ll need to sort out the definition of your `$PATH` variable.\n\n\nTwo alternate patterns for installing with `pip` may also be more robust to this problem:\n\n\n* Invoke `pip` on the command line with `python -m pip install <package>` rather than `pip install <package>`\n* Use `%pip install <package>` in a Jupyter notebook to install it in the same place as the kernel\n\n\n"}
{"instruction": "I can\u2019t import seaborn, even though it\u2019s definitely installed!#", "context": "seaborn", "output": "\n*You\u2019ve definitely installed seaborn in the right place, but importing it produces a long traceback and a confusing error message, perhaps something like* `ImportError: DLL load failed: The specified module could not be found`.\n\n\nSuch errors usually indicate a problem with the way Python libraries are using compiled resources. Because seaborn is pure Python, it won\u2019t directly encounter these problems, but its dependencies (numpy, scipy, matplotlib, and pandas) might. To fix the issue, you\u2019ll first need to read through the traceback and figure out which dependency was being imported at the time of the error. Then consult the installation documentation for the relevant package, which might have advice for getting an installation working on your specific system.\n\n\nThe most common culprit of these issues is scipy, which has many compiled components. Starting in seaborn version 0.12, scipy is an optional dependency, which should help to reduce the frequency of these issues.\n\n\n"}
{"instruction": "Why aren\u2019t my plots showing up?#", "context": "seaborn", "output": "\n*You\u2019re calling seaborn functions \u2014\u00a0maybe in a terminal or IDE with an integrated IPython console \u2014 but not seeing any plots.)*\n\n\nIn matplotlib, there is a distinction between *creating* a figure and *showing* it, and in some cases it\u2019s necessary to explicitly call [`matplotlib.pyplot.show()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html#matplotlib.pyplot.show \"(in Matplotlib v3.6.2)\") at the point when you want to see the plot. Because that command blocks by default and is not always desired (for instance, you may be executing a script that saves files to disk) seaborn does not deviate from standard matplotlib practice here.\n\n\nYet most of the examples in the seaborn docs do not have this line, because there are multiple ways to avoid needing it. In a Jupyter notebook with the [\u201cinline\u201d](https://ipython.readthedocs.io/en/stable/interactive/plotting.html#id1) (default) or [\u201cwidget\u201d](https://github.com/matplotlib/ipympl) backends, [`matplotlib.pyplot.show()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html#matplotlib.pyplot.show \"(in Matplotlib v3.6.2)\") is automatically called after executing a cell, so any figures will appear in the cell\u2019s outputs. You can also activate a more interactive experience by executing `%matplotlib` in any Jupyter or IPython interface or by calling [`matplotlib.pyplot.ion()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.ion.html#matplotlib.pyplot.ion \"(in Matplotlib v3.6.2)\") anywhere in Python. Both methods will configure matplotlib to show or update the figure after every plotting command.\n\n\n"}
{"instruction": "Why is something printed after every notebook cell?#", "context": "seaborn", "output": "\n*You\u2019re using seaborn in a Jupyter notebook, and every cell prints something like <AxesSuplot:> or <seaborn.axisgrid.FacetGrid at 0x7f840e279c10> before showing the plot.*\n\n\nJupyter notebooks will show the result of the final statement in the cell as part of its output, and each of seaborn\u2019s plotting functions return a reference to the matplotlib or seaborn object that contain the plot. If this is bothersome, you can suppress this output in a few ways:\n\n\n* Always assign the result of the final statement to a variable (e.g. `ax = sns.histplot(...)`)\n* Add a semicolon to the end of the final statement (e.g. `sns.histplot(...);`)\n* End every cell with a function that has no return value (e.g. `plt.show()`, which isn\u2019t needed but also causes no problems)\n* Add [cell metadata tags](https://nbformat.readthedocs.io/en/latest/format_description.html#cell-metadata), if you\u2019re converting the notebook to a different representation\n\n\n"}
{"instruction": "Why do the plots look fuzzy in a Jupyter notebook?#", "context": "seaborn", "output": "\nThe default \u201cinline\u201d backend (defined by [IPython](https://github.com/ipython/matplotlib-inline)) uses an unusually low dpi ([\u201cdots per inch\u201d](https://en.wikipedia.org/wiki/Dots_per_inch)) for figure output. This is a space-saving measure: lower dpi figures take up less disk space. (Also, lower dpi inline graphics appear *physically* smaller because they are represented as [PNGs](https://en.wikipedia.org/wiki/Portable_Network_Graphics), which do not exactly have a concept of resolution.) So one faces an economy/quality tradeoff.\n\n\nYou can increase the DPI by resetting the rc parameters through the matplotlib API, using\n\n\n\n```\nplt.rcParams.update({\"figure.dpi\": 96})\n\n```\n\n\nOr do it as you activate the seaborn theme:\n\n\n\n```\nsns.set\\_theme(rc={\"figure.dpi\": 96})\n\n```\n\n\nIf you have a high pixel-density monitor, you can make your plots sharper using \u201cretina mode\u201d:\n\n\n\n```\n%config InlineBackend.figure\\_format = \"retina\"\n\n```\n\n\nThis won\u2019t change the apparent size of your plots in a Jupyter interface, but they might appear very large in other contexts (i.e. on GitHub). And they will take up 4x the disk space. Alternatively, you can make SVG plots:\n\n\n\n```\n%config InlineBackend.figure\\_format = \"svg\"\n\n```\n\n\nThis will configure matplotlib to emit [vector graphics](https://en.wikipedia.org/wiki/Vector_graphics) with \u201cinfinite resolution\u201d. The downside is that file size will now scale with the number and complexity of the artists in your plot, and in some cases (e.g., a large scatterplot matrix) the load will impact browser responsiveness.\n\n\n"}
{"instruction": "What do \u201cfigure-level\u201d and \u201caxes-level\u201d mean?#", "context": "seaborn", "output": "\n*You\u2019ve encountered the term \u201cfigure-level\u201d or \u201caxes-level\u201d, maybe in the seaborn docs, StackOverflow answer, or GitHub thread, but you don\u2019t understand what it means.*\n\n\nIn brief, all plotting functions in seaborn fall into one of two categories:\n\n\n* \u201caxes-level\u201d functions, which plot onto a single subplot that may or may not exist at the time the function is called\n* \u201cfigure-level\u201d functions, which internally create a matplotlib figure, potentially including multiple subplots\n\n\nThis design is intended to satisfy two objectives:\n\n\n* seaborn should offer functions that are \u201cdrop-in\u201d replacements for matplotlib methods\n* seaborn should be able to produce figures that show \u201cfacets\u201d or marginal distributions on distinct subplots\n\n\nThe figure-level functions always combine one or more axes-level functions with an object that manages the layout. So, for example, [`relplot()`](generated/seaborn.relplot.html#seaborn.relplot \"seaborn.relplot\") is a figure-level function that combines either [`scatterplot()`](generated/seaborn.scatterplot.html#seaborn.scatterplot \"seaborn.scatterplot\") or [`lineplot()`](generated/seaborn.lineplot.html#seaborn.lineplot \"seaborn.lineplot\") with a [`FacetGrid`](generated/seaborn.FacetGrid.html#seaborn.FacetGrid \"seaborn.FacetGrid\"). In contrast, [`jointplot()`](generated/seaborn.jointplot.html#seaborn.jointplot \"seaborn.jointplot\") is a figure-level function that can combine multiple different axes-level functions \u2014 [`scatterplot()`](generated/seaborn.scatterplot.html#seaborn.scatterplot \"seaborn.scatterplot\") and [`histplot()`](generated/seaborn.histplot.html#seaborn.histplot \"seaborn.histplot\") by default \u2014 with a [`JointGrid`](generated/seaborn.JointGrid.html#seaborn.JointGrid \"seaborn.JointGrid\").\n\n\nIf all you\u2019re doing is creating a plot with a single seaborn function call, this is not something you need to worry too much about. But it becomes relevant when you want to customize at a level beyond what the API of each function offers. It is also the source of various other points of confusion, so it is an important distinction understand (at least broadly) and keep in mind.\n\n\nThis is explained in more detail in the [tutorial](tutorial/function_overview.html) and in [this blog post](https://michaelwaskom.medium.com/three-common-seaborn-difficulties-10fdd0cc2a8b).\n\n\n"}
{"instruction": "What is a \u201ccategorical plot\u201d or \u201ccategorical function\u201d?#", "context": "seaborn", "output": "\nNext to the figure-level/axes-level distinction, this concept is probably the second biggest source of confusing behavior.\n\n\nSeveral [seaborn functions](api.html#categorical-api) are referred to as \u201ccategorical\u201d because they are designed to support a use-case where either the x or y variable in a plot is categorical (that is, the variable takes a finite number of potentially non-numeric values).\n\n\nAt the time these functions were written, matplotlib did not have any direct support for non-numeric data types. So seaborn internally builds a mapping from unique values in the data to 0-based integer indexes, which is what it passes to matplotlib. If your data are strings, that\u2019s great, and it more-or-less matches how [matplotlib now handles](https://matplotlib.org/stable/gallery/lines_bars_and_markers/categorical_variables.html) string-typed data.\n\n\nBut a potential gotcha is that these functions *always do this*, even if both the x and y variables are numeric. This gives rise to a number of confusing behaviors, especially when mixing categorical and non-categorical plots (e.g., a combo bar-and-line plot).\n\n\nThe v0.12 release added a `native\\_scale` parameter to [`stripplot()`](generated/seaborn.stripplot.html#seaborn.stripplot \"seaborn.stripplot\") and [`swarmplot()`](generated/seaborn.swarmplot.html#seaborn.swarmplot \"seaborn.swarmplot\"), which provides control over this behavior. It will be rolled out to other categorical functions in future releases. But the current behavior will almost certainly remain the default, so this is an important API wrinkle to understand.\n\n\n"}
{"instruction": "How does my data need to be organized?#", "context": "seaborn", "output": "\nTo get the most out of seaborn, your data should have a \u201clong-form\u201d or \u201ctidy\u201d representation. In a dataframe, [this means that](https://r4ds.had.co.nz/tidy-data.html#tidy-data) each variable has its own column, each observation has its own row, and each value has its own cell. With long-form data, you can succinctly and exactly specify a visualization by assigning variables in the dataset (columns) to roles in the plot.\n\n\nData organization is a common stumbling block for beginners, in part because data are often not collected or stored in a long-form representation. Therefore, it is often necessary to [reshape](https://pandas.pydata.org/pandas-docs/stable/user_guide/reshaping.html) the data using pandas before plotting. Data reshaping can be a complex undertaking, requiring both a solid grasp of dataframe structure and knowledge of the pandas API. Investing some time in developing this skill can pay large dividends.\n\n\nBut while seaborn is *most* powerful when provided with long-form data, nearly every seaborn function will accept and plot \u201cwide-form\u201d data too. You can trigger this by passing an object to seaborn\u2019s `data=` parameter without specifying other plot variables (`x`, `y`, \u2026). You\u2019ll be limited when using wide-form data: each function can make only one kind of wide-form plot. In most cases, seaborn tries to match what matplotlib or pandas would do with a dataset of the same structure. Reshaping your data into long-form will give you substantially more flexibility, but it can be helpful to take a quick look at your data very early in the process, and seaborn tries to make this possible.\n\n\nUnderstanding how your data should be represented \u2014 and how to get it that way if it starts out messy \u2014 is very important for making efficient and complete use of seaborn, and it is elaborated on at length in the [user-guide](tutorial/data_structure.html).\n\n\n"}
{"instruction": "Does seaborn only work with pandas?#", "context": "seaborn", "output": "\nGenerally speaking, no: seaborn is [quite flexible](https://seaborn.pydata.org/tutorial/data_structure.html#options-for-visualizing-long-form-data) about how your dataset needs to be represented.\n\n\nIn most cases, [long-form data](#faq-data-format) represented by multiple vector-like types can be passed directly to `x`, `y`, or other plotting parameters. Or you can pass a dictionary of vector types to `data` rather than a DataFrame. And when plotting with wide-form data, you can use a 2D numpy array or even nested lists to plot in wide-form mode.\n\n\nThere are a couple older functions (namely, [`catplot()`](generated/seaborn.catplot.html#seaborn.catplot \"seaborn.catplot\") and [`lmplot()`](generated/seaborn.lmplot.html#seaborn.lmplot \"seaborn.lmplot\")) that do require you to pass a [`pandas.DataFrame`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame \"(in pandas v1.5.2)\"). But at this point, they are the exception, and they will gain more flexibility over the next few release cycles.\n\n\n"}
{"instruction": "How do I change the figure size?#", "context": "seaborn", "output": "\nThis is going to be more complicated than you might hope, in part because there are multiple ways to change the figure size in matplotlib, and in part because of the [figure-level/axes-level](#faq-function-levels) distinction in seaborn.\n\n\nIn matplotlib, you can usually set the default size for all figures through the [rc parameters](https://matplotlib.org/stable/tutorials/introductory/customizing.html), specifically `figure.figsize`. And you can set the size of an individual figure when you create it (e.g. `plt.subplots(figsize=(w, h))`). If you\u2019re using an axes-level seaborn function, both of these will work as expected.\n\n\nFigure-level functions both ignore the default figure size and [parameterize the figure size differently](tutorial/function_overview.html#figure-size-tutorial). When calling a figure-level function, you can pass values to `height=` and `aspect=` to set (roughly) the size of each *subplot*. The advantage here is that the size of the figure automatically adapts when you add faceting variables. But it can be confusing.\n\n\nFortunately, there\u2019s a consistent way to set the exact figure size in a function-independent manner. Instead of setting the figure size when the figure is created, modify it after you plot by calling `obj.figure.set\\_size\\_inches(...)`, where `obj` is either a matplotlib axes (usually assigned to `ax`) or a seaborn `FacetGrid` (usually assigned to `g`).\n\n\nNote that `FacetGrid.figure` exists only on seaborn >= 0.11.2; before that you\u2019ll have to access `FacetGrid.fig`.\n\n\nAlso, if you\u2019re making pngs (or in a Jupyter notebook), you can \u2014 perhaps surprisingly \u2014 scale all your plots up or down by [changing the dpi](#faq-inline-dpi).\n\n\n"}
{"instruction": "Why isn\u2019t seaborn drawing the plot where I tell it to?#", "context": "seaborn", "output": "\n*You\u2019ve explicitly created a matplotlib figure with one or more subplots and tried to draw a seaborn plot on it, but you end up with an extra figure and a blank subplot. Perhaps your code looks something like*\n\n\n\n```\nf, ax = plt.subplots()\nsns.catplot(..., ax=ax)\n\n```\n\n\nThis is a [figure-level/axes-level](#faq-function-levels) gotcha. Figure-level functions always create their own figure, so you can\u2019t direct them towards an existing axes the way you can with axes-level functions. Most functions will warn you when this happens, suggest the appropriate axes-level function, and ignore the `ax=` parameter. A few older functions might put the plot where you want it (because they internally pass `ax` to their axes-level function) while still creating an extra figure. This latter behavior should be considered a bug, and it is not to be relied on.\n\n\nThe way things currently work, you can either set up the matplotlib figure yourself, or you can use a figure-level function, but you can\u2019t do both at the same time.\n\n\n"}
{"instruction": "Why can\u2019t I draw a line over a bar/box/strip/violin plot?#", "context": "seaborn", "output": "\n*You\u2019re trying to create a single plot using multiple seaborn functions, perhaps by drawing a lineplot or regplot over a barplot or violinplot. You expect the line to go through the mean value for each box (etc.), but it looks to be misalgined, or maybe it\u2019s all the way off to the side.*\n\n\nYou are trying to combine a [\u201ccategorical plot\u201d](#faq-categorical-plots) with another plot type. If your `x` variable has numeric values, it seems like this should work. But recall: seaborn\u2019s categorical plots map unique values on the categorical axis to integer indexes. So if your data have unique `x` values of 1, 6, 20, 94, the corresponding plot elements will get drawn at 0, 1, 2, 3 (and the tick labels will be changed to represent the actual value).\n\n\nThe line or regression plot doesn\u2019t know that this has happened, so it will use the actual numeric values, and the plots won\u2019t line up at all.\n\n\nAs of now, there are two ways to work around this. In situations where you want to draw a line, you could use the (somewhat misleadingly named) [`pointplot()`](generated/seaborn.pointplot.html#seaborn.pointplot \"seaborn.pointplot\") function, which is also a \u201ccategorical\u201d function and will use the same rules for drawing the plot. If this doesn\u2019t solve the problem (for one, it\u2019s not as visually flexible as [`lineplot()`](generated/seaborn.lineplot.html#seaborn.lineplot \"seaborn.lineplot\"), you could implement the mapping from actual values to integer indexes yourself and draw the plot that way:\n\n\n\n```\nunique\\_xs = sorted(df[\"x\"].unique())\nsns.violinplot(data=df, x=\"x\", y=\"y\")\nsns.lineplot(data=df, x=df[\"x\"].map(unique\\_xs.index), y=\"y\")\n\n```\n\n\nThis is something that will be easier in a planned future release, as it will become possible to make the categorical functions treat numeric data as numeric. (As of v0.12, it\u2019s possible only in [`stripplot()`](generated/seaborn.stripplot.html#seaborn.stripplot \"seaborn.stripplot\") and [`swarmplot()`](generated/seaborn.swarmplot.html#seaborn.swarmplot \"seaborn.swarmplot\"), using `native\\_scale=True`).\n\n\n"}
{"instruction": "How do I move the legend?#", "context": "seaborn", "output": "\n*When applying a semantic mapping to a plot, seaborn will automatically create a legend and add it to the figure. But the automatic choice of legend position is not always ideal.*\n\n\nWith seaborn v0.11.2 or later, use the [`move\\_legend()`](generated/seaborn.move_legend.html#seaborn.move_legend \"seaborn.move_legend\") function.\n\n\nOn older versions, a common pattern was to call `ax.legend(loc=...)` after plotting. While this appears to move the legend, it actually *replaces* it with a new one, using any labeled artists that happen to be attached to the axes. This does [not consistently work](https://github.com/mwaskom/seaborn/issues/2280) across plot types. And it does not propagate the legend title or positioning tweaks that are used to format a multi-variable legend.\n\n\nThe [`move\\_legend()`](generated/seaborn.move_legend.html#seaborn.move_legend \"seaborn.move_legend\") function is actually more powerful than its name suggests, and it can also be used to modify other [legend parameters](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.legend.html) (font size, handle length, etc.) after plotting.\n\n\n"}
{"instruction": "How can I can I change something about the figure?#", "context": "seaborn", "output": "\n*You want to make a very specific plot, and seaborn\u2019s defaults aren\u2019t doing it for you.*\n\n\nThere\u2019s basically a four-layer hierarchy to customizing a seaborn figure:\n\n\n1. Explicit seaborn function parameters\n2. Passed-through matplotlib keyword arguments\n3. Matplotlib axes methods\n4. Matplotlib artist methods\n\n\nFirst, read through the API docs for the relevant seaborn function. Each has a lot of parameters (probably too many), and you may be able to accomplish your desired customization using seaborn\u2019s own API.\n\n\nBut seaborn does delegate a lot of customization to matplotlib. Most functions have `\\*\\*kwargs` in their signature, which will catch extra keyword arguments and pass them through to the underlying matplotlib function. For example, [`scatterplot()`](generated/seaborn.scatterplot.html#seaborn.scatterplot \"seaborn.scatterplot\") has a number of parameters, but you can also use any valid keyword argument for [`matplotlib.axes.Axes.scatter()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.scatter.html#matplotlib.axes.Axes.scatter \"(in Matplotlib v3.6.2)\"), which it calls internally.\n\n\nPassing through keyword arguments lets you customize the artists that represent data, but often you will want to customize other aspects of the figure, such as labels, ticks, and titles. You can do this by calling methods on the object that seaborn\u2019s plotting functions return. Depending on whether you\u2019re calling an [axes-level or figure-level function](#faq-function-levels), this may be a [`matplotlib.axes.Axes`](https://matplotlib.org/stable/api/axes_api.html#matplotlib.axes.Axes \"(in Matplotlib v3.6.2)\") object or a seaborn wrapper (such as [`seaborn.FacetGrid`](generated/seaborn.FacetGrid.html#seaborn.FacetGrid \"seaborn.FacetGrid\")). Both kinds of objects have numerous methods that you can call to customize nearly anything about the figure. The easiest thing is usually to call [`matplotlib.axes.Axes.set()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.set.html#matplotlib.axes.Axes.set \"(in Matplotlib v3.6.2)\") or [`seaborn.FacetGrid.set()`](generated/seaborn.FacetGrid.set.html#seaborn.FacetGrid.set \"seaborn.FacetGrid.set\"), which let you modify multiple attributes at once, e.g.:\n\n\n\n```\nax = sns.scatterplot(...)\nax.set(\n    xlabel=\"The x label\",\n    ylabel=\"The y label\",\n    title=\"The title\"\n    xlim=(xmin, xmax),\n    xticks=[...],\n    xticklabels=[...],\n)\n\n```\n\n\nFinally, the deepest customization may require you to reach \u201cinto\u201d the matplotlib axes and tweak the artists that are stored on it. These will be in artist lists, such as `ax.lines`, `ax.collections`, `ax.patches`, etc.\n\n\n*Warning:* Neither matplotlib nor seaborn consider the specific artists produced by their plotting functions to be part of stable API. Because it\u2019s not possible to gracefully warn about upcoming changes to the artist types or the order in which they are stored, code that interacts with these attributes could break unexpectedly. With that said, seaborn does try hard to avoid making this kind of change.\n\n\n"}
{"instruction": "Wait, I need to learn how to use matplotlib too?#", "context": "seaborn", "output": "\nIt really depends on how much customization you need. You can certainly perform a lot of exploratory data analysis while primarily or exclusively interacting with the seaborn API. But, if you\u2019re polishing a figure for a presentation or publication, you\u2019ll likely find yourself needing to understand at least a little bit about how matplotlib works. Matplotlib is extremely flexible, and it lets you control literally everything about a figure if you drill down far enough.\n\n\nSeaborn was originally designed with the idea that it would handle a specific set of well-defined operations through a very high-level API, while letting users \u201cdrop down\u201d to matplotlib when they desired additional customization. This can be a pretty powerful combination, and it works reasonably well if you already know how to use matplotlib. But as seaborn as gained more features, it has become more feasible to learn seaborn *first*. In that situation, the need to switch APIs tends to be a bit more confusing / frustrating. This has motivated the development of seaborn\u2019s new [objects interface](tutorial/objects_interface.html), which aims to provide a more cohesive API for both high-level and low-level figure specification. Hopefully, it will alleviate the \u201ctwo-library problem\u201d as it matures.\n\n\nWith that said, the level of deep control that matplotlib affords really can\u2019t be beat, so if you care about doing very specific things, it really is worth learning.\n\n\n"}
{"instruction": "How do I use seaborn with matplotlib\u2019s object-oriented interface?#", "context": "seaborn", "output": "\n*You prefer to use matplotlib\u2019s explicit or* [\u201cobject-oriented\u201d](https://matplotlib.org/stable/users/explain/api_interfaces.html) *interface, because it makes your code easier to reason about and maintain. But the object-orient interface consists of methods on matplotlib objects, whereas seaborn offers you independent functions.*\n\n\nThis is another case where it will be helpful to keep the [figure-level/axes-level](#faq-function-levels) distinction in mind.\n\n\nAxes-level functions can be used like any matplotlib axes method, but instead of calling `ax.func(...)`, you call `func(..., ax=ax)`. They also return the axes object (which they may have created, if no figure was currently active in matplotlib\u2019s global state). You can use the methods on that object to further customize the plot even if you didn\u2019t start with [`matplotlib.pyplot.figure()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.figure.html#matplotlib.pyplot.figure \"(in Matplotlib v3.6.2)\") or [`matplotlib.pyplot.subplots()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html#matplotlib.pyplot.subplots \"(in Matplotlib v3.6.2)\"):\n\n\n\n```\nax = sns.histplot(...)\nax.set(...)\n\n```\n\n\nFigure-level functions [can\u2019t be directed towards an existing figure](#faq-plot-misplaced), but they do store the matplotlib objects on the [`FacetGrid`](generated/seaborn.FacetGrid.html#seaborn.FacetGrid \"seaborn.FacetGrid\") object that they return (which seaborn docs always assign to a variable named `g`).\n\n\nIf your figure-level function created only one subplot, you can access it directly:\n\n\n\n```\ng = sns.displot(...)\ng.ax.set(...)\n\n```\n\n\nFor multiple subplots, you can either use `FacetGrid.axes` (which is always a 2D array of axes) or `FacetGrid.axes\\_dict` (which maps the row/col keys to the corresponding matplotlib object):\n\n\n\n```\ng = sns.displot(..., col=...)\nfor col, ax in g.axes\\_dict.items():\n    ax.set(...)\n\n```\n\n\nBut if you\u2019re batch-setting attributes on all subplots, use the [`FacetGrid.set()`](generated/seaborn.FacetGrid.set.html#seaborn.FacetGrid.set \"seaborn.FacetGrid.set\") method rather than iterating over the individual axes:\n\n\n\n```\ng = sns.displot(...)\ng.set(...)\n\n```\n\n\nTo access the underlying matplotlib *figure*, use `FacetGrid.figure` on seaborn >= 0.11.2 (or `FacetGrid.fig` on any other version).\n\n\n"}
{"instruction": "Can I annotate bar plots with the bar values?#", "context": "seaborn", "output": "\nNothing like this is built into seaborn, but matplotlib v3.4.0 added a convenience function ([`matplotlib.axes.Axes.bar\\_label()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.bar_label.html#matplotlib.axes.Axes.bar_label \"(in Matplotlib v3.6.2)\")) that makes it relatively easy. Here are a couple of recipes; note that you\u2019ll need to use a different approach depending on whether your bars come from a [figure-level or axes-level function](#faq-function-levels):\n\n\n\n```\n# Axes-level\nax = sns.histplot(df, x=\"x\\_var\")\nfor bars in ax.containers:\n    ax.bar\\_label(bars)\n\n# Figure-level, one subplot\ng = sns.displot(df, x=\"x\\_var\")\nfor bars in g.ax.containers:\n    g.ax.bar\\_label(bars)\n\n# Figure-level, multiple subplots\ng = sns.displot(df, x=\"x\\_var\", col=\"col\\_var)\nfor ax in g.axes.flat:\n    for bars in ax.containers:\n        ax.bar\\_label(bars)\n\n```\n\n\n"}
{"instruction": "Can I use seaborn in dark mode?#", "context": "seaborn", "output": "\nThere\u2019s no direct support for this in seaborn, but matplotlib has a [\u201cdark\\_background\u201d](https://matplotlib.org/stable/gallery/style_sheets/dark_background.html) style-sheet that you could use, e.g.:\n\n\n\n```\nsns.set\\_theme(style=\"ticks\", rc=plt.style.library[\"dark\\_background\"])\n\n```\n\n\nNote that \u201cdark\\_background\u201d changes the default color palette to \u201cSet2\u201d, and that will override any palette you define in [`set\\_theme()`](generated/seaborn.set_theme.html#seaborn.set_theme \"seaborn.set_theme\"). If you\u2019d rather use a different color palette, you\u2019ll have to call `sns.set\\_palette()` separately. The default [seaborn palette](tutorial/color_palettes.html) (\u201cdeep\u201d) has poor contrast against a dark background, so you\u2019d be better off using \u201cmuted\u201d, \u201cbright\u201d, or \u201cpastel\u201d.\n\n\n"}
{"instruction": "Can I access the results of seaborn\u2019s statistical transformations?#", "context": "seaborn", "output": "\nBecause seaborn performs some statistical operations as it builds plots (aggregating, bootstrapping, fitting regression models), some users would like access to the statistics that it computes. This is not possible: it\u2019s explicitly considered out of scope for seaborn (a visualization library) to offer an API for interrogating statistical models.\n\n\nIf you simply want to be diligent and verify that seaborn is doing things correctly (or that it matches your own code), it\u2019s open-source, so feel free to read the code. Or, because it\u2019s Python, you can call into the private methods that calculate the stats (just don\u2019t do this in production code). But don\u2019t expect seaborn to offer features that are more at home in [scipy](https://scipy.org/) or [statsmodels](https://www.statsmodels.org/).\n\n\n"}
{"instruction": "Can I show standard error instead of a confidence interval?#", "context": "seaborn", "output": "\nAs of v0.12, this is possible in most places, using the new `errorbar` API (see the [tutorial](tutorial/error_bars.html) for more details).\n\n\n"}
{"instruction": "Why does the y axis for a KDE plot go above 1?#", "context": "seaborn", "output": "\n*You\u2019ve estimated a probability distribution for your data using* [`kdeplot()`](generated/seaborn.kdeplot.html#seaborn.kdeplot \"seaborn.kdeplot\"), *but the y axis goes above 1. Aren\u2019t probabilities bounded by 1? Is this a bug?*\n\n\nThis is not a bug, but it is a common confusion (about kernel density plots and probability distributions more broadly). A continuous probability distribution is defined by a [probability density function](https://en.wikipedia.org/wiki/Probability_density_function), which [`kdeplot()`](generated/seaborn.kdeplot.html#seaborn.kdeplot \"seaborn.kdeplot\") estimates. The probability density function does **not** output *a probability*: a continuous random variable can take an infinite number of values, so the probability of observing any *specific* value is infinitely small. You can only talk meaningfully about the probability of observing a value that falls within some *range*. The probability of observing a value that falls within the complete range of possible values is 1. Likewise, the probability density function is normalized so that the area under it (that is, the integral of the function across its domain) equals 1. If the range of likely values is small, the curve will have to go above 1 to make this possible.\n\n\n"}
{"instruction": "Why is seaborn imported as sns?#", "context": "seaborn", "output": "\nThis is an obscure reference to the [namesake](https://pbs.twimg.com/media/C3C6q1ZUYAALXX0.jpg) of the library, but you can also think of it as \u201cseaborn name space\u201d.\n\n\n"}
{"instruction": "Why is ggplot so much better than seaborn?#", "context": "seaborn", "output": "\nGood question. Probably because you get to use the word \u201cgeom\u201d a lot, and it\u2019s fun to say. \u201cGeom\u201d. \u201cGeeeeeooom\u201d.\n\n\n"}
{"instruction": "My model reports \u201ccuda runtime error(2): out of memory\u201d\u00b6", "context": "pytorch", "output": "\nAs the error message suggests, you have run out of memory on your\nGPU. Since we often deal with large amounts of data in PyTorch,\nsmall mistakes can rapidly cause your program to use up all of your\nGPU; fortunately, the fixes in these cases are often simple.\nHere are a few common things to check:\n\n\n**Don\u2019t accumulate history across your training loop.**\nBy default, computations involving variables that require gradients\nwill keep history. This means that you should avoid using such\nvariables in computations which will live beyond your training loops,\ne.g., when tracking statistics. Instead, you should detach the variable\nor access its underlying data.\n\n\nSometimes, it can be non-obvious when differentiable variables can\noccur. Consider the following training loop (abridged from [source](https://discuss.pytorch.org/t/high-memory-usage-while-training/162)):\n\n\n\n```\ntotal\\_loss = 0\nfor i in range(10000):\n    optimizer.zero\\_grad()\n    output = model(input)\n    loss = criterion(output)\n    loss.backward()\n    optimizer.step()\n    total\\_loss += loss\n\n```\n\n\nHere, `total\\_loss` is accumulating history across your training loop, since\n`loss` is a differentiable variable with autograd history. You can fix this by\nwriting total\\_loss += float(loss) instead.\n\n\nOther instances of this problem:\n[1](https://discuss.pytorch.org/t/resolved-gpu-out-of-memory-error-with-batch-size-1/3719).\n\n\n**Don\u2019t hold onto tensors and variables you don\u2019t need.**\nIf you assign a Tensor or Variable to a local, Python will not\ndeallocate until the local goes out of scope. You can free\nthis reference by using `del x`. Similarly, if you assign\na Tensor or Variable to a member variable of an object, it will\nnot deallocate until the object goes out of scope. You will\nget the best memory usage if you don\u2019t hold onto temporaries\nyou don\u2019t need.\n\n\nThe scopes of locals can be larger than you expect. For example:\n\n\n\n```\nfor i in range(5):\n    intermediate = f(input[i])\n    result += g(intermediate)\noutput = h(result)\nreturn output\n\n```\n\n\nHere, `intermediate` remains live even while `h` is executing,\nbecause its scope extrudes past the end of the loop. To free it\nearlier, you should `del intermediate` when you are done with it.\n\n\n**Avoid running RNNs on sequences that are too large.**\nThe amount of memory required to backpropagate through an RNN scales\nlinearly with the length of the RNN input; thus, you will run out of memory\nif you try to feed an RNN a sequence that is too long.\n\n\nThe technical term for this phenomenon is [backpropagation through time](https://en.wikipedia.org/wiki/Backpropagation_through_time),\nand there are plenty of references for how to implement truncated\nBPTT, including in the [word language model](https://github.com/pytorch/examples/tree/master/word_language_model) example; truncation is handled by the\n`repackage` function as described in\n[this forum post](https://discuss.pytorch.org/t/help-clarifying-repackage-hidden-in-word-language-model/226).\n\n\n**Don\u2019t use linear layers that are too large.**\nA linear layer `nn.Linear(m, n)` uses O(nm)O(nm)O(nm) memory: that is to say,\nthe memory requirements of the weights\nscales quadratically with the number of features. It is very easy\nto [blow through your memory](https://github.com/pytorch/pytorch/issues/958)\nthis way (and remember that you will need at least twice the size of the\nweights, since you also need to store the gradients.)\n\n\n**Consider checkpointing.**\nYou can trade-off memory for compute by using [checkpoint](https://pytorch.org/docs/stable/checkpoint.html).\n\n\n"}
{"instruction": "My GPU memory isn\u2019t freed properly\u00b6", "context": "pytorch", "output": "\nPyTorch uses a caching memory allocator to speed up memory allocations. As a\nresult, the values shown in `nvidia-smi` usually don\u2019t reflect the true\nmemory usage. See [Memory management](cuda.html#cuda-memory-management) for more details about GPU\nmemory management.\n\n\nIf your GPU memory isn\u2019t freed even after Python quits, it is very likely that\nsome Python subprocesses are still alive. You may find them via\n`ps -elf | grep python` and manually kill them with `kill -9 [pid]`.\n\n\n"}
{"instruction": "My out of memory exception handler can\u2019t allocate memory\u00b6", "context": "pytorch", "output": "\nYou may have some code that tries to recover from out of memory errors.\n\n\n\n```\ntry:\n    run\\_model(batch\\_size)\nexcept RuntimeError: # Out of memory\n    for \\_ in range(batch\\_size):\n        run\\_model(1)\n\n```\n\n\nBut find that when you do run out of memory, your recovery code can\u2019t allocate\neither. That\u2019s because the python exception object holds a reference to the\nstack frame where the error was raised. Which prevents the original tensor\nobjects from being freed. The solution is to move you OOM recovery code outside\nof the `except` clause.\n\n\n\n```\noom = False\ntry:\n    run\\_model(batch\\_size)\nexcept RuntimeError: # Out of memory\n    oom = True\n\nif oom:\n    for \\_ in range(batch\\_size):\n        run\\_model(1)\n\n```\n\n\n"}
{"instruction": "My data loader workers return identical random numbers\u00b6", "context": "pytorch", "output": "\nYou are likely using other libraries to generate random numbers in the dataset\nand worker subprocesses are started via `fork`. See\n[`torch.utils.data.DataLoader`](../data.html#torch.utils.data.DataLoader \"torch.utils.data.DataLoader\")\u2019s documentation for how to\nproperly set up random seeds in workers with its `worker\\_init\\_fn` option.\n\n\n"}
{"instruction": "My recurrent network doesn\u2019t work with data parallelism\u00b6", "context": "pytorch", "output": "\nThere is a subtlety in using the\n`pack sequence -> recurrent network -> unpack sequence` pattern in a\n[`Module`](../generated/torch.nn.Module.html#torch.nn.Module \"torch.nn.Module\") with [`DataParallel`](../generated/torch.nn.DataParallel.html#torch.nn.DataParallel \"torch.nn.DataParallel\") or\n[`data\\_parallel()`](../generated/torch.nn.functional.torch.nn.parallel.data_parallel.html#torch.nn.parallel.data_parallel \"torch.nn.parallel.data_parallel\"). Input to each the `forward()` on\neach device will only be part of the entire input. Because the unpack operation\n[`torch.nn.utils.rnn.pad\\_packed\\_sequence()`](../generated/torch.nn.utils.rnn.pad_packed_sequence.html#torch.nn.utils.rnn.pad_packed_sequence \"torch.nn.utils.rnn.pad_packed_sequence\") by default only pads up to the\nlongest input it sees, i.e., the longest on that particular device, size\nmismatches will happen when results are gathered together. Therefore, you can\ninstead take advantage of the `total\\_length` argument of\n[`pad\\_packed\\_sequence()`](../generated/torch.nn.utils.rnn.pad_packed_sequence.html#torch.nn.utils.rnn.pad_packed_sequence \"torch.nn.utils.rnn.pad_packed_sequence\") to make sure that the\n`forward()` calls return sequences of same length. For example, you can\nwrite:\n\n\n\n```\nfrom torch.nn.utils.rnn import pack\\_padded\\_sequence, pad\\_packed\\_sequence\n\nclass MyModule(nn.Module):\n    # ... \\_\\_init\\_\\_, other methods, etc.\n\n    # padded\\_input is of shape [B x T x \\*] (batch\\_first mode) and contains\n    # the sequences sorted by lengths\n    # B is the batch size\n    # T is max sequence length\n    def forward(self, padded\\_input, input\\_lengths):\n        total\\_length = padded\\_input.size(1)  # get the max sequence length\n        packed\\_input = pack\\_padded\\_sequence(padded\\_input, input\\_lengths,\n                                            batch\\_first=True)\n        packed\\_output, \\_ = self.my\\_lstm(packed\\_input)\n        output, \\_ = pad\\_packed\\_sequence(packed\\_output, batch\\_first=True,\n                                        total\\_length=total\\_length)\n        return output\n\n\nm = MyModule().cuda()\ndp\\_m = nn.DataParallel(m)\n\n```\n\n\nAdditionally, extra care needs to be taken when batch dimension is dim `1`\n(i.e., `batch\\_first=False`) with data parallelism. In this case, the first\nargument of pack\\_padded\\_sequence `padding\\_input` will be of shape\n`[T x B x \\*]` and should be scattered along dim `1`, but the second argument\n`input\\_lengths` will be of shape `[B]` and should be scattered along dim\n`0`. Extra code to manipulate the tensor shapes will be needed.\n\n\n"}
{"instruction": "Docs", "context": "pytorch", "output": "\nAccess comprehensive developer documentation for PyTorch\n\n\n[View Docs](https://pytorch.org/docs/stable/index.html)\n"}
{"instruction": "Tutorials", "context": "pytorch", "output": "\nGet in-depth tutorials for beginners and advanced developers\n\n\n[View Tutorials](https://pytorch.org/tutorials)\n"}
{"instruction": "Resources", "context": "pytorch", "output": "\nFind development resources and get your questions answered\n\n\n[View Resources](https://pytorch.org/resources)\n"}
{"instruction": "How does Scrapy compare to BeautifulSoup or lxml?\u00b6", "context": "scrapy", "output": "\n[BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) and [lxml](https://lxml.de/) are libraries for parsing HTML and XML. Scrapy is\nan application framework for writing web spiders that crawl web sites and\nextract data from them.\n\n\nScrapy provides a built-in mechanism for extracting data (called\n[selectors](topics/selectors.html#topics-selectors)) but you can easily use [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/)\n(or [lxml](https://lxml.de/)) instead, if you feel more comfortable working with them. After\nall, they\u2019re just parsing libraries which can be imported and used from any\nPython code.\n\n\nIn other words, comparing [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) (or [lxml](https://lxml.de/)) to Scrapy is like\ncomparing [jinja2](https://palletsprojects.com/p/jinja/) to [Django](https://www.djangoproject.com/).\n\n\n"}
{"instruction": "Can I use Scrapy with BeautifulSoup?\u00b6", "context": "scrapy", "output": "\nYes, you can.\nAs mentioned [above](#faq-scrapy-bs-cmp), [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) can be used\nfor parsing HTML responses in Scrapy callbacks.\nYou just have to feed the response\u2019s body into a `BeautifulSoup` object\nand extract whatever data you need from it.\n\n\nHere\u2019s an example spider using BeautifulSoup API, with `lxml` as the HTML parser:\n\n\n\n```\nfrom bs4 import BeautifulSoup\nimport scrapy\n\n\nclass ExampleSpider(scrapy.Spider):\n    name = \"example\"\n    allowed\\_domains = [\"example.com\"]\n    start\\_urls = (\"http://www.example.com/\",)\n\n    def parse(self, response):\n        # use lxml to get decent HTML parsing speed\n        soup = BeautifulSoup(response.text, \"lxml\")\n        yield {\"url\": response.url, \"title\": soup.h1.string}\n\n```\n\n\n\nNote\n\n\n`BeautifulSoup` supports several HTML/XML parsers.\nSee [BeautifulSoup\u2019s official documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#specifying-the-parser-to-use) on which ones are available.\n\n\n\n"}
{"instruction": "Did Scrapy \u201csteal\u201d X from Django?\u00b6", "context": "scrapy", "output": "\nProbably, but we don\u2019t like that word. We think [Django](https://www.djangoproject.com/) is a great open source\nproject and an example to follow, so we\u2019ve used it as an inspiration for\nScrapy.\n\n\nWe believe that, if something is already done well, there\u2019s no need to reinvent\nit. This concept, besides being one of the foundations for open source and free\nsoftware, not only applies to software but also to documentation, procedures,\npolicies, etc. So, instead of going through each problem ourselves, we choose\nto copy ideas from those projects that have already solved them properly, and\nfocus on the real problems we need to solve.\n\n\nWe\u2019d be proud if Scrapy serves as an inspiration for other projects. Feel free\nto steal from us!\n\n\n"}
{"instruction": "Does Scrapy work with HTTP proxies?\u00b6", "context": "scrapy", "output": "\nYes. Support for HTTP proxies is provided (since Scrapy 0.8) through the HTTP\nProxy downloader middleware. See\n[`HttpProxyMiddleware`](topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware \"scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware\").\n\n\n"}
{"instruction": "How can I scrape an item with attributes in different pages?\u00b6", "context": "scrapy", "output": "\nSee [Passing additional data to callback functions](topics/request-response.html#topics-request-response-ref-request-callback-arguments).\n\n\n"}
{"instruction": "How can I simulate a user login in my spider?\u00b6", "context": "scrapy", "output": "\nSee [Using FormRequest.from\\_response() to simulate a user login](topics/request-response.html#topics-request-response-ref-request-userlogin).\n\n\n"}
{"instruction": "Does Scrapy crawl in breadth-first or depth-first order?\u00b6", "context": "scrapy", "output": "\nBy default, Scrapy uses a [LIFO](https://en.wikipedia.org/wiki/Stack_(abstract_data_type)) queue for storing pending requests, which\nbasically means that it crawls in [DFO order](https://en.wikipedia.org/wiki/Depth-first_search). This order is more convenient\nin most cases.\n\n\nIf you do want to crawl in true [BFO order](https://en.wikipedia.org/wiki/Breadth-first_search), you can do it by\nsetting the following settings:\n\n\n\n```\nDEPTH\\_PRIORITY = 1\nSCHEDULER\\_DISK\\_QUEUE = \"scrapy.squeues.PickleFifoDiskQueue\"\nSCHEDULER\\_MEMORY\\_QUEUE = \"scrapy.squeues.FifoMemoryQueue\"\n\n```\n\n\nWhile pending requests are below the configured values of\n[`CONCURRENT\\_REQUESTS`](topics/settings.html#std-setting-CONCURRENT_REQUESTS), [`CONCURRENT\\_REQUESTS\\_PER\\_DOMAIN`](topics/settings.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN) or\n[`CONCURRENT\\_REQUESTS\\_PER\\_IP`](topics/settings.html#std-setting-CONCURRENT_REQUESTS_PER_IP), those requests are sent\nconcurrently. As a result, the first few requests of a crawl rarely follow the\ndesired order. Lowering those settings to `1` enforces the desired order, but\nit significantly slows down the crawl as a whole.\n\n\n"}
{"instruction": "My Scrapy crawler has memory leaks. What can I do?\u00b6", "context": "scrapy", "output": "\nSee [Debugging memory leaks](topics/leaks.html#topics-leaks).\n\n\nAlso, Python has a builtin memory leak issue which is described in\n[Leaks without leaks](topics/leaks.html#topics-leaks-without-leaks).\n\n\n"}
{"instruction": "How can I make Scrapy consume less memory?\u00b6", "context": "scrapy", "output": "\nSee previous question.\n\n\n"}
{"instruction": "How can I prevent memory errors due to many allowed domains?\u00b6", "context": "scrapy", "output": "\nIf you have a spider with a long list of\n[`allowed\\_domains`](topics/spiders.html#scrapy.Spider.allowed_domains \"scrapy.Spider.allowed_domains\") (e.g. 50,000+), consider\nreplacing the default\n[`OffsiteMiddleware`](topics/spider-middleware.html#scrapy.spidermiddlewares.offsite.OffsiteMiddleware \"scrapy.spidermiddlewares.offsite.OffsiteMiddleware\") spider middleware\nwith a [custom spider middleware](topics/spider-middleware.html#custom-spider-middleware) that requires\nless memory. For example:\n\n\n* If your domain names are similar enough, use your own regular expression\ninstead joining the strings in\n[`allowed\\_domains`](topics/spiders.html#scrapy.Spider.allowed_domains \"scrapy.Spider.allowed_domains\") into a complex regular\nexpression.\n* If you can [meet the installation requirements](https://github.com/andreasvc/pyre2#installation), use [pyre2](https://github.com/andreasvc/pyre2) instead of\nPython\u2019s [re](https://docs.python.org/library/re.html) to compile your URL-filtering regular expression. See\n[issue 1908](https://github.com/scrapy/scrapy/issues/1908).\n\n\nSee also other suggestions at [StackOverflow](https://stackoverflow.com/q/36440681/939364).\n\n\n\nNote\n\n\nRemember to disable\n[`scrapy.spidermiddlewares.offsite.OffsiteMiddleware`](topics/spider-middleware.html#scrapy.spidermiddlewares.offsite.OffsiteMiddleware \"scrapy.spidermiddlewares.offsite.OffsiteMiddleware\") when you enable\nyour custom implementation:\n\n\n\n```\nSPIDER\\_MIDDLEWARES = {\n    \"scrapy.spidermiddlewares.offsite.OffsiteMiddleware\": None,\n    \"myproject.middlewares.CustomOffsiteMiddleware\": 500,\n}\n\n```\n\n\n\n"}
{"instruction": "Can I use Basic HTTP Authentication in my spiders?\u00b6", "context": "scrapy", "output": "\nYes, see [`HttpAuthMiddleware`](topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware \"scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware\").\n\n\n"}
{"instruction": "Why does Scrapy download pages in English instead of my native language?\u00b6", "context": "scrapy", "output": "\nTry changing the default [Accept-Language](https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.4) request header by overriding the\n[`DEFAULT\\_REQUEST\\_HEADERS`](topics/settings.html#std-setting-DEFAULT_REQUEST_HEADERS) setting.\n\n\n"}
{"instruction": "Where can I find some example Scrapy projects?\u00b6", "context": "scrapy", "output": "\nSee [Examples](intro/examples.html#intro-examples).\n\n\n"}
{"instruction": "Can I run a spider without creating a project?\u00b6", "context": "scrapy", "output": "\nYes. You can use the [`runspider`](topics/commands.html#std-command-runspider) command. For example, if you have a\nspider written in a `my\\_spider.py` file you can run it with:\n\n\n\n```\nscrapy runspider my\\_spider.py\n\n```\n\n\nSee [`runspider`](topics/commands.html#std-command-runspider) command for more info.\n\n\n"}
{"instruction": "I get \u201cFiltered offsite request\u201d messages. How can I fix them?\u00b6", "context": "scrapy", "output": "\nThose messages (logged with `DEBUG` level) don\u2019t necessarily mean there is a\nproblem, so you may not need to fix them.\n\n\nThose messages are thrown by the Offsite Spider Middleware, which is a spider\nmiddleware (enabled by default) whose purpose is to filter out requests to\ndomains outside the ones covered by the spider.\n\n\nFor more info see:\n[`OffsiteMiddleware`](topics/spider-middleware.html#scrapy.spidermiddlewares.offsite.OffsiteMiddleware \"scrapy.spidermiddlewares.offsite.OffsiteMiddleware\").\n\n\n"}
{"instruction": "What is the recommended way to deploy a Scrapy crawler in production?\u00b6", "context": "scrapy", "output": "\nSee [Deploying Spiders](topics/deploy.html#topics-deploy).\n\n\n"}
{"instruction": "Can I use JSON for large exports?\u00b6", "context": "scrapy", "output": "\nIt\u2019ll depend on how large your output is. See [this warning](topics/exporters.html#json-with-large-data) in [`JsonItemExporter`](topics/exporters.html#scrapy.exporters.JsonItemExporter \"scrapy.exporters.JsonItemExporter\")\ndocumentation.\n\n\n"}
{"instruction": "Can I return (Twisted) deferreds from signal handlers?\u00b6", "context": "scrapy", "output": "\nSome signals support returning deferreds from their handlers, others don\u2019t. See\nthe [Built-in signals reference](topics/signals.html#topics-signals-ref) to know which ones.\n\n\n"}
{"instruction": "What does the response status code 999 mean?\u00b6", "context": "scrapy", "output": "\n999 is a custom response status code used by Yahoo sites to throttle requests.\nTry slowing down the crawling speed by using a download delay of `2` (or\nhigher) in your spider:\n\n\n\n```\nfrom scrapy.spiders import CrawlSpider\n\n\nclass MySpider(CrawlSpider):\n    name = \"myspider\"\n\n    download\\_delay = 2\n\n    # [ ... rest of the spider code ... ]\n\n```\n\n\nOr by setting a global download delay in your project with the\n[`DOWNLOAD\\_DELAY`](topics/settings.html#std-setting-DOWNLOAD_DELAY) setting.\n\n\n"}
{"instruction": "Can I call pdb.set_trace() from my spiders to debug them?\u00b6", "context": "scrapy", "output": "\nYes, but you can also use the Scrapy shell which allows you to quickly analyze\n(and even modify) the response being processed by your spider, which is, quite\noften, more useful than plain old `pdb.set\\_trace()`.\n\n\nFor more info see [Invoking the shell from spiders to inspect responses](topics/shell.html#topics-shell-inspect-response).\n\n\n"}
{"instruction": "Simplest way to dump all my scraped items into a JSON/CSV/XML file?\u00b6", "context": "scrapy", "output": "\nTo dump into a JSON file:\n\n\n\n```\nscrapy crawl myspider -O items.json\n\n```\n\n\nTo dump into a CSV file:\n\n\n\n```\nscrapy crawl myspider -O items.csv\n\n```\n\n\nTo dump into a XML file:\n\n\n\n```\nscrapy crawl myspider -O items.xml\n\n```\n\n\nFor more information see [Feed exports](topics/feed-exports.html#topics-feed-exports)\n\n\n"}
{"instruction": "What\u2019s this huge cryptic __VIEWSTATE parameter used in some forms?\u00b6", "context": "scrapy", "output": "\nThe `\\_\\_VIEWSTATE` parameter is used in sites built with ASP.NET/VB.NET. For\nmore info on how it works see [this page](https://metacpan.org/pod/release/ECARROLL/HTML-TreeBuilderX-ASP_NET-0.09/lib/HTML/TreeBuilderX/ASP_NET.pm). Also, here\u2019s an [example spider](https://github.com/AmbientLighter/rpn-fas/blob/master/fas/spiders/rnp.py)\nwhich scrapes one of these sites.\n\n\n"}
{"instruction": "What\u2019s the best way to parse big XML/CSV data feeds?\u00b6", "context": "scrapy", "output": "\nParsing big feeds with XPath selectors can be problematic since they need to\nbuild the DOM of the entire feed in memory, and this can be quite slow and\nconsume a lot of memory.\n\n\nIn order to avoid parsing all the entire feed at once in memory, you can use\nthe functions `xmliter` and `csviter` from `scrapy.utils.iterators`\nmodule. In fact, this is what the feed spiders (see [Spiders](topics/spiders.html#topics-spiders)) use\nunder the cover.\n\n\n"}
{"instruction": "Does Scrapy manage cookies automatically?\u00b6", "context": "scrapy", "output": "\nYes, Scrapy receives and keeps track of cookies sent by servers, and sends them\nback on subsequent requests, like any regular web browser does.\n\n\nFor more info see [Requests and Responses](topics/request-response.html#topics-request-response) and [CookiesMiddleware](topics/downloader-middleware.html#cookies-mw).\n\n\n"}
{"instruction": "How can I see the cookies being sent and received from Scrapy?\u00b6", "context": "scrapy", "output": "\nEnable the [`COOKIES\\_DEBUG`](topics/downloader-middleware.html#std-setting-COOKIES_DEBUG) setting.\n\n\n"}
{"instruction": "How can I instruct a spider to stop itself?\u00b6", "context": "scrapy", "output": "\nRaise the [`CloseSpider`](topics/exceptions.html#scrapy.exceptions.CloseSpider \"scrapy.exceptions.CloseSpider\") exception from a callback. For\nmore info see: [`CloseSpider`](topics/exceptions.html#scrapy.exceptions.CloseSpider \"scrapy.exceptions.CloseSpider\").\n\n\n"}
{"instruction": "How can I prevent my Scrapy bot from getting banned?\u00b6", "context": "scrapy", "output": "\nSee [Avoiding getting banned](topics/practices.html#bans).\n\n\n"}
{"instruction": "Should I use spider arguments or settings to configure my spider?\u00b6", "context": "scrapy", "output": "\nBoth [spider arguments](topics/spiders.html#spiderargs) and [settings](topics/settings.html#topics-settings)\ncan be used to configure your spider. There is no strict rule that mandates to\nuse one or the other, but settings are more suited for parameters that, once\nset, don\u2019t change much, while spider arguments are meant to change more often,\neven on each spider run and sometimes are required for the spider to run at all\n(for example, to set the start url of a spider).\n\n\nTo illustrate with an example, assuming you have a spider that needs to log\ninto a site to scrape data, and you only want to scrape data from a certain\nsection of the site (which varies each time). In that case, the credentials to\nlog in would be settings, while the url of the section to scrape would be a\nspider argument.\n\n\n"}
{"instruction": "I\u2019m scraping a XML document and my XPath selector doesn\u2019t return any items\u00b6", "context": "scrapy", "output": "\nYou may need to remove namespaces. See [Removing namespaces](topics/selectors.html#removing-namespaces).\n\n\n"}
{"instruction": "How to split an item into multiple items in an item pipeline?\u00b6", "context": "scrapy", "output": "\n[Item pipelines](topics/item-pipeline.html#topics-item-pipeline) cannot yield multiple items per\ninput item. [Create a spider middleware](topics/spider-middleware.html#custom-spider-middleware)\ninstead, and use its\n[`process\\_spider\\_output()`](topics/spider-middleware.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output \"scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output\")\nmethod for this purpose. For example:\n\n\n\n```\nfrom copy import deepcopy\n\nfrom itemadapter import is\\_item, ItemAdapter\n\n\nclass MultiplyItemsMiddleware:\n    def process\\_spider\\_output(self, response, result, spider):\n        for item in result:\n            if is\\_item(item):\n                adapter = ItemAdapter(item)\n                for \\_ in range(adapter[\"multiply\\_by\"]):\n                    yield deepcopy(item)\n\n```\n\n\n"}
{"instruction": "Does Scrapy support IPv6 addresses?\u00b6", "context": "scrapy", "output": "\nYes, by setting [`DNS\\_RESOLVER`](topics/settings.html#std-setting-DNS_RESOLVER) to `scrapy.resolver.CachingHostnameResolver`.\nNote that by doing so, you lose the ability to set a specific timeout for DNS requests\n(the value of the [`DNS\\_TIMEOUT`](topics/settings.html#std-setting-DNS_TIMEOUT) setting is ignored).\n\n\n"}
{"instruction": "How to deal with <class 'ValueError'>: filedescriptor out of range in select() exceptions?\u00b6", "context": "scrapy", "output": "\nThis issue [has been reported](https://github.com/scrapy/scrapy/issues/2905) to appear when running broad crawls in macOS, where the default\nTwisted reactor is [`twisted.internet.selectreactor.SelectReactor`](https://docs.twisted.org/en/stable/api/twisted.internet.selectreactor.SelectReactor.html \"(in Twisted)\"). Switching to a\ndifferent reactor is possible by using the [`TWISTED\\_REACTOR`](topics/settings.html#std-setting-TWISTED_REACTOR) setting.\n\n\n"}
{"instruction": "How can I cancel the download of a given response?\u00b6", "context": "scrapy", "output": "\nIn some situations, it might be useful to stop the download of a certain response.\nFor instance, sometimes you can determine whether or not you need the full contents\nof a response by inspecting its headers or the first bytes of its body. In that case,\nyou could save resources by attaching a handler to the [`bytes\\_received`](topics/signals.html#scrapy.signals.bytes_received \"scrapy.signals.bytes_received\")\nor [`headers\\_received`](topics/signals.html#scrapy.signals.headers_received \"scrapy.signals.headers_received\") signals and raising a\n[`StopDownload`](topics/exceptions.html#scrapy.exceptions.StopDownload \"scrapy.exceptions.StopDownload\") exception. Please refer to the\n[Stopping the download of a Response](topics/request-response.html#topics-stop-response-download) topic for additional information and examples.\n\n\n"}
{"instruction": "Running runspider I get error: No spider found in file: <filename>\u00b6", "context": "scrapy", "output": "\nThis may happen if your Scrapy project has a spider module with a name that\nconflicts with the name of one of the [Python standard library modules](https://docs.python.org/py-modindex.html), such\nas `csv.py` or `os.py`, or any [Python package](https://pypi.org/) that you have installed.\nSee [issue 2680](https://github.com/scrapy/scrapy/issues/2680).\n\n\n"}
{"instruction": "1. Where do I find more details about LightGBM parameters?\uf0c1", "context": "lightgbm", "output": "\nTake a look at [Parameters](./Parameters.html) and the [Laurae++/Parameters](https://sites.google.com/view/lauraepp/parameters) website.\n\n\n"}
{"instruction": "2. On datasets with millions of features, training does not start (or starts after a very long time).\uf0c1", "context": "lightgbm", "output": "\nUse a smaller value for `bin\\_construct\\_sample\\_cnt` and a larger value for `min\\_data`.\n\n\n"}
{"instruction": "3. When running LightGBM on a large dataset, my computer runs out of RAM.\uf0c1", "context": "lightgbm", "output": "\n**Multiple Solutions**: set the `histogram\\_pool\\_size` parameter to the MB you want to use for LightGBM (histogram\\_pool\\_size + dataset size = approximately RAM used),\nlower `num\\_leaves` or lower `max\\_bin` (see [Microsoft/LightGBM#562](https://github.com/microsoft/LightGBM/issues/562)).\n\n\n"}
{"instruction": "4. I am using Windows. Should I use Visual Studio or MinGW for compiling LightGBM?\uf0c1", "context": "lightgbm", "output": "\nVisual Studio [performs best for LightGBM](https://github.com/microsoft/LightGBM/issues/542).\n\n\n"}
{"instruction": "5. When using LightGBM GPU, I cannot reproduce results over several runs.\uf0c1", "context": "lightgbm", "output": "\nThis is normal and expected behaviour, but you may try to use `gpu\\_use\\_dp = true` for reproducibility\n(see [Microsoft/LightGBM#560](https://github.com/microsoft/LightGBM/pull/560#issuecomment-304561654)).\nYou may also use the CPU version.\n\n\n"}
{"instruction": "6. Bagging is not reproducible when changing the number of threads.\uf0c1", "context": "lightgbm", "output": "\n\nLightGBM bagging is multithreaded, so its output depends on the number of threads used.\nThere is [no workaround currently](https://github.com/microsoft/LightGBM/issues/632).\n\n\n\nStarting from [#2804](https://github.com/microsoft/LightGBM/pull/2804) bagging result doesn\u2019t depend on the number of threads.\nSo this issue should be solved in the latest version.\n\n\n"}
{"instruction": "7. I tried to use Random Forest mode, and LightGBM crashes!\uf0c1", "context": "lightgbm", "output": "\nThis is expected behaviour for arbitrary parameters. To enable Random Forest,\nyou must use `bagging\\_fraction` and `feature\\_fraction` different from 1, along with a `bagging\\_freq`.\n[This thread](https://github.com/microsoft/LightGBM/issues/691) includes an example.\n\n\n"}
{"instruction": "8. CPU usage is low (like 10%) in Windows when using LightGBM on very large datasets with many-core systems.\uf0c1", "context": "lightgbm", "output": "\nPlease use [Visual Studio](https://visualstudio.microsoft.com/downloads/)\nas it may be [10x faster than MinGW](https://github.com/microsoft/LightGBM/issues/749) especially for very large trees.\n\n\n"}
{"instruction": "9. When I\u2019m trying to specify a categorical column with the categorical_feature parameter, I get the following sequence of warnings, but there are no negative values in the column.\uf0c1", "context": "lightgbm", "output": "\n\n```\n[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n[LightGBM] [Warning] There are no meaningful features, as all feature values are constant.\n\n```\n\n\nThe column you\u2019re trying to pass via `categorical\\_feature` likely contains very large values.\nCategorical features in LightGBM are limited by int32 range,\nso you cannot pass values that are greater than `Int32.MaxValue` (2147483647) as categorical features (see [Microsoft/LightGBM#1359](https://github.com/microsoft/LightGBM/issues/1359)).\nYou should convert them to integers ranging from zero to the number of categories first.\n\n\n"}
{"instruction": "10. LightGBM crashes randomly with the error like: Initializing libiomp5.dylib, but found libomp.dylib already initialized.\uf0c1", "context": "lightgbm", "output": "\n\n```\nOMP: Error #15: Initializing libiomp5.dylib, but found libomp.dylib already initialized.\nOMP: Hint: This means that multiple copies of the OpenMP runtime have been linked into the program. That is dangerous, since it can degrade performance or cause incorrect results. The best thing to do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding static linking of the OpenMP runtime in any library. As an unsafe, unsupported, undocumented workaround you can set the environment variable KMP\\_DUPLICATE\\_LIB\\_OK=TRUE to allow the program to continue to execute, but that may cause crashes or silently produce incorrect results. For more information, please see http://www.intel.com/software/products/support/.\n\n```\n\n\n**Possible Cause**: This error means that you have multiple OpenMP libraries installed on your machine and they conflict with each other.\n(File extensions in the error message may differ depending on the operating system).\n\n\nIf you are using Python distributed by Conda, then it is highly likely that the error is caused by the `numpy` package from Conda which includes the `mkl` package which in turn conflicts with the system-wide library.\nIn this case you can update the `numpy` package in Conda or replace the Conda\u2019s OpenMP library instance with system-wide one by creating a symlink to it in Conda environment folder `$CONDA\\_PREFIX/lib`.\n\n\n**Solution**: Assuming you are using macOS with Homebrew, the command which overwrites OpenMP library files in the current active Conda environment with symlinks to the system-wide library ones installed by Homebrew:\n\n\n\n```\nln -sf `ls -d \"$(brew --cellar libomp)\"/*/lib`/* $CONDA\\_PREFIX/lib\n\n```\n\n\nThe described above fix worked fine before the release of OpenMP 8.0.0 version.\nStarting from 8.0.0 version, Homebrew formula for OpenMP includes `-DLIBOMP\\_INSTALL\\_ALIASES=OFF` option which leads to that the fix doesn\u2019t work anymore.\nHowever, you can create symlinks to library aliases manually:\n\n\n\n```\nfor LIBOMP_ALIAS in libgomp.dylib libiomp5.dylib libomp.dylib; do sudo ln -sf \"$(brew --cellar libomp)\"/*/lib/libomp.dylib $CONDA\\_PREFIX/lib/$LIBOMP\\_ALIAS; done\n\n```\n\n\nAnother workaround would be removing MKL optimizations from Conda\u2019s packages completely:\n\n\n\n```\nconda install nomkl\n\n```\n\n\nIf this is not your case, then you should find conflicting OpenMP library installations on your own and leave only one of them.\n\n\n"}
{"instruction": "11. LightGBM hangs when multithreading (OpenMP) and using forking in Linux at the same time.\uf0c1", "context": "lightgbm", "output": "\nUse `nthreads=1` to disable multithreading of LightGBM. There is a bug with OpenMP which hangs forked sessions\nwith multithreading activated. A more expensive solution is to use new processes instead of using fork, however,\nkeep in mind it is creating new processes where you have to copy memory and load libraries (example: if you want to\nfork 16 times your current process, then you will require to make 16 copies of your dataset in memory)\n(see [Microsoft/LightGBM#1789](https://github.com/microsoft/LightGBM/issues/1789#issuecomment-433713383)).\n\n\nAn alternative, if multithreading is really necessary inside the forked sessions, would be to compile LightGBM with\nIntel toolchain. Intel compilers are unaffected by this bug.\n\n\nFor C/C++ users, any OpenMP feature cannot be used before the fork happens. If an OpenMP feature is used before the\nfork happens (example: using OpenMP for forking), OpenMP will hang inside the forked sessions. Use new processes instead\nand copy memory as required by creating new processes instead of forking (or, use Intel compilers).\n\n\nCloud platform container services may cause LightGBM to hang, if they use Linux fork to run multiple containers on a\nsingle instance. For example, LightGBM hangs in AWS Batch array jobs, which [use the ECS agent](https://aws.amazon.com/batch/faqs/#Features) to manage multiple running jobs. Setting `nthreads=1` mitigates the issue.\n\n\n"}
{"instruction": "12. Why is early stopping not enabled by default in LightGBM?\uf0c1", "context": "lightgbm", "output": "\nEarly stopping involves choosing a validation set, a special type of holdout which is used to evaluate the current state of the model after each iteration to see if training can stop.\n\n\nIn `LightGBM`, [we have decided to require that users specify this set directly](./Parameters.html#valid). Many options exist for splitting training data into training, test, and validation sets.\n\n\nThe appropriate splitting strategy depends on the task and domain of the data, information that a modeler has but which `LightGBM` as a general-purpose tool does not.\n\n\n"}
{"instruction": "13. Does LightGBM support direct loading data from zero-based or one-based LibSVM format file?\uf0c1", "context": "lightgbm", "output": "\nLightGBM supports loading data from zero-based LibSVM format file directly.\n\n\n"}
{"instruction": "14. Why CMake cannot find the compiler when compiling LightGBM with MinGW?\uf0c1", "context": "lightgbm", "output": "\n\n```\nCMake Error: CMAKE_C_COMPILER not set, after EnableLanguage\nCMake Error: CMAKE_CXX_COMPILER not set, after EnableLanguage\n\n```\n\n\nThis is a known issue of CMake when using MinGW. The easiest solution is to run again your `cmake` command to bypass the one time stopper from CMake. Or you can upgrade your version of CMake to at least version 3.17.0.\n\n\nSee [Microsoft/LightGBM#3060](https://github.com/microsoft/LightGBM/issues/3060#issuecomment-626338538) for more details.\n\n\n"}
{"instruction": "15. Where can I find LightGBM\u2019s logo to use it in my presentation?\uf0c1", "context": "lightgbm", "output": "\nYou can find LightGBM\u2019s logo in different file formats and resolutions [here](https://github.com/microsoft/LightGBM/tree/master/docs/logo).\n\n\n"}
{"instruction": "16. LightGBM crashes randomly or operating system hangs during or after running LightGBM.\uf0c1", "context": "lightgbm", "output": "\n**Possible Cause**: This behavior may indicate that you have multiple OpenMP libraries installed on your machine and they conflict with each other, similarly to the `FAQ #10`.\n\n\nIf you are using any Python package that depends on `threadpoolctl`, you also may see the following warning in your logs in this case:\n\n\n\n```\n/root/miniconda/envs/test-env/lib/python3.8/site-packages/threadpoolctl.py:546: RuntimeWarning:\nFound Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\nthe same time. Both libraries are known to be incompatible and this\ncan cause random crashes or deadlocks on Linux when loaded in the\nsame Python program.\nUsing threadpoolctl may cause crashes or deadlocks. For more\ninformation and possible workarounds, please see\n https://github.com/joblib/threadpoolctl/blob/master/multiple\\_openmp.md\n\n```\n\n\nDetailed description of conflicts between multiple OpenMP instances is provided in the [following document](https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md).\n\n\n**Solution**: Assuming you are using LightGBM Python-package and conda as a package manager, we strongly recommend using `conda-forge` channel as the only source of all your Python package installations because it contains built-in patches to workaround OpenMP conflicts. Some other workarounds are listed [here](https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md#user-content-workarounds-for-intel-openmp-and-llvm-openmp-case).\n\n\nIf this is not your case, then you should find conflicting OpenMP library installations on your own and leave only one of them.\n\n\n"}
{"instruction": "1. Any training command using LightGBM does not work after an error occurred during the training of a previous LightGBM model.\uf0c1", "context": "lightgbm", "output": "\nIn older versions of the R package (prior to `v3.3.0`), this could happen occasionally and the solution was to run `lgb.unloader(wipe = TRUE)` to remove all LightGBM-related objects. Some conversation about this could be found in [Microsoft/LightGBM#698](https://github.com/microsoft/LightGBM/issues/698).\n\n\nThat is no longer necessary as of `v3.3.0`, and function `lgb.unloader()` has since been removed from the R package.\n\n\n"}
{"instruction": "2. I used setinfo(), tried to print my lgb.Dataset, and now the R console froze!\uf0c1", "context": "lightgbm", "output": "\nAs of at least LightGBM v3.3.0, this issue has been resolved and printing a `Dataset` object does not cause the console to freeze.\n\n\nIn older versions, avoid printing the `Dataset` after calling `setinfo()`.\n\n\nAs of LightGBM v4.0.0, `setinfo()` has been replaced by a new method, `set\\_field()`.\n\n\n"}
{"instruction": "3. error in data.table::data.table()...argument 2 is NULL\uf0c1", "context": "lightgbm", "output": "\nIf you are experiencing this error when running `lightgbm`, you may be facing the same issue reported in [#2715](https://github.com/microsoft/LightGBM/issues/2715) and later in [#2989](https://github.com/microsoft/LightGBM/pull/2989#issuecomment-614374151). We have seen that some in some situations, using `data.table` 1.11.x results in this error. To get around this, you can upgrade your version of `data.table` to at least version 1.12.0.\n\n\n"}
{"instruction": "1. Error: setup script specifies an absolute path when installing from GitHub using python setup.py install.\uf0c1", "context": "lightgbm", "output": "\n\nNote\n\n\nAs of v4.0.0, `lightgbm` does not support directly invoking `setup.py`.\nThis answer refers only to versions of `lightgbm` prior to v4.0.0.\n\n\n\n\n```\nerror: Error: setup script specifies an absolute path:\n/Users/Microsoft/LightGBM/python-package/lightgbm/../../lib\\_lightgbm.so\nsetup() arguments must \\*always\\* be /-separated paths relative to the setup.py directory, \\*never\\* absolute paths.\n\n```\n\n\nThis error should be solved in latest version.\nIf you still meet this error, try to remove `lightgbm.egg-info` folder in your Python-package and reinstall,\nor check [this thread on stackoverflow](http://stackoverflow.com/questions/18085571/pip-install-error-setup-script-specifies-an-absolute-path).\n\n\n"}
{"instruction": "2. Error messages: Cannot ... before construct dataset.\uf0c1", "context": "lightgbm", "output": "\nI see error messages like\u2026\n\n\n\n```\nCannot get/set label/weight/init\\_score/group/num\\_data/num\\_feature before construct dataset\n\n```\n\n\nbut I\u2019ve already constructed a dataset by some code like:\n\n\n\n```\ntrain = lightgbm.Dataset(X\\_train, y\\_train)\n\n```\n\n\nor error messages like\n\n\n\n```\nCannot set predictor/reference/categorical feature after freed raw data, set free\\_raw\\_data=False when construct Dataset to avoid this.\n\n```\n\n\n**Solution**: Because LightGBM constructs bin mappers to build trees, and train and valid Datasets within one Booster share the same bin mappers,\ncategorical features and feature names etc., the Dataset objects are constructed when constructing a Booster.\nIf you set `free\\_raw\\_data=True` (default), the raw data (with Python data struct) will be freed.\nSo, if you want to:\n\n\n* get label (or weight/init\\_score/group/data) before constructing a dataset, it\u2019s same as get `self.label`;\n* set label (or weight/init\\_score/group) before constructing a dataset, it\u2019s same as `self.label=some\\_label\\_array`;\n* get num\\_data (or num\\_feature) before constructing a dataset, you can get data with `self.data`.\nThen, if your data is `numpy.ndarray`, use some code like `self.data.shape`. But do not do this after subsetting the Dataset, because you\u2019ll get always `None`;\n* set predictor (or reference/categorical feature) after constructing a dataset,\nyou should set `free\\_raw\\_data=False` or init a Dataset object with the same raw data.\n\n\n"}
{"instruction": "3. I encounter segmentation faults (segfaults) randomly after installing LightGBM from PyPI using pip install lightgbm.\uf0c1", "context": "lightgbm", "output": "\nWe are doing our best to provide universal wheels which have high running speed and are compatible with any hardware, OS, compiler, etc. at the same time.\nHowever, sometimes it\u2019s just impossible to guarantee the possibility of usage of LightGBM in any specific environment (see [Microsoft/LightGBM#1743](https://github.com/microsoft/LightGBM/issues/1743)).\n\n\nTherefore, the first thing you should try in case of segfaults is **compiling from the source** using `pip install --no-binary lightgbm lightgbm`.\nFor the OS-specific prerequisites see [this guide](https://github.com/microsoft/LightGBM/blob/master/python-package/README.rst#user-content-build-from-sources).\n\n\nAlso, feel free to post a new issue in our GitHub repository. We always look at each case individually and try to find a root cause.\n\n\n"}
{"instruction": "4. I would like to install LightGBM from conda. What channel should I choose?\uf0c1", "context": "lightgbm", "output": "\nWe strongly recommend installation from the `conda-forge` channel and not from the `default` one due to many reasons.\nThe main ones are less time delay for new releases, greater number of supported architectures and better handling of dependency conflicts, especially workaround for OpenMP is crucial for LightGBM.\nMore details can be found in [this comment](https://github.com/microsoft/LightGBM/issues/4948#issuecomment-1013766397).\n\n\n"}
{"instruction": "What is PyPy?\u00b6", "context": "pypy", "output": "\nPyPy is a reimplementation of Python in Python, using the RPython translation\ntoolchain.\n\n\nPyPy tries to find new answers about ease of creation, flexibility,\nmaintainability and speed trade-offs for language implementations.\nFor further details see our [goal and architecture document](architecture.html).\n\n\n"}
{"instruction": "Is PyPy a drop in replacement for CPython?\u00b6", "context": "pypy", "output": "\nAlmost!\n\n\nThe most likely stumbling block for any given project is support for\n[extension modules](cpython_differences.html#extension-modules). PyPy supports a continually growing\nnumber of extension modules, but so far mostly only those found in the\nstandard library.\n\n\nThe language features (including builtin types and functions) are very\nrefined and well tested, so if your project doesn\u2019t use many\nextension modules there is a good chance that it will work with PyPy.\n\n\nWe list the known differences in [cpython differences](cpython_differences.html).\n\n\n"}
{"instruction": "Module xyz does not work with PyPy: ImportError\u00b6", "context": "pypy", "output": "\nA module installed for CPython is not automatically available for PyPy\n\u2014 just like a module installed for CPython 2.6 is not automatically\navailable for CPython 2.7 if you installed both. In other words, you\nneed to install the module xyz specifically for PyPy.\n\n\nOn Linux, this means that you cannot use `apt-get` or some similar\npackage manager: these tools are only meant *for the version of CPython\nprovided by the same package manager.* So forget about them for now\nand read on.\n\n\nIt is quite common nowadays that xyz is available on [PyPI](https://pypi.org) and\ninstallable with `<pypy> -mpip install xyz`. The simplest solution is to\n[use virtualenv (as documented here)](install.html#installing-using-virtualenv). Then enter (activate) the virtualenv\nand type: `pypy -mpip install xyz`. If you don\u2019t know or don\u2019t want\nvirtualenv, you can also use `pip` locally after `pypy -m ensurepip`.\nThe [ensurepip module](https://docs.python.org/3.6/library/ensurepip.html) is built-in to the PyPy downloads we provide.\nBest practices with `pip` is to always call it as `<python> -mpip ...`,\nbut if you wish to be able to call `pip` directly from the command line, you\nmust call `pypy -mensurepip --default-pip`.\n\n\nIf you get errors from the C compiler, the module is a CPython C\nExtension module using unsupported features. [See below.](#see-below)\n\n\nAlternatively, if either the module xyz is not available on PyPI or you\ndon\u2019t want to use virtualenv, then download the source code of xyz,\ndecompress the zip/tarball, and run the standard command: `pypy\nsetup.py install`. (Note: pypy here instead of python.) As usual\nyou may need to run the command with sudo for a global installation.\nThe other commands of `setup.py` are available too, like `build`.\n\n\n"}
{"instruction": "Module xyz does not work in the sandboxed PyPy?\u00b6", "context": "pypy", "output": "\nYou cannot import *any* extension module in a [sandboxed PyPy](sandbox.html),\nsorry. Even the built-in modules available are very limited.\nSandboxing in PyPy is a good proof of concept, and is without a doubt\nsafe IMHO, however it is only a proof of concept. It currently requires\nsome work from a motivated developer. However, until then it can only be used for \u201cpure Python\u201d\nexample: programs that import mostly nothing (or only pure Python\nmodules, recursively).\n\n\n"}
{"instruction": "Do C-extension modules work with PyPy?\u00b6", "context": "pypy", "output": "\n**First note that some Linux distributions (e.g. Ubuntu, Debian) split\nPyPy into several packages. If you installed a package called \u201cpypy\u201d,\nthen you may also need to install \u201cpypy-dev\u201d for the following to work.**\n\n\nWe have support for c-extension modules (modules written using the C-API), so\nthey run without modifications. This has been a part of PyPy since\nthe 1.4 release, and support is almost complete. CPython\nextension modules in PyPy are often much slower than in CPython due to\nthe need to emulate refcounting. It is often faster to take out your\nc-extension and replace it with a pure python or CFFI version that the\nJIT can optimize. If trying to install module xyz, and the module has both\na C and a Python version of the same code, try first to disable the C\nversion; this is usually easily done by changing some line in `setup.py`.\n\n\nWe fully support ctypes-based extensions. But for best performance, we\nrecommend that you use the [cffi](https://cffi.readthedocs.org/) module to interface with C code.\n\n\nFor more information about how we manage refcounting semamtics see\n[rawrefcount](discussion/rawrefcount.html)\n\n\n"}
{"instruction": "On which platforms does PyPy run?\u00b6", "context": "pypy", "output": "\nPyPy currently supports:\n\n\n\n> \n> * **x86** machines on most common operating systems\n> (Linux 32/64 bits, Mac OS X 64 bits, Windows 32/64 bits, OpenBSD, FreeBSD),\n> * 64-bit **AArch**, also known as ARM64,\n> * **ARM** hardware (ARMv6 or ARMv7, with VFPv3) running Linux\n> (we no longer provide prebuilt binaries for these),\n> * big- and little-endian variants of **PPC64** running Linux,\n> * **s390x** running Linux\n> \n> \n> \n\n\nPyPy is regularly and extensively tested on Linux machines. It\nworks on Mac and Windows: it is tested there, but most of us are running\nLinux so fixes may depend on 3rd-party contributions.\n\n\nTo bootstrap from sources, PyPy can use either CPython 2.7 or\nanother (e.g. older) PyPy. Cross-translation is not really supported:\ne.g. to build a 32-bit PyPy, you need to have a 32-bit environment.\n\n\n"}
{"instruction": "Which Python version (2.x?) does PyPy implement?\u00b6", "context": "pypy", "output": "\nPyPy comes in two versions:\n\n\n* one is fully compatible with Python 2.7;\n* the other is fully compatible with one 3.x version. At the time of\nthis writing, this is 3.7.\n\n\n"}
{"instruction": "Does PyPy have a GIL?  Why?\u00b6", "context": "pypy", "output": "\nYes, PyPy has a GIL. Removing the GIL is very hard. On top of CPython,\nyou have two problems: (1) GC, in this case reference counting; (2) the\nwhole Python language.\n\n\nFor PyPy, the hard issue is (2): by that I mean issues like what occurs\nif a mutable object is changed from one thread and read from another\nconcurrently. This is a problem for *any* mutable type: it needs\ncareful review and fixes (fine-grained locks, mostly) through the\n*whole* Python interpreter. It is a major effort, although not\ncompletely impossible, as Jython/IronPython showed. This includes\nsubtle decisions about whether some effects are ok or not for the user\n(i.e. the Python programmer).\n\n\nCPython has additionally the problem (1) of reference counting. With\nPyPy, this sub-problem is simpler: we need to make our GC\nmultithread-aware. This is easier to do efficiently in PyPy than in\nCPython. It doesn\u2019t solve the issue (2), though.\n\n\nNote that there was work to support a\n[Software Transactional Memory](stm.html) (STM) version of PyPy. This\nshould give an alternative PyPy which works without a GIL, while at the\nsame time continuing to give the Python programmer the complete illusion\nof having one. This work is currently a bit stalled because of its own\ntechnical difficulties.\n\n\n"}
{"instruction": "What about numpy, numpypy, micronumpy?\u00b6", "context": "pypy", "output": "\nWay back in 2011, the PyPy team [started to reimplement](https://www.pypy.org/posts/2011/05/numpy-in-pypy-status-and-roadmap-8332894230779779992.html) numpy in PyPy. It\nhas two pieces:\n\n\n\n> \n> * the builtin module [pypy/module/micronumpy](https://foss.heptapod.net/pypy/pypy/-/tree/branch/default/pypy/module/micronumpy): this is written in\n> RPython and roughly covers the content of the `numpy.core.multiarray`\n> module. Confusingly enough, this is available in PyPy under the name\n> `\\_numpypy`. It is included by default in all the official releases of\n> PyPy (but it might be dropped in the future).\n> * a [fork](https://github.com/pypy/numpypy) of the official numpy repository maintained by us and informally\n> called `numpypy`: The main difference with the upstream numpy, is that\n> it is based on the micronumpy module written in RPython, instead of of\n> `numpy.core.multiarray` which is written in C.\n> \n> \n> \n\n\n"}
{"instruction": "Should I install numpy or numpypy?\u00b6", "context": "pypy", "output": "\nTL;DR version: you should use numpy. You can install it by doing `pypy -m pip\ninstall numpy`. You might also be interested in using the experimental [PyPy\nbinary wheels](https://github.com/antocuni/pypy-wheels) to save compilation time.\n\n\nThe upstream `numpy` is written in C, and runs under the cpyext\ncompatibility layer. Nowadays, cpyext is mature enough that you can simply\nuse the upstream `numpy`, since it passes the test suite. At the\nmoment of writing (October 2017) the main drawback of `numpy` is that cpyext\nis infamously slow, and thus it has worse performance compared to\n`numpypy`. However, we are actively working on improving it, as we expect to\nreach the same speed when [HPy](https://hpyproject.org/) can be used.\n\n\nOn the other hand, `numpypy` is more JIT-friendly and very fast to call,\nsince it is written in RPython: but it is a reimplementation, and it\u2019s hard to\nbe completely compatible: over the years the project slowly matured and\neventually it was able to call out to the LAPACK and BLAS libraries to speed\nmatrix calculations, and reached around an 80% parity with the upstream\nnumpy. However, 80% is far from 100%. Since cpyext/numpy compatibility is\nprogressing fast, we have discontinued support for `numpypy`.\n\n\n"}
{"instruction": "Is PyPy more clever than CPython about Tail Calls?\u00b6", "context": "pypy", "output": "\nNo. PyPy follows the Python language design, including the built-in\ndebugger features. This prevents tail calls, as summarized by Guido\nvan Rossum in [two](https://neopythonic.blogspot.com/2009/04/tail-recursion-elimination.html) [blog](https://neopythonic.blogspot.com/2009/04/final-words-on-tail-calls.html) posts. Moreover, neither the JIT nor\n[Stackless](stackless.html) change anything to that.\n\n\n"}
{"instruction": "How do I write extension modules for PyPy?\u00b6", "context": "pypy", "output": "\nSee [Writing extension modules for pypy](extending.html).\n\n\n"}
{"instruction": "How fast is PyPy?\u00b6", "context": "pypy", "output": "\nThis really depends on your code.\nFor pure Python algorithmic code, it is very fast. For more typical\nPython programs we generally are 3 times the speed of CPython 2.7.\nYou might be interested in our [benchmarking site](https://speed.pypy.org) and our\n[jit documentation](https://rpython.readthedocs.io/en/latest/jit/index.html#jit \"(in PyPy v7.3)\").\n\n\n[Your tests are not a benchmark](https://alexgaynor.net/2013/jul/15/your-tests-are-not-benchmark/): tests tend to be slow under PyPy\nbecause they run exactly once; if they are good tests, they exercise\nvarious corner cases in your code. This is a bad case for JIT\ncompilers. Note also that our JIT has a very high warm-up cost, meaning\nthat any program is slow at the beginning. If you want to compare the\ntimings with CPython, even relatively simple programs need to run *at\nleast* one second, preferrably at least a few seconds. Large,\ncomplicated programs need even more time to warm-up the JIT.\n\n\n"}
{"instruction": "I wrote a 3-lines benchmark and it\u2019s not faster than CPython.  Why?\u00b6", "context": "pypy", "output": "\nThree-lines benchmarks are benchmarks that either do absolutely nothing (in\nwhich case PyPy is probably a lot faster than CPython), or more likely, they\nare benchmarks that spend most of their time doing things in C.\n\n\nFor example, a loop that repeatedly issues one complex SQL operation will only\nmeasure how performant the SQL database is. Similarly, computing many elements\nfrom the Fibonacci series builds very large integers, so it only measures how\nperformant the long integer library is. This library is written in C for\nCPython, and in RPython for PyPy, but that boils down to the same thing.\n\n\nPyPy speeds up the code written *in Python*.\n\n\n"}
{"instruction": "Couldn\u2019t the JIT dump and reload already-compiled machine code?\u00b6", "context": "pypy", "output": "\nNo, we found no way of doing that. The JIT generates machine code\ncontaining a large number of constant addresses \u2014 constant at the time\nthe machine code is generated. The vast majority is probably not at all\nconstants that you find in the executable, with a nice link name. E.g.\nthe addresses of Python classes are used all the time, but Python\nclasses don\u2019t come statically from the executable; they are created anew\nevery time you restart your program. This makes saving and reloading\nmachine code completely impossible without some very advanced way of\nmapping addresses in the old (now-dead) process to addresses in the new\nprocess, including checking that all the previous assumptions about the\n(now-dead) object are still true about the new object.\n\n\n"}
{"instruction": "Would type annotations help PyPy\u2019s performance?\u00b6", "context": "pypy", "output": "\nTwo examples of type annotations that are being proposed for improved\nperformance are [Cython types](https://docs.cython.org/src/reference/language_basics.html#declaring-data-types) and [PEP 484 - Type Hints](https://www.python.org/dev/peps/pep-0484/).\n\n\n**Cython types** are, by construction, similar to C declarations. For\nexample, a local variable or an instance attribute can be declared\n`\"cdef int\"` to force a machine word to be used. This changes the\nusual Python semantics (e.g. no overflow checks, and errors when\ntrying to write other types of objects there). It gives some extra\nperformance, but the exact benefits are unclear: right now\n(January 2015) for example we are investigating a technique that would\nstore machine-word integers directly on instances, giving part of the\nbenefits without the user-supplied `\"cdef int\"`.\n\n\n**PEP 484 - Type Hints,** on the other hand, is almost entirely\nuseless if you\u2019re looking at performance. First, as the name implies,\nthey are *hints:* they must still be checked at runtime, like PEP 484\nsays. Or maybe you\u2019re fine with a mode in which you get very obscure\ncrashes when the type annotations are wrong; but even in that case the\nspeed benefits would be extremely minor.\n\n\nThere are several reasons for why. One of them is that annotations\nare at the wrong level (e.g. a PEP 484 \u201cint\u201d corresponds to Python 3\u2019s\nint type, which does not necessarily fits inside one machine word;\neven worse, an \u201cint\u201d annotation allows arbitrary int subclasses).\nAnother is that a lot more information is needed to produce good code\n(e.g. \u201cthis `f()` called here really means this function there, and\nwill never be monkey-patched\u201d \u2013 same with `len()` or `list()`,\nbtw). The third reason is that some \u201cguards\u201d in PyPy\u2019s JIT traces\ndon\u2019t really have an obvious corresponding type (e.g. \u201cthis dict is so\nfar using keys which don\u2019t override `\\_\\_hash\\_\\_` so a more efficient\nimplementation was used\u201d). Many guards don\u2019t even have any correspondence\nwith types at all (\u201cthis class attribute was not modified\u201d; \u201cthe loop\ncounter did not reach zero so we don\u2019t need to release the GIL\u201d; and\nso on).\n\n\nAs PyPy works right now, it is able to derive far more useful\ninformation than can ever be given by PEP 484, and it works\nautomatically. As far as we know, this is true even if we would add\nother techniques to PyPy, like a fast first-pass JIT.\n\n\n"}
{"instruction": "Can I use PyPy\u2019s translation toolchain for other languages besides Python?\u00b6", "context": "pypy", "output": "\nYes. The toolsuite that translates the PyPy interpreter is quite\ngeneral and can be used to create optimized versions of interpreters\nfor any language, not just Python. Of course, these interpreters\ncan make use of the same features that PyPy brings to Python:\ntranslation to various languages, stackless features,\ngarbage collection, implementation of various things like arbitrarily long\nintegers, etc.\n\n\nCurrently, we have [Topaz](https://github.com/topazproject/topaz), a Ruby interpreter; [Hippy](https://morepypy.blogspot.ch/2012/07/hello-everyone.html), a PHP\ninterpreter; preliminary versions of a [JavaScript interpreter](https://github.com/progval/rpython-langjs)\n(Leonardo Santagada as his Summer of PyPy project); a [Prolog interpreter](https://bitbucket.org/cfbolz/pyrolog/)\n(Carl Friedrich Bolz as his Bachelor thesis); and a [SmallTalk interpreter](https://dx.doi.org/10.1007/978-3-540-89275-5_7)\n(produced during a sprint). There is also an unfinished [Scheme](https://github.com/tomoh1r/rpython-lang-scheme) implementation.\n\n\n"}
{"instruction": "How do I get into PyPy development?  Can I come to sprints?\u00b6", "context": "pypy", "output": "\nCertainly you can come to sprints! We always welcome newcomers and try\nto help them as much as possible to get started with the project. We\nprovide tutorials and pair them with experienced PyPy\ndevelopers. Newcomers should have some Python experience and read some\nof the PyPy documentation before coming to a sprint.\n\n\nComing to a sprint is usually the best way to get into PyPy development.\nIf you get stuck or need advice, [contact us](index.html). IRC is\nthe most immediate way to get feedback (at least during some parts of the day;\nmost PyPy developers are in Europe) and the [mailing list](https://mail.python.org/mailman/listinfo/pypy-dev) is better for long\ndiscussions.\n\n\nWe also encourage engagement via the gitlab repo at\n<https://foss.heptapod.net/pypy/pypy>. Issues can be filed and discussed in the\n[issue tracker](https://foss.heptapod.net/heptapod/foss.heptapod.net/-/issues) and we welcome merge requests.\n\n\n"}
{"instruction": "OSError: \u2026 cannot restore segment prot after reloc\u2026 Help?\u00b6", "context": "pypy", "output": "\nOn Linux, if SELinux is enabled, you may get errors along the lines of\n\u201cOSError: externmod.so: cannot restore segment prot after reloc: Permission\ndenied.\u201d This is caused by a slight abuse of the C compiler during\nconfiguration, and can be disabled by running the following command with root\nprivileges:\n\n\n\n```\n# setenforce 0\n\n```\n\n\nThis will disable SELinux\u2019s protection and allow PyPy to configure correctly.\nBe sure to enable it again if you need it!\n\n\n"}
{"instruction": "How should I report a bug?\u00b6", "context": "pypy", "output": "\nOur bug tracker is here: <https://foss.heptapod.net/pypy/pypy/issues/>\n\n\nMissing features or incompatibilities with CPython are considered\nbugs, and they are welcome. (See also our list of [known\nincompatibilities](https://pypy.org/compat.html).)\n\n\nFor bugs of the kind \u201cI\u2019m getting a PyPy crash or a strange\nexception\u201d, please note that: **We can\u2019t do anything without\nreproducing the bug ourselves**. We cannot do anything with\ntracebacks from gdb, or core dumps. This is not only because the\nstandard PyPy is compiled without debug symbols. The real reason is\nthat a C-level traceback is usually of no help at all in PyPy.\nDebugging PyPy can be annoying.\n\n\n[This is a clear and useful bug report.](https://foss.heptapod.net/pypy/pypy/issues/2363/segfault-in-gc-pinned-object-in) (Admittedly, sometimes\nthe problem is really hard to reproduce, but please try to.)\n\n\nIn more details:\n\n\n* First, please give the exact PyPy version, and the OS.\n* It might help focus our search if we know if the bug can be\nreproduced on a \u201c`pypy --jit off`\u201d or not. If \u201c`pypy --jit\noff`\u201d always works, then the problem might be in the JIT.\nOtherwise, we know we can ignore that part.\n* If you got the bug using only Open Source components, please give a\nstep-by-step guide that we can follow to reproduce the problem\nourselves. Don\u2019t assume we know anything about any program other\nthan PyPy. We would like a guide that we can follow point by point\n(without guessing or having to figure things out)\non a machine similar to yours, starting from a bare PyPy, until we\nsee the same problem. (If you can, you can try to reduce the number\nof steps and the time it needs to run, but that is not mandatory.)\n* If the bug involves Closed Source components, or just too many Open\nSource components to install them all ourselves, then maybe you can\ngive us some temporary ssh access to a machine where the bug can be\nreproduced. Or, maybe we can download a VirtualBox or VMWare\nvirtual machine where the problem occurs.\n* If giving us access would require us to use tools other than ssh,\nmake appointments, or sign a NDA, then we can consider a commerical\nsupport contract for a small sum of money.\n* If even that is not possible for you, then sorry, we can\u2019t help.\n\n\nOf course, you can try to debug the problem yourself, and we can help\nyou get started if you ask on the #pypy IRC channel, but be prepared:\ndebugging an annoying PyPy problem usually involves quite a lot of gdb\nin auto-generated C code, and at least some knowledge about the\nvarious components involved, from PyPy\u2019s own RPython source code to\nthe GC and possibly the JIT.\n\n\n"}
{"instruction": "Why doesn\u2019t PyPy use Git and move to GitHub?\u00b6", "context": "pypy", "output": "\nWe discussed it during the switch away from bitbucket. We concluded that (1)\nthe Git workflow is not as well suited as the Mercurial workflow for our style,\nand (2) moving to github \u201cjust because everybody else does\u201d is a argument on\nthin grounds.\n\n\nFor (1), there are a few issues, but maybe the most important one is that the\nPyPy repository has got thousands of *named* branches. Git has no equivalent\nconcept. Git has *branches,* of course, which in Mercurial are called\nbookmarks. We\u2019re not talking about bookmarks.\n\n\nThe difference between git branches and named branches is not that important in\na repo with 10 branches (no matter how big). But in the case of PyPy, we have\nat the moment 1840 branches. Most are closed by now, of course. But we would\nreally like to retain (both now and in the future) the ability to look at a\ncommit from the past, and know in which branch it was made. Please make sure\nyou understand the difference between the Git and the Mercurial branches to\nrealize that this is not always possible with Git\u2014 we looked hard, and there\nis no built-in way to get this workflow.\n\n\nStill not convinced? Consider this git repo with three commits: commit #2 with\nparent #1 and head of git branch \u201cA\u201d; commit #3 with also parent #1 but head of\ngit branch \u201cB\u201d. When commit #1 was made, was it in the branch \u201cA\u201d or \u201cB\u201d?\n(It could also be yet another branch whose head was also moved forward, or even\ncompletely deleted.)\n\n\n"}
{"instruction": "What is needed for better Windows 64 support of PyPy?\u00b6", "context": "pypy", "output": "\nAs of PyPy 7.3.5, PyPy supports Windows 64-bits. Since only on that platform\n`sizeof(long) != sizeof(void \\*)`, and the underlying data type for RPython is\n`long`, this proved to be challenging. It seems we have crossed that bridge,\nand welcome help in bringing the Windows version into parity with CPython. In\nparticular, we still do not support Windows-specific features like\n`winconsoleio`, windows audit events, and the Windows `faulthandler`.\nPerformance may lag behind Linux64, and the `wininstaller` branch is still\nunfinished.\n\n\nHelp is welcome!\n\n\n"}
{"instruction": "How long will PyPy support Python2?\u00b6", "context": "pypy", "output": "\nSince RPython is built on top of Python2 and that is extremely unlikely to\nchange, the Python2 version of PyPy will be around \u201cforever\u201d, i.e. as long as\nPyPy itself is around.\n\n\n"}
{"instruction": "What is Python?\u00b6", "context": "python general", "output": "\nPython is an interpreted, interactive, object-oriented programming language. It\nincorporates modules, exceptions, dynamic typing, very high level dynamic data\ntypes, and classes. It supports multiple programming paradigms beyond\nobject-oriented programming, such as procedural and functional programming.\nPython combines remarkable power with very clear syntax. It has interfaces to\nmany system calls and libraries, as well as to various window systems, and is\nextensible in C or C++. It is also usable as an extension language for\napplications that need a programmable interface. Finally, Python is portable:\nit runs on many Unix variants including Linux and macOS, and on Windows.\n\n\nTo find out more, start with [The Python Tutorial](../tutorial/index.html#tutorial-index). The [Beginner\u2019s Guide to\nPython](https://wiki.python.org/moin/BeginnersGuide) links to other\nintroductory tutorials and resources for learning Python.\n\n\n"}
{"instruction": "What is the Python Software Foundation?\u00b6", "context": "python general", "output": "\nThe Python Software Foundation is an independent non-profit organization that\nholds the copyright on Python versions 2.1 and newer. The PSF\u2019s mission is to\nadvance open source technology related to the Python programming language and to\npublicize the use of Python. The PSF\u2019s home page is at\n<https://www.python.org/psf/>.\n\n\nDonations to the PSF are tax-exempt in the US. If you use Python and find it\nhelpful, please contribute via [the PSF donation page](https://www.python.org/psf/donations/).\n\n\n"}
{"instruction": "Are there copyright restrictions on the use of Python?\u00b6", "context": "python general", "output": "\nYou can do anything you want with the source, as long as you leave the\ncopyrights in and display those copyrights in any documentation about Python\nthat you produce. If you honor the copyright rules, it\u2019s OK to use Python for\ncommercial use, to sell copies of Python in source or binary form (modified or\nunmodified), or to sell products that incorporate Python in some form. We would\nstill like to know about all commercial use of Python, of course.\n\n\nSee [the license page](https://docs.python.org/3/license.html) to find further\nexplanations and the full text of the PSF License.\n\n\nThe Python logo is trademarked, and in certain cases permission is required to\nuse it. Consult [the Trademark Usage Policy](https://www.python.org/psf/trademarks/) for more information.\n\n\n"}
{"instruction": "Why was Python created in the first place?\u00b6", "context": "python general", "output": "\nHere\u2019s a *very* brief summary of what started it all, written by Guido van\nRossum:\n\n\n\n> \n> I had extensive experience with implementing an interpreted language in the\n> ABC group at CWI, and from working with this group I had learned a lot about\n> language design. This is the origin of many Python features, including the\n> use of indentation for statement grouping and the inclusion of\n> very-high-level data types (although the details are all different in\n> Python).\n> \n> \n> I had a number of gripes about the ABC language, but also liked many of its\n> features. It was impossible to extend the ABC language (or its\n> implementation) to remedy my complaints \u2013 in fact its lack of extensibility\n> was one of its biggest problems. I had some experience with using Modula-2+\n> and talked with the designers of Modula-3 and read the Modula-3 report.\n> Modula-3 is the origin of the syntax and semantics used for exceptions, and\n> some other Python features.\n> \n> \n> I was working in the Amoeba distributed operating system group at CWI. We\n> needed a better way to do system administration than by writing either C\n> programs or Bourne shell scripts, since Amoeba had its own system call\n> interface which wasn\u2019t easily accessible from the Bourne shell. My\n> experience with error handling in Amoeba made me acutely aware of the\n> importance of exceptions as a programming language feature.\n> \n> \n> It occurred to me that a scripting language with a syntax like ABC but with\n> access to the Amoeba system calls would fill the need. I realized that it\n> would be foolish to write an Amoeba-specific language, so I decided that I\n> needed a language that was generally extensible.\n> \n> \n> During the 1989 Christmas holidays, I had a lot of time on my hand, so I\n> decided to give it a try. During the next year, while still mostly working\n> on it in my own time, Python was used in the Amoeba project with increasing\n> success, and the feedback from colleagues made me add many early\n> improvements.\n> \n> \n> In February 1991, after just over a year of development, I decided to post to\n> USENET. The rest is in the `Misc/HISTORY` file.\n> \n> \n> \n\n\n"}
{"instruction": "What is Python good for?\u00b6", "context": "python general", "output": "\nPython is a high-level general-purpose programming language that can be applied\nto many different classes of problems.\n\n\nThe language comes with a large standard library that covers areas such as\nstring processing (regular expressions, Unicode, calculating differences between\nfiles), internet protocols (HTTP, FTP, SMTP, XML-RPC, POP, IMAP),\nsoftware engineering (unit testing, logging, profiling, parsing\nPython code), and operating system interfaces (system calls, filesystems, TCP/IP\nsockets). Look at the table of contents for [The Python Standard Library](../library/index.html#library-index) to get an idea\nof what\u2019s available. A wide variety of third-party extensions are also\navailable. Consult [the Python Package Index](https://pypi.org) to\nfind packages of interest to you.\n\n\n"}
{"instruction": "How does the Python version numbering scheme work?\u00b6", "context": "python general", "output": "\nPython versions are numbered \u201cA.B.C\u201d or \u201cA.B\u201d:\n\n\n* *A* is the major version number \u2013 it is only incremented for really major\nchanges in the language.\n* *B* is the minor version number \u2013 it is incremented for less earth-shattering\nchanges.\n* *C* is the micro version number \u2013 it is incremented for each bugfix release.\n\n\nSee [**PEP 6**](https://peps.python.org/pep-0006/) for more information about bugfix releases.\n\n\nNot all releases are bugfix releases. In the run-up to a new feature release, a\nseries of development releases are made, denoted as alpha, beta, or release\ncandidate. Alphas are early releases in which interfaces aren\u2019t yet finalized;\nit\u2019s not unexpected to see an interface change between two alpha releases.\nBetas are more stable, preserving existing interfaces but possibly adding new\nmodules, and release candidates are frozen, making no changes except as needed\nto fix critical bugs.\n\n\nAlpha, beta and release candidate versions have an additional suffix:\n\n\n* The suffix for an alpha version is \u201caN\u201d for some small number *N*.\n* The suffix for a beta version is \u201cbN\u201d for some small number *N*.\n* The suffix for a release candidate version is \u201crcN\u201d for some small number *N*.\n\n\nIn other words, all versions labeled *2.0aN* precede the versions labeled\n*2.0bN*, which precede versions labeled *2.0rcN*, and *those* precede 2.0.\n\n\nYou may also find version numbers with a \u201c+\u201d suffix, e.g. \u201c2.2+\u201d. These are\nunreleased versions, built directly from the CPython development repository. In\npractice, after a final minor release is made, the version is incremented to the\nnext minor version, which becomes the \u201ca0\u201d version, e.g. \u201c2.4a0\u201d.\n\n\nSee also the documentation for [`sys.version`](../library/sys.html#sys.version \"sys.version\"), [`sys.hexversion`](../library/sys.html#sys.hexversion \"sys.hexversion\"), and\n[`sys.version\\_info`](../library/sys.html#sys.version_info \"sys.version_info\").\n\n\n"}
{"instruction": "How do I obtain a copy of the Python source?\u00b6", "context": "python general", "output": "\nThe latest Python source distribution is always available from python.org, at\n<https://www.python.org/downloads/>. The latest development sources can be obtained\nat <https://github.com/python/cpython/>.\n\n\nThe source distribution is a gzipped tar file containing the complete C source,\nSphinx-formatted documentation, Python library modules, example programs, and\nseveral useful pieces of freely distributable software. The source will compile\nand run out of the box on most UNIX platforms.\n\n\nConsult the [Getting Started section of the Python Developer\u2019s Guide](https://devguide.python.org/setup/) for more\ninformation on getting the source code and compiling it.\n\n\n"}
{"instruction": "How do I get documentation on Python?\u00b6", "context": "python general", "output": "\nThe standard documentation for the current stable version of Python is available\nat <https://docs.python.org/3/>. PDF, plain text, and downloadable HTML versions are\nalso available at <https://docs.python.org/3/download.html>.\n\n\nThe documentation is written in reStructuredText and processed by [the Sphinx\ndocumentation tool](https://www.sphinx-doc.org/). The reStructuredText source for\nthe documentation is part of the Python source distribution.\n\n\n"}
{"instruction": "I\u2019ve never programmed before. Is there a Python tutorial?\u00b6", "context": "python general", "output": "\nThere are numerous tutorials and books available. The standard documentation\nincludes [The Python Tutorial](../tutorial/index.html#tutorial-index).\n\n\nConsult [the Beginner\u2019s Guide](https://wiki.python.org/moin/BeginnersGuide) to\nfind information for beginning Python programmers, including lists of tutorials.\n\n\n"}
{"instruction": "Is there a newsgroup or mailing list devoted to Python?\u00b6", "context": "python general", "output": "\nThere is a newsgroup, *comp.lang.python*, and a mailing list,\n[python-list](https://mail.python.org/mailman/listinfo/python-list). The\nnewsgroup and mailing list are gatewayed into each other \u2013 if you can read news\nit\u2019s unnecessary to subscribe to the mailing list.\n*comp.lang.python* is high-traffic, receiving hundreds of postings\nevery day, and Usenet readers are often more able to cope with this volume.\n\n\nAnnouncements of new software releases and events can be found in\ncomp.lang.python.announce, a low-traffic moderated list that receives about five\npostings per day. It\u2019s available as [the python-announce mailing list](https://mail.python.org/mailman3/lists/python-announce-list.python.org/).\n\n\nMore info about other mailing lists and newsgroups\ncan be found at <https://www.python.org/community/lists/>.\n\n\n"}
{"instruction": "How do I get a beta test version of Python?\u00b6", "context": "python general", "output": "\nAlpha and beta releases are available from <https://www.python.org/downloads/>. All\nreleases are announced on the comp.lang.python and comp.lang.python.announce\nnewsgroups and on the Python home page at <https://www.python.org/>; an RSS feed of\nnews is available.\n\n\nYou can also access the development version of Python through Git. See\n[The Python Developer\u2019s Guide](https://devguide.python.org/) for details.\n\n\n"}
{"instruction": "How do I submit bug reports and patches for Python?\u00b6", "context": "python general", "output": "\nTo report a bug or submit a patch, use the issue tracker at\n<https://github.com/python/cpython/issues>.\n\n\nFor more information on how Python is developed, consult [the Python Developer\u2019s\nGuide](https://devguide.python.org/).\n\n\n"}
{"instruction": "Are there any published articles about Python that I can reference?\u00b6", "context": "python general", "output": "\nIt\u2019s probably best to cite your favorite book about Python.\n\n\nThe [very first article](https://ir.cwi.nl/pub/18204) about Python was\nwritten in 1991 and is now quite outdated.\n\n\n\n> \n> Guido van Rossum and Jelke de Boer, \u201cInteractively Testing Remote Servers\n> Using the Python Programming Language\u201d, CWI Quarterly, Volume 4, Issue 4\n> (December 1991), Amsterdam, pp 283\u2013303.\n> \n> \n> \n\n\n"}
{"instruction": "Are there any books on Python?\u00b6", "context": "python general", "output": "\nYes, there are many, and more are being published. See the python.org wiki at\n<https://wiki.python.org/moin/PythonBooks> for a list.\n\n\nYou can also search online bookstores for \u201cPython\u201d and filter out the Monty\nPython references; or perhaps search for \u201cPython\u201d and \u201clanguage\u201d.\n\n\n"}
{"instruction": "Where in the world is www.python.org located?\u00b6", "context": "python general", "output": "\nThe Python project\u2019s infrastructure is located all over the world and is managed\nby the Python Infrastructure Team. Details [here](https://infra.psf.io).\n\n\n"}
{"instruction": "Why is it called Python?\u00b6", "context": "python general", "output": "\nWhen he began implementing Python, Guido van Rossum was also reading the\npublished scripts from [\u201cMonty Python\u2019s Flying Circus\u201d](https://en.wikipedia.org/wiki/Monty_Python), a BBC comedy series from the 1970s. Van Rossum\nthought he needed a name that was short, unique, and slightly mysterious, so he\ndecided to call the language Python.\n\n\n"}
{"instruction": "Do I have to like \u201cMonty Python\u2019s Flying Circus\u201d?\u00b6", "context": "python general", "output": "\nNo, but it helps. :)\n\n\n"}
{"instruction": "How stable is Python?\u00b6", "context": "python general", "output": "\nVery stable. New, stable releases have been coming out roughly every 6 to 18\nmonths since 1991, and this seems likely to continue. As of version 3.9,\nPython will have a new feature release every 12 months ([**PEP 602**](https://peps.python.org/pep-0602/)).\n\n\nThe developers issue bugfix releases of older versions, so the stability of\nexisting releases gradually improves. Bugfix releases, indicated by a third\ncomponent of the version number (e.g. 3.5.3, 3.6.2), are managed for stability;\nonly fixes for known problems are included in a bugfix release, and it\u2019s\nguaranteed that interfaces will remain the same throughout a series of bugfix\nreleases.\n\n\nThe latest stable releases can always be found on the [Python download page](https://www.python.org/downloads/). There are two production-ready versions\nof Python: 2.x and 3.x. The recommended version is 3.x, which is supported by\nmost widely used libraries. Although 2.x is still widely used, [it is not\nmaintained anymore](https://peps.python.org/pep-0373/).\n\n\n"}
{"instruction": "How many people are using Python?\u00b6", "context": "python general", "output": "\nThere are probably millions of users, though it\u2019s difficult to obtain an exact\ncount.\n\n\nPython is available for free download, so there are no sales figures, and it\u2019s\navailable from many different sites and packaged with many Linux distributions,\nso download statistics don\u2019t tell the whole story either.\n\n\nThe comp.lang.python newsgroup is very active, but not all Python users post to\nthe group or even read it.\n\n\n"}
{"instruction": "Have any significant projects been done in Python?\u00b6", "context": "python general", "output": "\nSee <https://www.python.org/about/success> for a list of projects that use Python.\nConsulting the proceedings for [past Python conferences](https://www.python.org/community/workshops/) will reveal contributions from many\ndifferent companies and organizations.\n\n\nHigh-profile Python projects include [the Mailman mailing list manager](https://www.list.org) and [the Zope application server](https://www.zope.dev). Several Linux distributions, most notably [Red Hat](https://www.redhat.com), have written part or all of their installer and\nsystem administration software in Python. Companies that use Python internally\ninclude Google, Yahoo, and Lucasfilm Ltd.\n\n\n"}
{"instruction": "What new developments are expected for Python in the future?\u00b6", "context": "python general", "output": "\nSee <https://peps.python.org/> for the Python Enhancement Proposals\n(PEPs). PEPs are design documents describing a suggested new feature for Python,\nproviding a concise technical specification and a rationale. Look for a PEP\ntitled \u201cPython X.Y Release Schedule\u201d, where X.Y is a version that hasn\u2019t been\npublicly released yet.\n\n\nNew development is discussed on [the python-dev mailing list](https://mail.python.org/mailman3/lists/python-dev.python.org/).\n\n\n"}
{"instruction": "Is it reasonable to propose incompatible changes to Python?\u00b6", "context": "python general", "output": "\nIn general, no. There are already millions of lines of Python code around the\nworld, so any change in the language that invalidates more than a very small\nfraction of existing programs has to be frowned upon. Even if you can provide a\nconversion program, there\u2019s still the problem of updating all documentation;\nmany books have been written about Python, and we don\u2019t want to invalidate them\nall at a single stroke.\n\n\nProviding a gradual upgrade path is necessary if a feature has to be changed.\n[**PEP 5**](https://peps.python.org/pep-0005/) describes the procedure followed for introducing backward-incompatible\nchanges while minimizing disruption for users.\n\n\n"}
{"instruction": "Is Python a good language for beginning programmers?\u00b6", "context": "python general", "output": "\nYes.\n\n\nIt is still common to start students with a procedural and statically typed\nlanguage such as Pascal, C, or a subset of C++ or Java. Students may be better\nserved by learning Python as their first language. Python has a very simple and\nconsistent syntax and a large standard library and, most importantly, using\nPython in a beginning programming course lets students concentrate on important\nprogramming skills such as problem decomposition and data type design. With\nPython, students can be quickly introduced to basic concepts such as loops and\nprocedures. They can probably even work with user-defined objects in their very\nfirst course.\n\n\nFor a student who has never programmed before, using a statically typed language\nseems unnatural. It presents additional complexity that the student must master\nand slows the pace of the course. The students are trying to learn to think\nlike a computer, decompose problems, design consistent interfaces, and\nencapsulate data. While learning to use a statically typed language is\nimportant in the long term, it is not necessarily the best topic to address in\nthe students\u2019 first programming course.\n\n\nMany other aspects of Python make it a good first language. Like Java, Python\nhas a large standard library so that students can be assigned programming\nprojects very early in the course that *do* something. Assignments aren\u2019t\nrestricted to the standard four-function calculator and check balancing\nprograms. By using the standard library, students can gain the satisfaction of\nworking on realistic applications as they learn the fundamentals of programming.\nUsing the standard library also teaches students about code reuse. Third-party\nmodules such as PyGame are also helpful in extending the students\u2019 reach.\n\n\nPython\u2019s interactive interpreter enables students to test language features\nwhile they\u2019re programming. They can keep a window with the interpreter running\nwhile they enter their program\u2019s source in another window. If they can\u2019t\nremember the methods for a list, they can do something like this:\n\n\n\n```\n>>> L = []\n>>> dir(L) \n['\\_\\_add\\_\\_', '\\_\\_class\\_\\_', '\\_\\_contains\\_\\_', '\\_\\_delattr\\_\\_', '\\_\\_delitem\\_\\_',\n'\\_\\_dir\\_\\_', '\\_\\_doc\\_\\_', '\\_\\_eq\\_\\_', '\\_\\_format\\_\\_', '\\_\\_ge\\_\\_',\n'\\_\\_getattribute\\_\\_', '\\_\\_getitem\\_\\_', '\\_\\_gt\\_\\_', '\\_\\_hash\\_\\_', '\\_\\_iadd\\_\\_',\n'\\_\\_imul\\_\\_', '\\_\\_init\\_\\_', '\\_\\_iter\\_\\_', '\\_\\_le\\_\\_', '\\_\\_len\\_\\_', '\\_\\_lt\\_\\_',\n'\\_\\_mul\\_\\_', '\\_\\_ne\\_\\_', '\\_\\_new\\_\\_', '\\_\\_reduce\\_\\_', '\\_\\_reduce\\_ex\\_\\_',\n'\\_\\_repr\\_\\_', '\\_\\_reversed\\_\\_', '\\_\\_rmul\\_\\_', '\\_\\_setattr\\_\\_', '\\_\\_setitem\\_\\_',\n'\\_\\_sizeof\\_\\_', '\\_\\_str\\_\\_', '\\_\\_subclasshook\\_\\_', 'append', 'clear',\n'copy', 'count', 'extend', 'index', 'insert', 'pop', 'remove',\n'reverse', 'sort']\n>>> [d for d in dir(L) if '\\_\\_' not in d]\n['append', 'clear', 'copy', 'count', 'extend', 'index', 'insert', 'pop', 'remove', 'reverse', 'sort']\n\n>>> help(L.append)\nHelp on built-in function append:\n\nappend(...)\n L.append(object) -> None -- append object to end\n\n>>> L.append(1)\n>>> L\n[1]\n\n```\n\n\nWith the interpreter, documentation is never far from the student as they are\nprogramming.\n\n\nThere are also good IDEs for Python. IDLE is a cross-platform IDE for Python\nthat is written in Python using Tkinter.\nEmacs users will be happy to know that there is a very good Python mode for\nEmacs. All of these programming environments provide syntax highlighting,\nauto-indenting, and access to the interactive interpreter while coding. Consult\n[the Python wiki](https://wiki.python.org/moin/PythonEditors) for a full list\nof Python editing environments.\n\n\nIf you want to discuss Python\u2019s use in education, you may be interested in\njoining [the edu-sig mailing list](https://www.python.org/community/sigs/current/edu-sig).\n\n\n"}
{"instruction": "Is there a source code level debugger with breakpoints, single-stepping, etc.?\u00b6", "context": "python programming", "output": "\nYes.\n\n\nSeveral debuggers for Python are described below, and the built-in function\n[`breakpoint()`](../library/functions.html#breakpoint \"breakpoint\") allows you to drop into any of them.\n\n\nThe pdb module is a simple but adequate console-mode debugger for Python. It is\npart of the standard Python library, and is [`documented in the Library\nReference Manual`](../library/pdb.html#module-pdb \"pdb: The Python debugger for interactive interpreters.\"). You can also write your own debugger by using the code\nfor pdb as an example.\n\n\nThe IDLE interactive development environment, which is part of the standard\nPython distribution (normally available as\n[Tools/scripts/idle3](https://github.com/python/cpython/blob/main/Tools/scripts/idle3)),\nincludes a graphical debugger.\n\n\nPythonWin is a Python IDE that includes a GUI debugger based on pdb. The\nPythonWin debugger colors breakpoints and has quite a few cool features such as\ndebugging non-PythonWin programs. PythonWin is available as part of\n[pywin32](https://github.com/mhammond/pywin32) project and\nas a part of the\n[ActivePython](https://www.activestate.com/products/python/) distribution.\n\n\n[Eric](https://eric-ide.python-projects.org/) is an IDE built on PyQt\nand the Scintilla editing component.\n\n\n[trepan3k](https://github.com/rocky/python3-trepan/) is a gdb-like debugger.\n\n\n[Visual Studio Code](https://code.visualstudio.com/) is an IDE with debugging\ntools that integrates with version-control software.\n\n\nThere are a number of commercial Python IDEs that include graphical debuggers.\nThey include:\n\n\n* [Wing IDE](https://wingware.com/)\n* [Komodo IDE](https://www.activestate.com/products/komodo-ide/)\n* [PyCharm](https://www.jetbrains.com/pycharm/)\n\n\n"}
{"instruction": "Are there tools to help find bugs or perform static analysis?\u00b6", "context": "python programming", "output": "\nYes.\n\n\n[Pylint](https://pylint.pycqa.org/en/latest/index.html) and\n[Pyflakes](https://github.com/PyCQA/pyflakes) do basic checking that will\nhelp you catch bugs sooner.\n\n\nStatic type checkers such as [Mypy](https://mypy-lang.org/),\n[Pyre](https://pyre-check.org/), and\n[Pytype](https://github.com/google/pytype) can check type hints in Python\nsource code.\n\n\n"}
{"instruction": "How can I create a stand-alone binary from a Python script?\u00b6", "context": "python programming", "output": "\nYou don\u2019t need the ability to compile Python to C code if all you want is a\nstand-alone program that users can download and run without having to install\nthe Python distribution first. There are a number of tools that determine the\nset of modules required by a program and bind these modules together with a\nPython binary to produce a single executable.\n\n\nOne is to use the freeze tool, which is included in the Python source tree as\n[Tools/freeze](https://github.com/python/cpython/tree/main/Tools/freeze).\nIt converts Python byte code to C arrays; with a C compiler you can\nembed all your modules into a new program, which is then linked with the\nstandard Python modules.\n\n\nIt works by scanning your source recursively for import statements (in both\nforms) and looking for the modules in the standard Python path as well as in the\nsource directory (for built-in modules). It then turns the bytecode for modules\nwritten in Python into C code (array initializers that can be turned into code\nobjects using the marshal module) and creates a custom-made config file that\nonly contains those built-in modules which are actually used in the program. It\nthen compiles the generated C code and links it with the rest of the Python\ninterpreter to form a self-contained binary which acts exactly like your script.\n\n\nThe following packages can help with the creation of console and GUI\nexecutables:\n\n\n* [Nuitka](https://nuitka.net/) (Cross-platform)\n* [PyInstaller](https://pyinstaller.org/) (Cross-platform)\n* [PyOxidizer](https://pyoxidizer.readthedocs.io/en/stable/) (Cross-platform)\n* [cx\\_Freeze](https://marcelotduarte.github.io/cx_Freeze/) (Cross-platform)\n* [py2app](https://github.com/ronaldoussoren/py2app) (macOS only)\n* [py2exe](https://www.py2exe.org/) (Windows only)\n\n\n"}
{"instruction": "Are there coding standards or a style guide for Python programs?\u00b6", "context": "python programming", "output": "\nYes. The coding style required for standard library modules is documented as\n[**PEP 8**](https://peps.python.org/pep-0008/).\n\n\n"}
{"instruction": "Why am I getting an UnboundLocalError when the variable has a value?\u00b6", "context": "python programming", "output": "\nIt can be a surprise to get the [`UnboundLocalError`](../library/exceptions.html#UnboundLocalError \"UnboundLocalError\") in previously working\ncode when it is modified by adding an assignment statement somewhere in\nthe body of a function.\n\n\nThis code:\n\n\n\n```\n>>> x = 10\n>>> def bar():\n...     print(x)\n...\n>>> bar()\n10\n\n```\n\n\nworks, but this code:\n\n\n\n```\n>>> x = 10\n>>> def foo():\n...     print(x)\n...     x += 1\n\n```\n\n\nresults in an `UnboundLocalError`:\n\n\n\n```\n>>> foo()\nTraceback (most recent call last):\n  ...\nUnboundLocalError: local variable 'x' referenced before assignment\n\n```\n\n\nThis is because when you make an assignment to a variable in a scope, that\nvariable becomes local to that scope and shadows any similarly named variable\nin the outer scope. Since the last statement in foo assigns a new value to\n`x`, the compiler recognizes it as a local variable. Consequently when the\nearlier `print(x)` attempts to print the uninitialized local variable and\nan error results.\n\n\nIn the example above you can access the outer scope variable by declaring it\nglobal:\n\n\n\n```\n>>> x = 10\n>>> def foobar():\n...     global x\n...     print(x)\n...     x += 1\n...\n>>> foobar()\n10\n\n```\n\n\nThis explicit declaration is required in order to remind you that (unlike the\nsuperficially analogous situation with class and instance variables) you are\nactually modifying the value of the variable in the outer scope:\n\n\n\n```\n>>> print(x)\n11\n\n```\n\n\nYou can do a similar thing in a nested scope using the [`nonlocal`](../reference/simple_stmts.html#nonlocal)\nkeyword:\n\n\n\n```\n>>> def foo():\n...    x = 10\n...    def bar():\n...        nonlocal x\n...        print(x)\n...        x += 1\n...    bar()\n...    print(x)\n...\n>>> foo()\n10\n11\n\n```\n\n\n"}
{"instruction": "What are the rules for local and global variables in Python?\u00b6", "context": "python programming", "output": "\nIn Python, variables that are only referenced inside a function are implicitly\nglobal. If a variable is assigned a value anywhere within the function\u2019s body,\nit\u2019s assumed to be a local unless explicitly declared as global.\n\n\nThough a bit surprising at first, a moment\u2019s consideration explains this. On\none hand, requiring [`global`](../reference/simple_stmts.html#global) for assigned variables provides a bar\nagainst unintended side-effects. On the other hand, if `global` was required\nfor all global references, you\u2019d be using `global` all the time. You\u2019d have\nto declare as global every reference to a built-in function or to a component of\nan imported module. This clutter would defeat the usefulness of the `global`\ndeclaration for identifying side-effects.\n\n\n"}
{"instruction": "Why do lambdas defined in a loop with different values all return the same result?\u00b6", "context": "python programming", "output": "\nAssume you use a for loop to define a few different lambdas (or even plain\nfunctions), e.g.:\n\n\n\n```\n>>> squares = []\n>>> for x in range(5):\n...     squares.append(lambda: x\\*\\*2)\n\n```\n\n\nThis gives you a list that contains 5 lambdas that calculate `x\\*\\*2`. You\nmight expect that, when called, they would return, respectively, `0`, `1`,\n`4`, `9`, and `16`. However, when you actually try you will see that\nthey all return `16`:\n\n\n\n```\n>>> squares[2]()\n16\n>>> squares[4]()\n16\n\n```\n\n\nThis happens because `x` is not local to the lambdas, but is defined in\nthe outer scope, and it is accessed when the lambda is called \u2014 not when it\nis defined. At the end of the loop, the value of `x` is `4`, so all the\nfunctions now return `4\\*\\*2`, i.e. `16`. You can also verify this by\nchanging the value of `x` and see how the results of the lambdas change:\n\n\n\n```\n>>> x = 8\n>>> squares[2]()\n64\n\n```\n\n\nIn order to avoid this, you need to save the values in variables local to the\nlambdas, so that they don\u2019t rely on the value of the global `x`:\n\n\n\n```\n>>> squares = []\n>>> for x in range(5):\n...     squares.append(lambda n=x: n\\*\\*2)\n\n```\n\n\nHere, `n=x` creates a new variable `n` local to the lambda and computed\nwhen the lambda is defined so that it has the same value that `x` had at\nthat point in the loop. This means that the value of `n` will be `0`\nin the first lambda, `1` in the second, `2` in the third, and so on.\nTherefore each lambda will now return the correct result:\n\n\n\n```\n>>> squares[2]()\n4\n>>> squares[4]()\n16\n\n```\n\n\nNote that this behaviour is not peculiar to lambdas, but applies to regular\nfunctions too.\n\n\n"}
{"instruction": "How do I share global variables across modules?\u00b6", "context": "python programming", "output": "\nThe canonical way to share information across modules within a single program is\nto create a special module (often called config or cfg). Just import the config\nmodule in all modules of your application; the module then becomes available as\na global name. Because there is only one instance of each module, any changes\nmade to the module object get reflected everywhere. For example:\n\n\nconfig.py:\n\n\n\n```\nx = 0   # Default value of the 'x' configuration setting\n\n```\n\n\nmod.py:\n\n\n\n```\nimport config\nconfig.x = 1\n\n```\n\n\nmain.py:\n\n\n\n```\nimport config\nimport mod\nprint(config.x)\n\n```\n\n\nNote that using a module is also the basis for implementing the singleton design\npattern, for the same reason.\n\n\n"}
{"instruction": "What are the \u201cbest practices\u201d for using import in a module?\u00b6", "context": "python programming", "output": "\nIn general, don\u2019t use `from modulename import \\*`. Doing so clutters the\nimporter\u2019s namespace, and makes it much harder for linters to detect undefined\nnames.\n\n\nImport modules at the top of a file. Doing so makes it clear what other modules\nyour code requires and avoids questions of whether the module name is in scope.\nUsing one import per line makes it easy to add and delete module imports, but\nusing multiple imports per line uses less screen space.\n\n\nIt\u2019s good practice if you import modules in the following order:\n\n\n1. standard library modules \u2013 e.g. [`sys`](../library/sys.html#module-sys \"sys: Access system-specific parameters and functions.\"), [`os`](../library/os.html#module-os \"os: Miscellaneous operating system interfaces.\"), [`argparse`](../library/argparse.html#module-argparse \"argparse: Command-line option and argument parsing library.\"), [`re`](../library/re.html#module-re \"re: Regular expression operations.\")\n2. third-party library modules (anything installed in Python\u2019s site-packages\ndirectory) \u2013 e.g. `dateutil`, `requests`, `PIL.Image`\n3. locally developed modules\n\n\nIt is sometimes necessary to move imports to a function or class to avoid\nproblems with circular imports. Gordon McMillan says:\n\n\n\n> \n> Circular imports are fine where both modules use the \u201cimport <module>\u201d form\n> of import. They fail when the 2nd module wants to grab a name out of the\n> first (\u201cfrom module import name\u201d) and the import is at the top level. That\u2019s\n> because names in the 1st are not yet available, because the first module is\n> busy importing the 2nd.\n> \n> \n> \n\n\nIn this case, if the second module is only used in one function, then the import\ncan easily be moved into that function. By the time the import is called, the\nfirst module will have finished initializing, and the second module can do its\nimport.\n\n\nIt may also be necessary to move imports out of the top level of code if some of\nthe modules are platform-specific. In that case, it may not even be possible to\nimport all of the modules at the top of the file. In this case, importing the\ncorrect modules in the corresponding platform-specific code is a good option.\n\n\nOnly move imports into a local scope, such as inside a function definition, if\nit\u2019s necessary to solve a problem such as avoiding a circular import or are\ntrying to reduce the initialization time of a module. This technique is\nespecially helpful if many of the imports are unnecessary depending on how the\nprogram executes. You may also want to move imports into a function if the\nmodules are only ever used in that function. Note that loading a module the\nfirst time may be expensive because of the one time initialization of the\nmodule, but loading a module multiple times is virtually free, costing only a\ncouple of dictionary lookups. Even if the module name has gone out of scope,\nthe module is probably available in [`sys.modules`](../library/sys.html#sys.modules \"sys.modules\").\n\n\n"}
{"instruction": "Why are default values shared between objects?\u00b6", "context": "python programming", "output": "\nThis type of bug commonly bites neophyte programmers. Consider this function:\n\n\n\n```\ndef foo(mydict={}):  # Danger: shared reference to one dict for all calls\n    ... compute something ...\n    mydict[key] = value\n    return mydict\n\n```\n\n\nThe first time you call this function, `mydict` contains a single item. The\nsecond time, `mydict` contains two items because when `foo()` begins\nexecuting, `mydict` starts out with an item already in it.\n\n\nIt is often expected that a function call creates new objects for default\nvalues. This is not what happens. Default values are created exactly once, when\nthe function is defined. If that object is changed, like the dictionary in this\nexample, subsequent calls to the function will refer to this changed object.\n\n\nBy definition, immutable objects such as numbers, strings, tuples, and `None`,\nare safe from change. Changes to mutable objects such as dictionaries, lists,\nand class instances can lead to confusion.\n\n\nBecause of this feature, it is good programming practice to not use mutable\nobjects as default values. Instead, use `None` as the default value and\ninside the function, check if the parameter is `None` and create a new\nlist/dictionary/whatever if it is. For example, don\u2019t write:\n\n\n\n```\ndef foo(mydict={}):\n    ...\n\n```\n\n\nbut:\n\n\n\n```\ndef foo(mydict=None):\n    if mydict is None:\n        mydict = {}  # create a new dict for local namespace\n\n```\n\n\nThis feature can be useful. When you have a function that\u2019s time-consuming to\ncompute, a common technique is to cache the parameters and the resulting value\nof each call to the function, and return the cached value if the same value is\nrequested again. This is called \u201cmemoizing\u201d, and can be implemented like this:\n\n\n\n```\n# Callers can only provide two parameters and optionally pass \\_cache by keyword\ndef expensive(arg1, arg2, \\*, \\_cache={}):\n    if (arg1, arg2) in \\_cache:\n        return \\_cache[(arg1, arg2)]\n\n    # Calculate the value\n    result = ... expensive computation ...\n    \\_cache[(arg1, arg2)] = result           # Store result in the cache\n    return result\n\n```\n\n\nYou could use a global variable containing a dictionary instead of the default\nvalue; it\u2019s a matter of taste.\n\n\n"}
{"instruction": "How can I pass optional or keyword parameters from one function to another?\u00b6", "context": "python programming", "output": "\nCollect the arguments using the `\\*` and `\\*\\*` specifiers in the function\u2019s\nparameter list; this gives you the positional arguments as a tuple and the\nkeyword arguments as a dictionary. You can then pass these arguments when\ncalling another function by using `\\*` and `\\*\\*`:\n\n\n\n```\ndef f(x, \\*args, \\*\\*kwargs):\n    ...\n    kwargs['width'] = '14.3c'\n    ...\n    g(x, \\*args, \\*\\*kwargs)\n\n```\n\n\n"}
{"instruction": "What is the difference between arguments and parameters?\u00b6", "context": "python programming", "output": "\n[Parameters](../glossary.html#term-parameter) are defined by the names that appear in a\nfunction definition, whereas [arguments](../glossary.html#term-argument) are the values\nactually passed to a function when calling it. Parameters define what\n[kind of arguments](../glossary.html#term-parameter) a function can accept. For\nexample, given the function definition:\n\n\n\n```\ndef func(foo, bar=None, \\*\\*kwargs):\n    pass\n\n```\n\n\n*foo*, *bar* and *kwargs* are parameters of `func`. However, when calling\n`func`, for example:\n\n\n\n```\nfunc(42, bar=314, extra=somevar)\n\n```\n\n\nthe values `42`, `314`, and `somevar` are arguments.\n\n\n"}
{"instruction": "Why did changing list \u2018y\u2019 also change list \u2018x\u2019?\u00b6", "context": "python programming", "output": "\nIf you wrote code like:\n\n\n\n```\n>>> x = []\n>>> y = x\n>>> y.append(10)\n>>> y\n[10]\n>>> x\n[10]\n\n```\n\n\nyou might be wondering why appending an element to `y` changed `x` too.\n\n\nThere are two factors that produce this result:\n\n\n1. Variables are simply names that refer to objects. Doing `y = x` doesn\u2019t\ncreate a copy of the list \u2013 it creates a new variable `y` that refers to\nthe same object `x` refers to. This means that there is only one object\n(the list), and both `x` and `y` refer to it.\n2. Lists are [mutable](../glossary.html#term-mutable), which means that you can change their content.\n\n\nAfter the call to `append()`, the content of the mutable object has\nchanged from `[]` to `[10]`. Since both the variables refer to the same\nobject, using either name accesses the modified value `[10]`.\n\n\nIf we instead assign an immutable object to `x`:\n\n\n\n```\n>>> x = 5  # ints are immutable\n>>> y = x\n>>> x = x + 1  # 5 can't be mutated, we are creating a new object here\n>>> x\n6\n>>> y\n5\n\n```\n\n\nwe can see that in this case `x` and `y` are not equal anymore. This is\nbecause integers are [immutable](../glossary.html#term-immutable), and when we do `x = x + 1` we are not\nmutating the int `5` by incrementing its value; instead, we are creating a\nnew object (the int `6`) and assigning it to `x` (that is, changing which\nobject `x` refers to). After this assignment we have two objects (the ints\n`6` and `5`) and two variables that refer to them (`x` now refers to\n`6` but `y` still refers to `5`).\n\n\nSome operations (for example `y.append(10)` and `y.sort()`) mutate the\nobject, whereas superficially similar operations (for example `y = y + [10]`\nand [`sorted(y)`](../library/functions.html#sorted \"sorted\")) create a new object. In general in Python (and in all cases\nin the standard library) a method that mutates an object will return `None`\nto help avoid getting the two types of operations confused. So if you\nmistakenly write `y.sort()` thinking it will give you a sorted copy of `y`,\nyou\u2019ll instead end up with `None`, which will likely cause your program to\ngenerate an easily diagnosed error.\n\n\nHowever, there is one class of operations where the same operation sometimes\nhas different behaviors with different types: the augmented assignment\noperators. For example, `+=` mutates lists but not tuples or ints (`a\\_list\n+= [1, 2, 3]` is equivalent to `a\\_list.extend([1, 2, 3])` and mutates\n`a\\_list`, whereas `some\\_tuple += (1, 2, 3)` and `some\\_int += 1` create\nnew objects).\n\n\nIn other words:\n\n\n* If we have a mutable object ([`list`](../library/stdtypes.html#list \"list\"), [`dict`](../library/stdtypes.html#dict \"dict\"), [`set`](../library/stdtypes.html#set \"set\"),\netc.), we can use some specific operations to mutate it and all the variables\nthat refer to it will see the change.\n* If we have an immutable object ([`str`](../library/stdtypes.html#str \"str\"), [`int`](../library/functions.html#int \"int\"), [`tuple`](../library/stdtypes.html#tuple \"tuple\"),\netc.), all the variables that refer to it will always see the same value,\nbut operations that transform that value into a new value always return a new\nobject.\n\n\nIf you want to know if two variables refer to the same object or not, you can\nuse the [`is`](../reference/expressions.html#is) operator, or the built-in function [`id()`](../library/functions.html#id \"id\").\n\n\n"}
{"instruction": "How do I write a function with output parameters (call by reference)?\u00b6", "context": "python programming", "output": "\nRemember that arguments are passed by assignment in Python. Since assignment\njust creates references to objects, there\u2019s no alias between an argument name in\nthe caller and callee, and so no call-by-reference per se. You can achieve the\ndesired effect in a number of ways.\n\n\n1. By returning a tuple of the results:\n\n\n\n```\n>>> def func1(a, b):\n...     a = 'new-value'        # a and b are local names\n...     b = b + 1              # assigned to new objects\n...     return a, b            # return new values\n...\n>>> x, y = 'old-value', 99\n>>> func1(x, y)\n('new-value', 100)\n\n```\n\n\nThis is almost always the clearest solution.\n2. By using global variables. This isn\u2019t thread-safe, and is not recommended.\n3. By passing a mutable (changeable in-place) object:\n\n\n\n```\n>>> def func2(a):\n...     a[0] = 'new-value'     # 'a' references a mutable list\n...     a[1] = a[1] + 1        # changes a shared object\n...\n>>> args = ['old-value', 99]\n>>> func2(args)\n>>> args\n['new-value', 100]\n\n```\n4. By passing in a dictionary that gets mutated:\n\n\n\n```\n>>> def func3(args):\n...     args['a'] = 'new-value'     # args is a mutable dictionary\n...     args['b'] = args['b'] + 1   # change it in-place\n...\n>>> args = {'a': 'old-value', 'b': 99}\n>>> func3(args)\n>>> args\n{'a': 'new-value', 'b': 100}\n\n```\n5. Or bundle up values in a class instance:\n\n\n\n```\n>>> class Namespace:\n...     def \\_\\_init\\_\\_(self, /, \\*\\*args):\n...         for key, value in args.items():\n...             setattr(self, key, value)\n...\n>>> def func4(args):\n...     args.a = 'new-value'        # args is a mutable Namespace\n...     args.b = args.b + 1         # change object in-place\n...\n>>> args = Namespace(a='old-value', b=99)\n>>> func4(args)\n>>> vars(args)\n{'a': 'new-value', 'b': 100}\n\n```\n\n\nThere\u2019s almost never a good reason to get this complicated.\n\n\nYour best choice is to return a tuple containing the multiple results.\n\n\n"}
{"instruction": "How do you make a higher order function in Python?\u00b6", "context": "python programming", "output": "\nYou have two choices: you can use nested scopes or you can use callable objects.\nFor example, suppose you wanted to define `linear(a,b)` which returns a\nfunction `f(x)` that computes the value `a\\*x+b`. Using nested scopes:\n\n\n\n```\ndef linear(a, b):\n    def result(x):\n        return a \\* x + b\n    return result\n\n```\n\n\nOr using a callable object:\n\n\n\n```\nclass linear:\n\n    def \\_\\_init\\_\\_(self, a, b):\n        self.a, self.b = a, b\n\n    def \\_\\_call\\_\\_(self, x):\n        return self.a \\* x + self.b\n\n```\n\n\nIn both cases,\n\n\n\n```\ntaxes = linear(0.3, 2)\n\n```\n\n\ngives a callable object where `taxes(10e6) == 0.3 \\* 10e6 + 2`.\n\n\nThe callable object approach has the disadvantage that it is a bit slower and\nresults in slightly longer code. However, note that a collection of callables\ncan share their signature via inheritance:\n\n\n\n```\nclass exponential(linear):\n    # \\_\\_init\\_\\_ inherited\n    def \\_\\_call\\_\\_(self, x):\n        return self.a \\* (x \\*\\* self.b)\n\n```\n\n\nObject can encapsulate state for several methods:\n\n\n\n```\nclass counter:\n\n    value = 0\n\n    def set(self, x):\n        self.value = x\n\n    def up(self):\n        self.value = self.value + 1\n\n    def down(self):\n        self.value = self.value - 1\n\ncount = counter()\ninc, dec, reset = count.up, count.down, count.set\n\n```\n\n\nHere `inc()`, `dec()` and `reset()` act like functions which share the\nsame counting variable.\n\n\n"}
{"instruction": "How do I copy an object in Python?\u00b6", "context": "python programming", "output": "\nIn general, try [`copy.copy()`](../library/copy.html#copy.copy \"copy.copy\") or [`copy.deepcopy()`](../library/copy.html#copy.deepcopy \"copy.deepcopy\") for the general case.\nNot all objects can be copied, but most can.\n\n\nSome objects can be copied more easily. Dictionaries have a [`copy()`](../library/stdtypes.html#dict.copy \"dict.copy\")\nmethod:\n\n\n\n```\nnewdict = olddict.copy()\n\n```\n\n\nSequences can be copied by slicing:\n\n\n\n```\nnew\\_l = l[:]\n\n```\n\n\n"}
{"instruction": "How can I find the methods or attributes of an object?\u00b6", "context": "python programming", "output": "\nFor an instance `x` of a user-defined class, [`dir(x)`](../library/functions.html#dir \"dir\") returns an alphabetized\nlist of the names containing the instance attributes and methods and attributes\ndefined by its class.\n\n\n"}
{"instruction": "How can my code discover the name of an object?\u00b6", "context": "python programming", "output": "\nGenerally speaking, it can\u2019t, because objects don\u2019t really have names.\nEssentially, assignment always binds a name to a value; the same is true of\n`def` and `class` statements, but in that case the value is a\ncallable. Consider the following code:\n\n\n\n```\n>>> class A:\n...     pass\n...\n>>> B = A\n>>> a = B()\n>>> b = a\n>>> print(b)\n<\\_\\_main\\_\\_.A object at 0x16D07CC>\n>>> print(a)\n<\\_\\_main\\_\\_.A object at 0x16D07CC>\n\n```\n\n\nArguably the class has a name: even though it is bound to two names and invoked\nthrough the name `B` the created instance is still reported as an instance of\nclass `A`. However, it is impossible to say whether the instance\u2019s name is `a` or\n`b`, since both names are bound to the same value.\n\n\nGenerally speaking it should not be necessary for your code to \u201cknow the names\u201d\nof particular values. Unless you are deliberately writing introspective\nprograms, this is usually an indication that a change of approach might be\nbeneficial.\n\n\nIn comp.lang.python, Fredrik Lundh once gave an excellent analogy in answer to\nthis question:\n\n\n\n> \n> The same way as you get the name of that cat you found on your porch: the cat\n> (object) itself cannot tell you its name, and it doesn\u2019t really care \u2013 so\n> the only way to find out what it\u2019s called is to ask all your neighbours\n> (namespaces) if it\u2019s their cat (object)\u2026\n> \n> \n> \u2026.and don\u2019t be surprised if you\u2019ll find that it\u2019s known by many names, or\n> no name at all!\n> \n> \n> \n\n\n"}
{"instruction": "What\u2019s up with the comma operator\u2019s precedence?\u00b6", "context": "python programming", "output": "\nComma is not an operator in Python. Consider this session:\n\n\n\n```\n>>> \"a\" in \"b\", \"a\"\n(False, 'a')\n\n```\n\n\nSince the comma is not an operator, but a separator between expressions the\nabove is evaluated as if you had entered:\n\n\n\n```\n(\"a\" in \"b\"), \"a\"\n\n```\n\n\nnot:\n\n\n\n```\n\"a\" in (\"b\", \"a\")\n\n```\n\n\nThe same is true of the various assignment operators (`=`, `+=` etc). They\nare not truly operators but syntactic delimiters in assignment statements.\n\n\n"}
{"instruction": "Is there an equivalent of C\u2019s \u201c?:\u201d ternary operator?\u00b6", "context": "python programming", "output": "\nYes, there is. The syntax is as follows:\n\n\n\n```\n[on\\_true] if [expression] else [on\\_false]\n\nx, y = 50, 25\nsmall = x if x < y else y\n\n```\n\n\nBefore this syntax was introduced in Python 2.5, a common idiom was to use\nlogical operators:\n\n\n\n```\n[expression] and [on\\_true] or [on\\_false]\n\n```\n\n\nHowever, this idiom is unsafe, as it can give wrong results when *on\\_true*\nhas a false boolean value. Therefore, it is always better to use\nthe `... if ... else ...` form.\n\n\n"}
{"instruction": "Is it possible to write obfuscated one-liners in Python?\u00b6", "context": "python programming", "output": "\nYes. Usually this is done by nesting [`lambda`](../reference/expressions.html#lambda) within\n`lambda`. See the following three examples, slightly adapted from Ulf Bartelt:\n\n\n\n```\nfrom functools import reduce\n\n# Primes < 1000\nprint(list(filter(None,map(lambda y:y\\*reduce(lambda x,y:x\\*y!=0,\nmap(lambda x,y=y:y%x,range(2,int(pow(y,0.5)+1))),1),range(2,1000)))))\n\n# First 10 Fibonacci numbers\nprint(list(map(lambda x,f=lambda x,f:(f(x-1,f)+f(x-2,f)) if x>1 else 1:\nf(x,f), range(10))))\n\n# Mandelbrot set\nprint((lambda Ru,Ro,Iu,Io,IM,Sx,Sy:reduce(lambda x,y:x+'\\n'+y,map(lambda y,\nIu=Iu,Io=Io,Ru=Ru,Ro=Ro,Sy=Sy,L=lambda yc,Iu=Iu,Io=Io,Ru=Ru,Ro=Ro,i=IM,\nSx=Sx,Sy=Sy:reduce(lambda x,y:x+y,map(lambda x,xc=Ru,yc=yc,Ru=Ru,Ro=Ro,\ni=i,Sx=Sx,F=lambda xc,yc,x,y,k,f=lambda xc,yc,x,y,k,f:(k<=0)or (x\\*x+y\\*y\n>=4.0) or 1+f(xc,yc,x\\*x-y\\*y+xc,2.0\\*x\\*y+yc,k-1,f):f(xc,yc,x,y,k,f):chr(\n64+F(Ru+x\\*(Ro-Ru)/Sx,yc,0,0,i)),range(Sx))):L(Iu+y\\*(Io-Iu)/Sy),range(Sy\n))))(-2.1, 0.7, -1.2, 1.2, 30, 80, 24))\n# \\\\_\\_\\_ \\_\\_\\_/ \\\\_\\_\\_ \\_\\_\\_/ | | |\\_\\_ lines on screen\n# V V | |\\_\\_\\_\\_\\_\\_ columns on screen\n# | | |\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_ maximum of \"iterations\"\n# | |\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_ range on y axis\n# |\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_ range on x axis\n\n```\n\n\nDon\u2019t try this at home, kids!\n\n\n"}
{"instruction": "What does the slash(/) in the parameter list of a function mean?\u00b6", "context": "python programming", "output": "\nA slash in the argument list of a function denotes that the parameters prior to\nit are positional-only. Positional-only parameters are the ones without an\nexternally usable name. Upon calling a function that accepts positional-only\nparameters, arguments are mapped to parameters based solely on their position.\nFor example, [`divmod()`](../library/functions.html#divmod \"divmod\") is a function that accepts positional-only\nparameters. Its documentation looks like this:\n\n\n\n```\n>>> help(divmod)\nHelp on built-in function divmod in module builtins:\n\ndivmod(x, y, /)\n Return the tuple (x//y, x%y). Invariant: div\\*y + mod == x.\n\n```\n\n\nThe slash at the end of the parameter list means that both parameters are\npositional-only. Thus, calling [`divmod()`](../library/functions.html#divmod \"divmod\") with keyword arguments would lead\nto an error:\n\n\n\n```\n>>> divmod(x=3, y=4)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: divmod() takes no keyword arguments\n\n```\n\n\n"}
{"instruction": "How do I specify hexadecimal and octal integers?\u00b6", "context": "python programming", "output": "\nTo specify an octal digit, precede the octal value with a zero, and then a lower\nor uppercase \u201co\u201d. For example, to set the variable \u201ca\u201d to the octal value \u201c10\u201d\n(8 in decimal), type:\n\n\n\n```\n>>> a = 0o10\n>>> a\n8\n\n```\n\n\nHexadecimal is just as easy. Simply precede the hexadecimal number with a zero,\nand then a lower or uppercase \u201cx\u201d. Hexadecimal digits can be specified in lower\nor uppercase. For example, in the Python interpreter:\n\n\n\n```\n>>> a = 0xa5\n>>> a\n165\n>>> b = 0XB2\n>>> b\n178\n\n```\n\n\n"}
{"instruction": "Why does -22 // 10 return -3?\u00b6", "context": "python programming", "output": "\nIt\u2019s primarily driven by the desire that `i % j` have the same sign as `j`.\nIf you want that, and also want:\n\n\n\n```\ni == (i // j) \\* j + (i % j)\n\n```\n\n\nthen integer division has to return the floor. C also requires that identity to\nhold, and then compilers that truncate `i // j` need to make `i % j` have\nthe same sign as `i`.\n\n\nThere are few real use cases for `i % j` when `j` is negative. When `j`\nis positive, there are many, and in virtually all of them it\u2019s more useful for\n`i % j` to be `>= 0`. If the clock says 10 now, what did it say 200 hours\nago? `-190 % 12 == 2` is useful; `-190 % 12 == -10` is a bug waiting to\nbite.\n\n\n"}
{"instruction": "How do I get int literal attribute instead of SyntaxError?\u00b6", "context": "python programming", "output": "\nTrying to lookup an `int` literal attribute in the normal manner gives\na [`SyntaxError`](../library/exceptions.html#SyntaxError \"SyntaxError\") because the period is seen as a decimal point:\n\n\n\n```\n>>> 1.\\_\\_class\\_\\_\n  File \"<stdin>\", line 1\n 1.\\_\\_class\\_\\_\n ^\nSyntaxError: invalid decimal literal\n\n```\n\n\nThe solution is to separate the literal from the period\nwith either a space or parentheses.\n\n\n\n```\n>>> 1 .\\_\\_class\\_\\_\n<class 'int'>\n>>> (1).\\_\\_class\\_\\_\n<class 'int'>\n\n```\n\n\n"}
{"instruction": "How do I convert a string to a number?\u00b6", "context": "python programming", "output": "\nFor integers, use the built-in [`int()`](../library/functions.html#int \"int\") type constructor, e.g. `int('144')\n== 144`. Similarly, [`float()`](../library/functions.html#float \"float\") converts to floating-point,\ne.g. `float('144') == 144.0`.\n\n\nBy default, these interpret the number as decimal, so that `int('0144') ==\n144` holds true, and `int('0x144')` raises [`ValueError`](../library/exceptions.html#ValueError \"ValueError\"). `int(string,\nbase)` takes the base to convert from as a second optional argument, so `int(\n'0x144', 16) == 324`. If the base is specified as 0, the number is interpreted\nusing Python\u2019s rules: a leading \u20180o\u2019 indicates octal, and \u20180x\u2019 indicates a hex\nnumber.\n\n\nDo not use the built-in function [`eval()`](../library/functions.html#eval \"eval\") if all you need is to convert\nstrings to numbers. [`eval()`](../library/functions.html#eval \"eval\") will be significantly slower and it presents a\nsecurity risk: someone could pass you a Python expression that might have\nunwanted side effects. For example, someone could pass\n`\\_\\_import\\_\\_('os').system(\"rm -rf $HOME\")` which would erase your home\ndirectory.\n\n\n[`eval()`](../library/functions.html#eval \"eval\") also has the effect of interpreting numbers as Python expressions,\nso that e.g. `eval('09')` gives a syntax error because Python does not allow\nleading \u20180\u2019 in a decimal number (except \u20180\u2019).\n\n\n"}
{"instruction": "How do I convert a number to a string?\u00b6", "context": "python programming", "output": "\nTo convert, e.g., the number `144` to the string `'144'`, use the built-in type\nconstructor [`str()`](../library/stdtypes.html#str \"str\"). If you want a hexadecimal or octal representation, use\nthe built-in functions [`hex()`](../library/functions.html#hex \"hex\") or [`oct()`](../library/functions.html#oct \"oct\"). For fancy formatting, see\nthe [Formatted string literals](../reference/lexical_analysis.html#f-strings) and [Format String Syntax](../library/string.html#formatstrings) sections,\ne.g. `\"{:04d}\".format(144)` yields\n`'0144'` and `\"{:.3f}\".format(1.0/3.0)` yields `'0.333'`.\n\n\n"}
{"instruction": "How do I modify a string in place?\u00b6", "context": "python programming", "output": "\nYou can\u2019t, because strings are immutable. In most situations, you should\nsimply construct a new string from the various parts you want to assemble\nit from. However, if you need an object with the ability to modify in-place\nunicode data, try using an [`io.StringIO`](../library/io.html#io.StringIO \"io.StringIO\") object or the [`array`](../library/array.html#module-array \"array: Space efficient arrays of uniformly typed numeric values.\")\nmodule:\n\n\n\n```\n>>> import io\n>>> s = \"Hello, world\"\n>>> sio = io.StringIO(s)\n>>> sio.getvalue()\n'Hello, world'\n>>> sio.seek(7)\n7\n>>> sio.write(\"there!\")\n6\n>>> sio.getvalue()\n'Hello, there!'\n\n>>> import array\n>>> a = array.array('u', s)\n>>> print(a)\narray('u', 'Hello, world')\n>>> a[0] = 'y'\n>>> print(a)\narray('u', 'yello, world')\n>>> a.tounicode()\n'yello, world'\n\n```\n\n\n"}
{"instruction": "How do I use strings to call functions/methods?\u00b6", "context": "python programming", "output": "\nThere are various techniques.\n\n\n* The best is to use a dictionary that maps strings to functions. The primary\nadvantage of this technique is that the strings do not need to match the names\nof the functions. This is also the primary technique used to emulate a case\nconstruct:\n\n\n\n```\ndef a():\n    pass\n\ndef b():\n    pass\n\ndispatch = {'go': a, 'stop': b}  # Note lack of parens for funcs\n\ndispatch[get\\_input()]()  # Note trailing parens to call function\n\n```\n* Use the built-in function [`getattr()`](../library/functions.html#getattr \"getattr\"):\n\n\n\n```\nimport foo\ngetattr(foo, 'bar')()\n\n```\n\n\nNote that [`getattr()`](../library/functions.html#getattr \"getattr\") works on any object, including classes, class\ninstances, modules, and so on.\n\n\nThis is used in several places in the standard library, like this:\n\n\n\n```\nclass Foo:\n    def do\\_foo(self):\n        ...\n\n    def do\\_bar(self):\n        ...\n\nf = getattr(foo\\_instance, 'do\\_' + opname)\nf()\n\n```\n* Use [`locals()`](../library/functions.html#locals \"locals\") to resolve the function name:\n\n\n\n```\ndef myFunc():\n    print(\"hello\")\n\nfname = \"myFunc\"\n\nf = locals()[fname]\nf()\n\n```\n\n\n"}
{"instruction": "Is there an equivalent to Perl\u2019s chomp() for removing trailing newlines from strings?\u00b6", "context": "python programming", "output": "\nYou can use `S.rstrip(\"\\r\\n\")` to remove all occurrences of any line\nterminator from the end of the string `S` without removing other trailing\nwhitespace. If the string `S` represents more than one line, with several\nempty lines at the end, the line terminators for all the blank lines will\nbe removed:\n\n\n\n```\n>>> lines = (\"line 1 \\r\\n\"\n...          \"\\r\\n\"\n...          \"\\r\\n\")\n>>> lines.rstrip(\"\\n\\r\")\n'line 1 '\n\n```\n\n\nSince this is typically only desired when reading text one line at a time, using\n`S.rstrip()` this way works well.\n\n\n"}
{"instruction": "Is there a scanf() or sscanf() equivalent?\u00b6", "context": "python programming", "output": "\nNot as such.\n\n\nFor simple input parsing, the easiest approach is usually to split the line into\nwhitespace-delimited words using the [`split()`](../library/stdtypes.html#str.split \"str.split\") method of string objects\nand then convert decimal strings to numeric values using [`int()`](../library/functions.html#int \"int\") or\n[`float()`](../library/functions.html#float \"float\"). `split()` supports an optional \u201csep\u201d parameter which is useful\nif the line uses something other than whitespace as a separator.\n\n\nFor more complicated input parsing, regular expressions are more powerful\nthan C\u2019s `sscanf` and better suited for the task.\n\n\n"}
{"instruction": "What does \u2018UnicodeDecodeError\u2019 or \u2018UnicodeEncodeError\u2019 error  mean?\u00b6", "context": "python programming", "output": "\nSee the [Unicode HOWTO](../howto/unicode.html#unicode-howto).\n\n\n"}
{"instruction": "Can I end a raw string with an odd number of backslashes?\u00b6", "context": "python programming", "output": "\nA raw string ending with an odd number of backslashes will escape the string\u2019s quote:\n\n\n\n```\n>>> r'C:\\this\\will\\not\\work\\'\n  File \"<stdin>\", line 1\n    r'C:\\this\\will\\not\\work\\'\n         ^\nSyntaxError: unterminated string literal (detected at line 1)\n\n```\n\n\nThere are several workarounds for this. One is to use regular strings and double\nthe backslashes:\n\n\n\n```\n>>> 'C:\\\\this\\\\will\\\\work\\\\'\n'C:\\\\this\\\\will\\\\work\\\\'\n\n```\n\n\nAnother is to concatenate a regular string containing an escaped backslash to the\nraw string:\n\n\n\n```\n>>> r'C:\\this\\will\\work' '\\\\'\n'C:\\\\this\\\\will\\\\work\\\\'\n\n```\n\n\nIt is also possible to use [`os.path.join()`](../library/os.path.html#os.path.join \"os.path.join\") to append a backslash on Windows:\n\n\n\n```\n>>> os.path.join(r'C:\\this\\will\\work', '')\n'C:\\\\this\\\\will\\\\work\\\\'\n\n```\n\n\nNote that while a backslash will \u201cescape\u201d a quote for the purposes of\ndetermining where the raw string ends, no escaping occurs when interpreting the\nvalue of the raw string. That is, the backslash remains present in the value of\nthe raw string:\n\n\n\n```\n>>> r'backslash\\'preserved'\n\"backslash\\\\'preserved\"\n\n```\n\n\nAlso see the specification in the [language reference](../reference/lexical_analysis.html#strings).\n\n\n"}
{"instruction": "My program is too slow. How do I speed it up?\u00b6", "context": "python programming", "output": "\nThat\u2019s a tough one, in general. First, here are a list of things to\nremember before diving further:\n\n\n* Performance characteristics vary across Python implementations. This FAQ\nfocuses on [CPython](../glossary.html#term-CPython).\n* Behaviour can vary across operating systems, especially when talking about\nI/O or multi-threading.\n* You should always find the hot spots in your program *before* attempting to\noptimize any code (see the [`profile`](../library/profile.html#module-profile \"profile: Python source profiler.\") module).\n* Writing benchmark scripts will allow you to iterate quickly when searching\nfor improvements (see the [`timeit`](../library/timeit.html#module-timeit \"timeit: Measure the execution time of small code snippets.\") module).\n* It is highly recommended to have good code coverage (through unit testing\nor any other technique) before potentially introducing regressions hidden\nin sophisticated optimizations.\n\n\nThat being said, there are many tricks to speed up Python code. Here are\nsome general principles which go a long way towards reaching acceptable\nperformance levels:\n\n\n* Making your algorithms faster (or changing to faster ones) can yield\nmuch larger benefits than trying to sprinkle micro-optimization tricks\nall over your code.\n* Use the right data structures. Study documentation for the [Built-in Types](../library/stdtypes.html#bltin-types)\nand the [`collections`](../library/collections.html#module-collections \"collections: Container datatypes\") module.\n* When the standard library provides a primitive for doing something, it is\nlikely (although not guaranteed) to be faster than any alternative you\nmay come up with. This is doubly true for primitives written in C, such\nas builtins and some extension types. For example, be sure to use\neither the [`list.sort()`](../library/stdtypes.html#list.sort \"list.sort\") built-in method or the related [`sorted()`](../library/functions.html#sorted \"sorted\")\nfunction to do sorting (and see the [Sorting HOW TO](../howto/sorting.html#sortinghowto) for examples\nof moderately advanced usage).\n* Abstractions tend to create indirections and force the interpreter to work\nmore. If the levels of indirection outweigh the amount of useful work\ndone, your program will be slower. You should avoid excessive abstraction,\nespecially under the form of tiny functions or methods (which are also often\ndetrimental to readability).\n\n\nIf you have reached the limit of what pure Python can allow, there are tools\nto take you further away. For example, [Cython](https://cython.org) can\ncompile a slightly modified version of Python code into a C extension, and\ncan be used on many different platforms. Cython can take advantage of\ncompilation (and optional type annotations) to make your code significantly\nfaster than when interpreted. If you are confident in your C programming\nskills, you can also [write a C extension module](../extending/index.html#extending-index)\nyourself.\n\n\n\nSee also\n\n\nThe wiki page devoted to [performance tips](https://wiki.python.org/moin/PythonSpeed/PerformanceTips).\n\n\n\n"}
{"instruction": "What is the most efficient way to concatenate many strings together?\u00b6", "context": "python programming", "output": "\n[`str`](../library/stdtypes.html#str \"str\") and [`bytes`](../library/stdtypes.html#bytes \"bytes\") objects are immutable, therefore concatenating\nmany strings together is inefficient as each concatenation creates a new\nobject. In the general case, the total runtime cost is quadratic in the\ntotal string length.\n\n\nTo accumulate many [`str`](../library/stdtypes.html#str \"str\") objects, the recommended idiom is to place\nthem into a list and call [`str.join()`](../library/stdtypes.html#str.join \"str.join\") at the end:\n\n\n\n```\nchunks = []\nfor s in my\\_strings:\n    chunks.append(s)\nresult = ''.join(chunks)\n\n```\n\n\n(another reasonably efficient idiom is to use [`io.StringIO`](../library/io.html#io.StringIO \"io.StringIO\"))\n\n\nTo accumulate many [`bytes`](../library/stdtypes.html#bytes \"bytes\") objects, the recommended idiom is to extend\na [`bytearray`](../library/stdtypes.html#bytearray \"bytearray\") object using in-place concatenation (the `+=` operator):\n\n\n\n```\nresult = bytearray()\nfor b in my\\_bytes\\_objects:\n    result += b\n\n```\n\n\n"}
{"instruction": "How do I convert between tuples and lists?\u00b6", "context": "python programming", "output": "\nThe type constructor `tuple(seq)` converts any sequence (actually, any\niterable) into a tuple with the same items in the same order.\n\n\nFor example, `tuple([1, 2, 3])` yields `(1, 2, 3)` and `tuple('abc')`\nyields `('a', 'b', 'c')`. If the argument is a tuple, it does not make a copy\nbut returns the same object, so it is cheap to call [`tuple()`](../library/stdtypes.html#tuple \"tuple\") when you\naren\u2019t sure that an object is already a tuple.\n\n\nThe type constructor `list(seq)` converts any sequence or iterable into a list\nwith the same items in the same order. For example, `list((1, 2, 3))` yields\n`[1, 2, 3]` and `list('abc')` yields `['a', 'b', 'c']`. If the argument\nis a list, it makes a copy just like `seq[:]` would.\n\n\n"}
{"instruction": "What\u2019s a negative index?\u00b6", "context": "python programming", "output": "\nPython sequences are indexed with positive numbers and negative numbers. For\npositive numbers 0 is the first index 1 is the second index and so forth. For\nnegative indices -1 is the last index and -2 is the penultimate (next to last)\nindex and so forth. Think of `seq[-n]` as the same as `seq[len(seq)-n]`.\n\n\nUsing negative indices can be very convenient. For example `S[:-1]` is all of\nthe string except for its last character, which is useful for removing the\ntrailing newline from a string.\n\n\n"}
{"instruction": "How do I iterate over a sequence in reverse order?\u00b6", "context": "python programming", "output": "\nUse the [`reversed()`](../library/functions.html#reversed \"reversed\") built-in function:\n\n\n\n```\nfor x in reversed(sequence):\n    ...  # do something with x ...\n\n```\n\n\nThis won\u2019t touch your original sequence, but build a new copy with reversed\norder to iterate over.\n\n\n"}
{"instruction": "How do you remove duplicates from a list?\u00b6", "context": "python programming", "output": "\nSee the Python Cookbook for a long discussion of many ways to do this:\n\n\n\n> \n> <https://code.activestate.com/recipes/52560/>\n> \n> \n> \n\n\nIf you don\u2019t mind reordering the list, sort it and then scan from the end of the\nlist, deleting duplicates as you go:\n\n\n\n```\nif mylist:\n    mylist.sort()\n    last = mylist[-1]\n    for i in range(len(mylist)-2, -1, -1):\n        if last == mylist[i]:\n            del mylist[i]\n        else:\n            last = mylist[i]\n\n```\n\n\nIf all elements of the list may be used as set keys (i.e. they are all\n[hashable](../glossary.html#term-hashable)) this is often faster\n\n\n\n```\nmylist = list(set(mylist))\n\n```\n\n\nThis converts the list into a set, thereby removing duplicates, and then back\ninto a list.\n\n\n"}
{"instruction": "How do you remove multiple items from a list\u00b6", "context": "python programming", "output": "\nAs with removing duplicates, explicitly iterating in reverse with a\ndelete condition is one possibility. However, it is easier and faster\nto use slice replacement with an implicit or explicit forward iteration.\nHere are three variations.:\n\n\n\n```\nmylist[:] = filter(keep\\_function, mylist)\nmylist[:] = (x for x in mylist if keep\\_condition)\nmylist[:] = [x for x in mylist if keep\\_condition]\n\n```\n\n\nThe list comprehension may be fastest.\n\n\n"}
{"instruction": "How do you make an array in Python?\u00b6", "context": "python programming", "output": "\nUse a list:\n\n\n\n```\n[\"this\", 1, \"is\", \"an\", \"array\"]\n\n```\n\n\nLists are equivalent to C or Pascal arrays in their time complexity; the primary\ndifference is that a Python list can contain objects of many different types.\n\n\nThe `array` module also provides methods for creating arrays of fixed types\nwith compact representations, but they are slower to index than lists. Also\nnote that [NumPy](https://numpy.org/)\nand other third party packages define array-like structures with\nvarious characteristics as well.\n\n\nTo get Lisp-style linked lists, you can emulate *cons cells* using tuples:\n\n\n\n```\nlisp\\_list = (\"like\",  (\"this\",  (\"example\", None) ) )\n\n```\n\n\nIf mutability is desired, you could use lists instead of tuples. Here the\nanalogue of a Lisp *car* is `lisp\\_list[0]` and the analogue of *cdr* is\n`lisp\\_list[1]`. Only do this if you\u2019re sure you really need to, because it\u2019s\nusually a lot slower than using Python lists.\n\n\n"}
{"instruction": "How do I create a multidimensional list?\u00b6", "context": "python programming", "output": "\nYou probably tried to make a multidimensional array like this:\n\n\n\n```\n>>> A = [[None] \\* 2] \\* 3\n\n```\n\n\nThis looks correct if you print it:\n\n\n\n```\n>>> A\n[[None, None], [None, None], [None, None]]\n\n```\n\n\nBut when you assign a value, it shows up in multiple places:\n\n\n\n```\n>>> A[0][0] = 5\n>>> A\n[[5, None], [5, None], [5, None]]\n\n```\n\n\nThe reason is that replicating a list with `\\*` doesn\u2019t create copies, it only\ncreates references to the existing objects. The `\\*3` creates a list\ncontaining 3 references to the same list of length two. Changes to one row will\nshow in all rows, which is almost certainly not what you want.\n\n\nThe suggested approach is to create a list of the desired length first and then\nfill in each element with a newly created list:\n\n\n\n```\nA = [None] \\* 3\nfor i in range(3):\n    A[i] = [None] \\* 2\n\n```\n\n\nThis generates a list containing 3 different lists of length two. You can also\nuse a list comprehension:\n\n\n\n```\nw, h = 2, 3\nA = [[None] \\* w for i in range(h)]\n\n```\n\n\nOr, you can use an extension that provides a matrix datatype; [NumPy](https://numpy.org/) is the best known.\n\n\n"}
{"instruction": "How do I apply a method or function to a sequence of objects?\u00b6", "context": "python programming", "output": "\nTo call a method or function and accumulate the return values is a list,\na [list comprehension](../glossary.html#term-list-comprehension) is an elegant solution:\n\n\n\n```\nresult = [obj.method() for obj in mylist]\n\nresult = [function(obj) for obj in mylist]\n\n```\n\n\nTo just run the method or function without saving the return values,\na plain [`for`](../reference/compound_stmts.html#for) loop will suffice:\n\n\n\n```\nfor obj in mylist:\n    obj.method()\n\nfor obj in mylist:\n    function(obj)\n\n```\n\n\n"}
{"instruction": "Why does a_tuple[i] += [\u2018item\u2019] raise an exception when the addition works?\u00b6", "context": "python programming", "output": "\nThis is because of a combination of the fact that augmented assignment\noperators are *assignment* operators, and the difference between mutable and\nimmutable objects in Python.\n\n\nThis discussion applies in general when augmented assignment operators are\napplied to elements of a tuple that point to mutable objects, but we\u2019ll use\na `list` and `+=` as our exemplar.\n\n\nIf you wrote:\n\n\n\n```\n>>> a\\_tuple = (1, 2)\n>>> a\\_tuple[0] += 1\nTraceback (most recent call last):\n   ...\nTypeError: 'tuple' object does not support item assignment\n\n```\n\n\nThe reason for the exception should be immediately clear: `1` is added to the\nobject `a\\_tuple[0]` points to (`1`), producing the result object, `2`,\nbut when we attempt to assign the result of the computation, `2`, to element\n`0` of the tuple, we get an error because we can\u2019t change what an element of\na tuple points to.\n\n\nUnder the covers, what this augmented assignment statement is doing is\napproximately this:\n\n\n\n```\n>>> result = a\\_tuple[0] + 1\n>>> a\\_tuple[0] = result\nTraceback (most recent call last):\n  ...\nTypeError: 'tuple' object does not support item assignment\n\n```\n\n\nIt is the assignment part of the operation that produces the error, since a\ntuple is immutable.\n\n\nWhen you write something like:\n\n\n\n```\n>>> a\\_tuple = (['foo'], 'bar')\n>>> a\\_tuple[0] += ['item']\nTraceback (most recent call last):\n  ...\nTypeError: 'tuple' object does not support item assignment\n\n```\n\n\nThe exception is a bit more surprising, and even more surprising is the fact\nthat even though there was an error, the append worked:\n\n\n\n```\n>>> a\\_tuple[0]\n['foo', 'item']\n\n```\n\n\nTo see why this happens, you need to know that (a) if an object implements an\n[`\\_\\_iadd\\_\\_()`](../reference/datamodel.html#object.__iadd__ \"object.__iadd__\") magic method, it gets called when the `+=` augmented\nassignment\nis executed, and its return value is what gets used in the assignment statement;\nand (b) for lists, `\\_\\_iadd\\_\\_()` is equivalent to calling `extend()` on the list\nand returning the list. That\u2019s why we say that for lists, `+=` is a\n\u201cshorthand\u201d for `list.extend()`:\n\n\n\n```\n>>> a\\_list = []\n>>> a\\_list += [1]\n>>> a\\_list\n[1]\n\n```\n\n\nThis is equivalent to:\n\n\n\n```\n>>> result = a\\_list.\\_\\_iadd\\_\\_([1])\n>>> a\\_list = result\n\n```\n\n\nThe object pointed to by a\\_list has been mutated, and the pointer to the\nmutated object is assigned back to `a\\_list`. The end result of the\nassignment is a no-op, since it is a pointer to the same object that `a\\_list`\nwas previously pointing to, but the assignment still happens.\n\n\nThus, in our tuple example what is happening is equivalent to:\n\n\n\n```\n>>> result = a\\_tuple[0].\\_\\_iadd\\_\\_(['item'])\n>>> a\\_tuple[0] = result\nTraceback (most recent call last):\n  ...\nTypeError: 'tuple' object does not support item assignment\n\n```\n\n\nThe `\\_\\_iadd\\_\\_()` succeeds, and thus the list is extended, but even though\n`result` points to the same object that `a\\_tuple[0]` already points to,\nthat final assignment still results in an error, because tuples are immutable.\n\n\n"}
{"instruction": "I want to do a complicated sort: can you do a Schwartzian Transform in Python?\u00b6", "context": "python programming", "output": "\nThe technique, attributed to Randal Schwartz of the Perl community, sorts the\nelements of a list by a metric which maps each element to its \u201csort value\u201d. In\nPython, use the `key` argument for the [`list.sort()`](../library/stdtypes.html#list.sort \"list.sort\") method:\n\n\n\n```\nIsorted = L[:]\nIsorted.sort(key=lambda s: int(s[10:15]))\n\n```\n\n\n"}
{"instruction": "How can I sort one list by values from another list?\u00b6", "context": "python programming", "output": "\nMerge them into an iterator of tuples, sort the resulting list, and then pick\nout the element you want.\n\n\n\n```\n>>> list1 = [\"what\", \"I'm\", \"sorting\", \"by\"]\n>>> list2 = [\"something\", \"else\", \"to\", \"sort\"]\n>>> pairs = zip(list1, list2)\n>>> pairs = sorted(pairs)\n>>> pairs\n[(\"I'm\", 'else'), ('by', 'sort'), ('sorting', 'to'), ('what', 'something')]\n>>> result = [x[1] for x in pairs]\n>>> result\n['else', 'sort', 'to', 'something']\n\n```\n\n\n"}
{"instruction": "What is a class?\u00b6", "context": "python programming", "output": "\nA class is the particular object type created by executing a class statement.\nClass objects are used as templates to create instance objects, which embody\nboth the data (attributes) and code (methods) specific to a datatype.\n\n\nA class can be based on one or more other classes, called its base class(es). It\nthen inherits the attributes and methods of its base classes. This allows an\nobject model to be successively refined by inheritance. You might have a\ngeneric `Mailbox` class that provides basic accessor methods for a mailbox,\nand subclasses such as `MboxMailbox`, `MaildirMailbox`, `OutlookMailbox`\nthat handle various specific mailbox formats.\n\n\n"}
{"instruction": "What is a method?\u00b6", "context": "python programming", "output": "\nA method is a function on some object `x` that you normally call as\n`x.name(arguments...)`. Methods are defined as functions inside the class\ndefinition:\n\n\n\n```\nclass C:\n    def meth(self, arg):\n        return arg \\* 2 + self.attribute\n\n```\n\n\n"}
{"instruction": "What is self?\u00b6", "context": "python programming", "output": "\nSelf is merely a conventional name for the first argument of a method. A method\ndefined as `meth(self, a, b, c)` should be called as `x.meth(a, b, c)` for\nsome instance `x` of the class in which the definition occurs; the called\nmethod will think it is called as `meth(x, a, b, c)`.\n\n\nSee also [Why must \u2018self\u2019 be used explicitly in method definitions and calls?](design.html#why-self).\n\n\n"}
{"instruction": "How do I check if an object is an instance of a given class or of a subclass of it?\u00b6", "context": "python programming", "output": "\nUse the built-in function [`isinstance(obj, cls)`](../library/functions.html#isinstance \"isinstance\"). You can\ncheck if an object\nis an instance of any of a number of classes by providing a tuple instead of a\nsingle class, e.g. `isinstance(obj, (class1, class2, ...))`, and can also\ncheck whether an object is one of Python\u2019s built-in types, e.g.\n`isinstance(obj, str)` or `isinstance(obj, (int, float, complex))`.\n\n\nNote that [`isinstance()`](../library/functions.html#isinstance \"isinstance\") also checks for virtual inheritance from an\n[abstract base class](../glossary.html#term-abstract-base-class). So, the test will return `True` for a\nregistered class even if hasn\u2019t directly or indirectly inherited from it. To\ntest for \u201ctrue inheritance\u201d, scan the [MRO](../glossary.html#term-MRO) of the class:\n\n\n\n```\nfrom collections.abc import Mapping\n\nclass P:\n     pass\n\nclass C(P):\n    pass\n\nMapping.register(P)\n\n```\n\n\n\n```\n>>> c = C()\n>>> isinstance(c, C)        # direct\nTrue\n>>> isinstance(c, P)        # indirect\nTrue\n>>> isinstance(c, Mapping)  # virtual\nTrue\n\n# Actual inheritance chain\n>>> type(c).\\_\\_mro\\_\\_\n(<class 'C'>, <class 'P'>, <class 'object'>)\n\n# Test for \"true inheritance\"\n>>> Mapping in type(c).\\_\\_mro\\_\\_\nFalse\n\n```\n\n\nNote that most programs do not use [`isinstance()`](../library/functions.html#isinstance \"isinstance\") on user-defined classes\nvery often. If you are developing the classes yourself, a more proper\nobject-oriented style is to define methods on the classes that encapsulate a\nparticular behaviour, instead of checking the object\u2019s class and doing a\ndifferent thing based on what class it is. For example, if you have a function\nthat does something:\n\n\n\n```\ndef search(obj):\n    if isinstance(obj, Mailbox):\n        ...  # code to search a mailbox\n    elif isinstance(obj, Document):\n        ...  # code to search a document\n    elif ...\n\n```\n\n\nA better approach is to define a `search()` method on all the classes and just\ncall it:\n\n\n\n```\nclass Mailbox:\n    def search(self):\n        ...  # code to search a mailbox\n\nclass Document:\n    def search(self):\n        ...  # code to search a document\n\nobj.search()\n\n```\n\n\n"}
{"instruction": "What is delegation?\u00b6", "context": "python programming", "output": "\nDelegation is an object oriented technique (also called a design pattern).\nLet\u2019s say you have an object `x` and want to change the behaviour of just one\nof its methods. You can create a new class that provides a new implementation\nof the method you\u2019re interested in changing and delegates all other methods to\nthe corresponding method of `x`.\n\n\nPython programmers can easily implement delegation. For example, the following\nclass implements a class that behaves like a file but converts all written data\nto uppercase:\n\n\n\n```\nclass UpperOut:\n\n    def \\_\\_init\\_\\_(self, outfile):\n        self.\\_outfile = outfile\n\n    def write(self, s):\n        self.\\_outfile.write(s.upper())\n\n    def \\_\\_getattr\\_\\_(self, name):\n        return getattr(self.\\_outfile, name)\n\n```\n\n\nHere the `UpperOut` class redefines the `write()` method to convert the\nargument string to uppercase before calling the underlying\n`self.\\_outfile.write()` method. All other methods are delegated to the\nunderlying `self.\\_outfile` object. The delegation is accomplished via the\n[`\\_\\_getattr\\_\\_()`](../reference/datamodel.html#object.__getattr__ \"object.__getattr__\") method; consult [the language reference](../reference/datamodel.html#attribute-access)\nfor more information about controlling attribute access.\n\n\nNote that for more general cases delegation can get trickier. When attributes\nmust be set as well as retrieved, the class must define a [`\\_\\_setattr\\_\\_()`](../reference/datamodel.html#object.__setattr__ \"object.__setattr__\")\nmethod too, and it must do so carefully. The basic implementation of\n`\\_\\_setattr\\_\\_()` is roughly equivalent to the following:\n\n\n\n```\nclass X:\n    ...\n    def \\_\\_setattr\\_\\_(self, name, value):\n        self.\\_\\_dict\\_\\_[name] = value\n    ...\n\n```\n\n\nMost `\\_\\_setattr\\_\\_()` implementations must modify\n[`self.\\_\\_dict\\_\\_`](../library/stdtypes.html#object.__dict__ \"object.__dict__\") to store\nlocal state for self without causing an infinite recursion.\n\n\n"}
{"instruction": "How do I call a method defined in a base class from a derived class that extends it?\u00b6", "context": "python programming", "output": "\nUse the built-in [`super()`](../library/functions.html#super \"super\") function:\n\n\n\n```\nclass Derived(Base):\n    def meth(self):\n        super().meth()  # calls Base.meth\n\n```\n\n\nIn the example, [`super()`](../library/functions.html#super \"super\") will automatically determine the instance from\nwhich it was called (the `self` value), look up the [method resolution\norder](../glossary.html#term-method-resolution-order) (MRO) with `type(self).\\_\\_mro\\_\\_`, and return the next in line after\n`Derived` in the MRO: `Base`.\n\n\n"}
{"instruction": "How can I organize my code to make it easier to change the base class?\u00b6", "context": "python programming", "output": "\nYou could assign the base class to an alias and derive from the alias. Then all\nyou have to change is the value assigned to the alias. Incidentally, this trick\nis also handy if you want to decide dynamically (e.g. depending on availability\nof resources) which base class to use. Example:\n\n\n\n```\nclass Base:\n    ...\n\nBaseAlias = Base\n\nclass Derived(BaseAlias):\n    ...\n\n```\n\n\n"}
{"instruction": "How do I create static class data and static class methods?\u00b6", "context": "python programming", "output": "\nBoth static data and static methods (in the sense of C++ or Java) are supported\nin Python.\n\n\nFor static data, simply define a class attribute. To assign a new value to the\nattribute, you have to explicitly use the class name in the assignment:\n\n\n\n```\nclass C:\n    count = 0   # number of times C.\\_\\_init\\_\\_ called\n\n    def \\_\\_init\\_\\_(self):\n        C.count = C.count + 1\n\n    def getcount(self):\n        return C.count  # or return self.count\n\n```\n\n\n`c.count` also refers to `C.count` for any `c` such that `isinstance(c,\nC)` holds, unless overridden by `c` itself or by some class on the base-class\nsearch path from `c.\\_\\_class\\_\\_` back to `C`.\n\n\nCaution: within a method of C, an assignment like `self.count = 42` creates a\nnew and unrelated instance named \u201ccount\u201d in `self`\u2019s own dict. Rebinding of a\nclass-static data name must always specify the class whether inside a method or\nnot:\n\n\n\n```\nC.count = 314\n\n```\n\n\nStatic methods are possible:\n\n\n\n```\nclass C:\n    @staticmethod\n    def static(arg1, arg2, arg3):\n        # No 'self' parameter!\n        ...\n\n```\n\n\nHowever, a far more straightforward way to get the effect of a static method is\nvia a simple module-level function:\n\n\n\n```\ndef getcount():\n    return C.count\n\n```\n\n\nIf your code is structured so as to define one class (or tightly related class\nhierarchy) per module, this supplies the desired encapsulation.\n\n\n"}
{"instruction": "How can I overload constructors (or methods) in Python?\u00b6", "context": "python programming", "output": "\nThis answer actually applies to all methods, but the question usually comes up\nfirst in the context of constructors.\n\n\nIn C++ you\u2019d write\n\n\n\n```\nclass C {\n C() { cout << \"No arguments\\n\"; }\n C(int i) { cout << \"Argument is \" << i << \"\\n\"; }\n}\n\n```\n\n\nIn Python you have to write a single constructor that catches all cases using\ndefault arguments. For example:\n\n\n\n```\nclass C:\n    def \\_\\_init\\_\\_(self, i=None):\n        if i is None:\n            print(\"No arguments\")\n        else:\n            print(\"Argument is\", i)\n\n```\n\n\nThis is not entirely equivalent, but close enough in practice.\n\n\nYou could also try a variable-length argument list, e.g.\n\n\n\n```\ndef \\_\\_init\\_\\_(self, \\*args):\n    ...\n\n```\n\n\nThe same approach works for all method definitions.\n\n\n"}
{"instruction": "I try to use __spam and I get an error about _SomeClassName__spam.\u00b6", "context": "python programming", "output": "\nVariable names with double leading underscores are \u201cmangled\u201d to provide a simple\nbut effective way to define class private variables. Any identifier of the form\n`\\_\\_spam` (at least two leading underscores, at most one trailing underscore)\nis textually replaced with `\\_classname\\_\\_spam`, where `classname` is the\ncurrent class name with any leading underscores stripped.\n\n\nThis doesn\u2019t guarantee privacy: an outside user can still deliberately access\nthe \u201c\\_classname\\_\\_spam\u201d attribute, and private values are visible in the object\u2019s\n`\\_\\_dict\\_\\_`. Many Python programmers never bother to use private variable\nnames at all.\n\n\n"}
{"instruction": "My class defines __del__ but it is not called when I delete the object.\u00b6", "context": "python programming", "output": "\nThere are several possible reasons for this.\n\n\nThe [`del`](../reference/simple_stmts.html#del) statement does not necessarily call [`\\_\\_del\\_\\_()`](../reference/datamodel.html#object.__del__ \"object.__del__\") \u2013 it simply\ndecrements the object\u2019s reference count, and if this reaches zero\n`\\_\\_del\\_\\_()` is called.\n\n\nIf your data structures contain circular links (e.g. a tree where each child has\na parent reference and each parent has a list of children) the reference counts\nwill never go back to zero. Once in a while Python runs an algorithm to detect\nsuch cycles, but the garbage collector might run some time after the last\nreference to your data structure vanishes, so your `\\_\\_del\\_\\_()` method may be\ncalled at an inconvenient and random time. This is inconvenient if you\u2019re trying\nto reproduce a problem. Worse, the order in which object\u2019s `\\_\\_del\\_\\_()`\nmethods are executed is arbitrary. You can run [`gc.collect()`](../library/gc.html#gc.collect \"gc.collect\") to force a\ncollection, but there *are* pathological cases where objects will never be\ncollected.\n\n\nDespite the cycle collector, it\u2019s still a good idea to define an explicit\n`close()` method on objects to be called whenever you\u2019re done with them. The\n`close()` method can then remove attributes that refer to subobjects. Don\u2019t\ncall `\\_\\_del\\_\\_()` directly \u2013 `\\_\\_del\\_\\_()` should call `close()` and\n`close()` should make sure that it can be called more than once for the same\nobject.\n\n\nAnother way to avoid cyclical references is to use the [`weakref`](../library/weakref.html#module-weakref \"weakref: Support for weak references and weak dictionaries.\") module,\nwhich allows you to point to objects without incrementing their reference count.\nTree data structures, for instance, should use weak references for their parent\nand sibling references (if they need them!).\n\n\nFinally, if your `\\_\\_del\\_\\_()` method raises an exception, a warning message\nis printed to [`sys.stderr`](../library/sys.html#sys.stderr \"sys.stderr\").\n\n\n"}
{"instruction": "How do I get a list of all instances of a given class?\u00b6", "context": "python programming", "output": "\nPython does not keep track of all instances of a class (or of a built-in type).\nYou can program the class\u2019s constructor to keep track of all instances by\nkeeping a list of weak references to each instance.\n\n\n"}
{"instruction": "Why does the result of id() appear to be not unique?\u00b6", "context": "python programming", "output": "\nThe [`id()`](../library/functions.html#id \"id\") builtin returns an integer that is guaranteed to be unique during\nthe lifetime of the object. Since in CPython, this is the object\u2019s memory\naddress, it happens frequently that after an object is deleted from memory, the\nnext freshly created object is allocated at the same position in memory. This\nis illustrated by this example:\n\n\n\n```\n>>> id(1000) \n13901272\n>>> id(2000) \n13901272\n\n```\n\n\nThe two ids belong to different integer objects that are created before, and\ndeleted immediately after execution of the `id()` call. To be sure that\nobjects whose id you want to examine are still alive, create another reference\nto the object:\n\n\n\n```\n>>> a = 1000; b = 2000\n>>> id(a) \n13901272\n>>> id(b) \n13891296\n\n```\n\n\n"}
{"instruction": "When can I rely on identity tests with the is operator?\u00b6", "context": "python programming", "output": "\nThe `is` operator tests for object identity. The test `a is b` is\nequivalent to `id(a) == id(b)`.\n\n\nThe most important property of an identity test is that an object is always\nidentical to itself, `a is a` always returns `True`. Identity tests are\nusually faster than equality tests. And unlike equality tests, identity tests\nare guaranteed to return a boolean `True` or `False`.\n\n\nHowever, identity tests can *only* be substituted for equality tests when\nobject identity is assured. Generally, there are three circumstances where\nidentity is guaranteed:\n\n\n1) Assignments create new names but do not change object identity. After the\nassignment `new = old`, it is guaranteed that `new is old`.\n\n\n2) Putting an object in a container that stores object references does not\nchange object identity. After the list assignment `s[0] = x`, it is\nguaranteed that `s[0] is x`.\n\n\n3) If an object is a singleton, it means that only one instance of that object\ncan exist. After the assignments `a = None` and `b = None`, it is\nguaranteed that `a is b` because `None` is a singleton.\n\n\nIn most other circumstances, identity tests are inadvisable and equality tests\nare preferred. In particular, identity tests should not be used to check\nconstants such as [`int`](../library/functions.html#int \"int\") and [`str`](../library/stdtypes.html#str \"str\") which aren\u2019t guaranteed to be\nsingletons:\n\n\n\n```\n>>> a = 1000\n>>> b = 500\n>>> c = b + 500\n>>> a is c\nFalse\n\n>>> a = 'Python'\n>>> b = 'Py'\n>>> c = b + 'thon'\n>>> a is c\nFalse\n\n```\n\n\nLikewise, new instances of mutable containers are never identical:\n\n\n\n```\n>>> a = []\n>>> b = []\n>>> a is b\nFalse\n\n```\n\n\nIn the standard library code, you will see several common patterns for\ncorrectly using identity tests:\n\n\n1) As recommended by [**PEP 8**](https://peps.python.org/pep-0008/), an identity test is the preferred way to check\nfor `None`. This reads like plain English in code and avoids confusion with\nother objects that may have boolean values that evaluate to false.\n\n\n2) Detecting optional arguments can be tricky when `None` is a valid input\nvalue. In those situations, you can create a singleton sentinel object\nguaranteed to be distinct from other objects. For example, here is how\nto implement a method that behaves like [`dict.pop()`](../library/stdtypes.html#dict.pop \"dict.pop\"):\n\n\n\n```\n\\_sentinel = object()\n\ndef pop(self, key, default=\\_sentinel):\n    if key in self:\n        value = self[key]\n        del self[key]\n        return value\n    if default is \\_sentinel:\n        raise KeyError(key)\n    return default\n\n```\n\n\n3) Container implementations sometimes need to augment equality tests with\nidentity tests. This prevents the code from being confused by objects such as\n`float('NaN')` that are not equal to themselves.\n\n\nFor example, here is the implementation of\n`collections.abc.Sequence.\\_\\_contains\\_\\_()`:\n\n\n\n```\ndef \\_\\_contains\\_\\_(self, value):\n    for v in self:\n        if v is value or v == value:\n            return True\n    return False\n\n```\n\n\n"}
{"instruction": "How can a subclass control what data is stored in an immutable instance?\u00b6", "context": "python programming", "output": "\nWhen subclassing an immutable type, override the [`\\_\\_new\\_\\_()`](../reference/datamodel.html#object.__new__ \"object.__new__\") method\ninstead of the [`\\_\\_init\\_\\_()`](../reference/datamodel.html#object.__init__ \"object.__init__\") method. The latter only runs *after* an\ninstance is created, which is too late to alter data in an immutable\ninstance.\n\n\nAll of these immutable classes have a different signature than their\nparent class:\n\n\n\n```\nfrom datetime import date\n\nclass FirstOfMonthDate(date):\n    \"Always choose the first day of the month\"\n    def \\_\\_new\\_\\_(cls, year, month, day):\n        return super().\\_\\_new\\_\\_(cls, year, month, 1)\n\nclass NamedInt(int):\n    \"Allow text names for some numbers\"\n    xlat = {'zero': 0, 'one': 1, 'ten': 10}\n    def \\_\\_new\\_\\_(cls, value):\n        value = cls.xlat.get(value, value)\n        return super().\\_\\_new\\_\\_(cls, value)\n\nclass TitleStr(str):\n    \"Convert str to name suitable for a URL path\"\n    def \\_\\_new\\_\\_(cls, s):\n        s = s.lower().replace(' ', '-')\n        s = ''.join([c for c in s if c.isalnum() or c == '-'])\n        return super().\\_\\_new\\_\\_(cls, s)\n\n```\n\n\nThe classes can be used like this:\n\n\n\n```\n>>> FirstOfMonthDate(2012, 2, 14)\nFirstOfMonthDate(2012, 2, 1)\n>>> NamedInt('ten')\n10\n>>> NamedInt(20)\n20\n>>> TitleStr('Blog: Why Python Rocks')\n'blog-why-python-rocks'\n\n```\n\n\n"}
{"instruction": "How do I cache method calls?\u00b6", "context": "python programming", "output": "\nThe two principal tools for caching methods are\n[`functools.cached\\_property()`](../library/functools.html#functools.cached_property \"functools.cached_property\") and [`functools.lru\\_cache()`](../library/functools.html#functools.lru_cache \"functools.lru_cache\"). The\nformer stores results at the instance level and the latter at the class\nlevel.\n\n\nThe *cached\\_property* approach only works with methods that do not take\nany arguments. It does not create a reference to the instance. The\ncached method result will be kept only as long as the instance is alive.\n\n\nThe advantage is that when an instance is no longer used, the cached\nmethod result will be released right away. The disadvantage is that if\ninstances accumulate, so too will the accumulated method results. They\ncan grow without bound.\n\n\nThe *lru\\_cache* approach works with methods that have [hashable](../glossary.html#term-hashable)\narguments. It creates a reference to the instance unless special\nefforts are made to pass in weak references.\n\n\nThe advantage of the least recently used algorithm is that the cache is\nbounded by the specified *maxsize*. The disadvantage is that instances\nare kept alive until they age out of the cache or until the cache is\ncleared.\n\n\nThis example shows the various techniques:\n\n\n\n```\nclass Weather:\n    \"Lookup weather information on a government website\"\n\n    def \\_\\_init\\_\\_(self, station\\_id):\n        self.\\_station\\_id = station\\_id\n        # The \\_station\\_id is private and immutable\n\n    def current\\_temperature(self):\n        \"Latest hourly observation\"\n        # Do not cache this because old results\n        # can be out of date.\n\n    @cached\\_property\n    def location(self):\n        \"Return the longitude/latitude coordinates of the station\"\n        # Result only depends on the station\\_id\n\n    @lru\\_cache(maxsize=20)\n    def historic\\_rainfall(self, date, units='mm'):\n        \"Rainfall on a given date\"\n        # Depends on the station\\_id, date, and units.\n\n```\n\n\nThe above example assumes that the *station\\_id* never changes. If the\nrelevant instance attributes are mutable, the *cached\\_property* approach\ncan\u2019t be made to work because it cannot detect changes to the\nattributes.\n\n\nTo make the *lru\\_cache* approach work when the *station\\_id* is mutable,\nthe class needs to define the [`\\_\\_eq\\_\\_()`](../reference/datamodel.html#object.__eq__ \"object.__eq__\") and [`\\_\\_hash\\_\\_()`](../reference/datamodel.html#object.__hash__ \"object.__hash__\")\nmethods so that the cache can detect relevant attribute updates:\n\n\n\n```\nclass Weather:\n    \"Example with a mutable station identifier\"\n\n    def \\_\\_init\\_\\_(self, station\\_id):\n        self.station\\_id = station\\_id\n\n    def change\\_station(self, station\\_id):\n        self.station\\_id = station\\_id\n\n    def \\_\\_eq\\_\\_(self, other):\n        return self.station\\_id == other.station\\_id\n\n    def \\_\\_hash\\_\\_(self):\n        return hash(self.station\\_id)\n\n    @lru\\_cache(maxsize=20)\n    def historic\\_rainfall(self, date, units='cm'):\n        'Rainfall on a given date'\n        # Depends on the station\\_id, date, and units.\n\n```\n\n\n"}
{"instruction": "How do I create a .pyc file?\u00b6", "context": "python programming", "output": "\nWhen a module is imported for the first time (or when the source file has\nchanged since the current compiled file was created) a `.pyc` file containing\nthe compiled code should be created in a `\\_\\_pycache\\_\\_` subdirectory of the\ndirectory containing the `.py` file. The `.pyc` file will have a\nfilename that starts with the same name as the `.py` file, and ends with\n`.pyc`, with a middle component that depends on the particular `python`\nbinary that created it. (See [**PEP 3147**](https://peps.python.org/pep-3147/) for details.)\n\n\nOne reason that a `.pyc` file may not be created is a permissions problem\nwith the directory containing the source file, meaning that the `\\_\\_pycache\\_\\_`\nsubdirectory cannot be created. This can happen, for example, if you develop as\none user but run as another, such as if you are testing with a web server.\n\n\nUnless the [`PYTHONDONTWRITEBYTECODE`](../using/cmdline.html#envvar-PYTHONDONTWRITEBYTECODE) environment variable is set,\ncreation of a .pyc file is automatic if you\u2019re importing a module and Python\nhas the ability (permissions, free space, etc\u2026) to create a `\\_\\_pycache\\_\\_`\nsubdirectory and write the compiled module to that subdirectory.\n\n\nRunning Python on a top level script is not considered an import and no\n`.pyc` will be created. For example, if you have a top-level module\n`foo.py` that imports another module `xyz.py`, when you run `foo` (by\ntyping `python foo.py` as a shell command), a `.pyc` will be created for\n`xyz` because `xyz` is imported, but no `.pyc` file will be created for\n`foo` since `foo.py` isn\u2019t being imported.\n\n\nIf you need to create a `.pyc` file for `foo` \u2013 that is, to create a\n`.pyc` file for a module that is not imported \u2013 you can, using the\n[`py\\_compile`](../library/py_compile.html#module-py_compile \"py_compile: Generate byte-code files from Python source files.\") and [`compileall`](../library/compileall.html#module-compileall \"compileall: Tools for byte-compiling all Python source files in a directory tree.\") modules.\n\n\nThe [`py\\_compile`](../library/py_compile.html#module-py_compile \"py_compile: Generate byte-code files from Python source files.\") module can manually compile any module. One way is to use\nthe `compile()` function in that module interactively:\n\n\n\n```\n>>> import py\\_compile\n>>> py\\_compile.compile('foo.py')                 \n\n```\n\n\nThis will write the `.pyc` to a `\\_\\_pycache\\_\\_` subdirectory in the same\nlocation as `foo.py` (or you can override that with the optional parameter\n`cfile`).\n\n\nYou can also automatically compile all files in a directory or directories using\nthe [`compileall`](../library/compileall.html#module-compileall \"compileall: Tools for byte-compiling all Python source files in a directory tree.\") module. You can do it from the shell prompt by running\n`compileall.py` and providing the path of a directory containing Python files\nto compile:\n\n\n\n```\npython -m compileall .\n\n```\n\n\n"}
{"instruction": "How do I find the current module name?\u00b6", "context": "python programming", "output": "\nA module can find out its own module name by looking at the predefined global\nvariable `\\_\\_name\\_\\_`. If this has the value `'\\_\\_main\\_\\_'`, the program is\nrunning as a script. Many modules that are usually used by importing them also\nprovide a command-line interface or a self-test, and only execute this code\nafter checking `\\_\\_name\\_\\_`:\n\n\n\n```\ndef main():\n    print('Running test...')\n    ...\n\nif \\_\\_name\\_\\_ == '\\_\\_main\\_\\_':\n    main()\n\n```\n\n\n"}
{"instruction": "How can I have modules that mutually import each other?\u00b6", "context": "python programming", "output": "\nSuppose you have the following modules:\n\n\n`foo.py`:\n\n\n\n```\nfrom bar import bar\\_var\nfoo\\_var = 1\n\n```\n\n\n`bar.py`:\n\n\n\n```\nfrom foo import foo\\_var\nbar\\_var = 2\n\n```\n\n\nThe problem is that the interpreter will perform the following steps:\n\n\n* main imports `foo`\n* Empty globals for `foo` are created\n* `foo` is compiled and starts executing\n* `foo` imports `bar`\n* Empty globals for `bar` are created\n* `bar` is compiled and starts executing\n* `bar` imports `foo` (which is a no-op since there already is a module named `foo`)\n* The import mechanism tries to read `foo\\_var` from `foo` globals, to set `bar.foo\\_var = foo.foo\\_var`\n\n\nThe last step fails, because Python isn\u2019t done with interpreting `foo` yet and\nthe global symbol dictionary for `foo` is still empty.\n\n\nThe same thing happens when you use `import foo`, and then try to access\n`foo.foo\\_var` in global code.\n\n\nThere are (at least) three possible workarounds for this problem.\n\n\nGuido van Rossum recommends avoiding all uses of `from <module> import ...`,\nand placing all code inside functions. Initializations of global variables and\nclass variables should use constants or built-in functions only. This means\neverything from an imported module is referenced as `<module>.<name>`.\n\n\nJim Roskind suggests performing steps in the following order in each module:\n\n\n* exports (globals, functions, and classes that don\u2019t need imported base\nclasses)\n* `import` statements\n* active code (including globals that are initialized from imported values).\n\n\nVan Rossum doesn\u2019t like this approach much because the imports appear in a\nstrange place, but it does work.\n\n\nMatthias Urlichs recommends restructuring your code so that the recursive import\nis not necessary in the first place.\n\n\nThese solutions are not mutually exclusive.\n\n\n"}
{"instruction": "__import__(\u2018x.y.z\u2019) returns <module \u2018x\u2019>; how do I get z?\u00b6", "context": "python programming", "output": "\nConsider using the convenience function [`import\\_module()`](../library/importlib.html#importlib.import_module \"importlib.import_module\") from\n[`importlib`](../library/importlib.html#module-importlib \"importlib: The implementation of the import machinery.\") instead:\n\n\n\n```\nz = importlib.import\\_module('x.y.z')\n\n```\n\n\n"}
{"instruction": "When I edit an imported module and reimport it, the changes don\u2019t show up.  Why does this happen?\u00b6", "context": "python programming", "output": "\nFor reasons of efficiency as well as consistency, Python only reads the module\nfile on the first time a module is imported. If it didn\u2019t, in a program\nconsisting of many modules where each one imports the same basic module, the\nbasic module would be parsed and re-parsed many times. To force re-reading of a\nchanged module, do this:\n\n\n\n```\nimport importlib\nimport modname\nimportlib.reload(modname)\n\n```\n\n\nWarning: this technique is not 100% fool-proof. In particular, modules\ncontaining statements like\n\n\n\n```\nfrom modname import some\\_objects\n\n```\n\n\nwill continue to work with the old version of the imported objects. If the\nmodule contains class definitions, existing class instances will *not* be\nupdated to use the new class definition. This can result in the following\nparadoxical behaviour:\n\n\n\n```\n>>> import importlib\n>>> import cls\n>>> c = cls.C()                # Create an instance of C\n>>> importlib.reload(cls)\n<module 'cls' from 'cls.py'>\n>>> isinstance(c, cls.C)       # isinstance is false?!?\nFalse\n\n```\n\n\nThe nature of the problem is made clear if you print out the \u201cidentity\u201d of the\nclass objects:\n\n\n\n```\n>>> hex(id(c.\\_\\_class\\_\\_))\n'0x7352a0'\n>>> hex(id(cls.C))\n'0x4198d0'\n\n```\n\n\n"}
{"instruction": "Can I create my own functions in C?\u00b6", "context": "python extending", "output": "\nYes, you can create built-in modules containing functions, variables, exceptions\nand even new types in C. This is explained in the document\n[Extending and Embedding the Python Interpreter](../extending/index.html#extending-index).\n\n\nMost intermediate or advanced Python books will also cover this topic.\n\n\n"}
{"instruction": "Can I create my own functions in C++?\u00b6", "context": "python extending", "output": "\nYes, using the C compatibility features found in C++. Place `extern \"C\" {\n... }` around the Python include files and put `extern \"C\"` before each\nfunction that is going to be called by the Python interpreter. Global or static\nC++ objects with constructors are probably not a good idea.\n\n\n"}
{"instruction": "Writing C is hard; are there any alternatives?\u00b6", "context": "python extending", "output": "\nThere are a number of alternatives to writing your own C extensions, depending\non what you\u2019re trying to do.\n\n\n[Cython](https://cython.org) and its relative [Pyrex](https://www.csse.canterbury.ac.nz/greg.ewing/python/Pyrex/) are compilers\nthat accept a slightly modified form of Python and generate the corresponding\nC code. Cython and Pyrex make it possible to write an extension without having\nto learn Python\u2019s C API.\n\n\nIf you need to interface to some C or C++ library for which no Python extension\ncurrently exists, you can try wrapping the library\u2019s data types and functions\nwith a tool such as [SWIG](https://www.swig.org). [SIP](https://riverbankcomputing.com/software/sip/intro), [CXX](https://cxx.sourceforge.net/) [Boost](https://www.boost.org/libs/python/doc/index.html), or [Weave](https://github.com/scipy/weave) are also\nalternatives for wrapping C++ libraries.\n\n\n"}
{"instruction": "How can I execute arbitrary Python statements from C?\u00b6", "context": "python extending", "output": "\nThe highest-level function to do this is [`PyRun\\_SimpleString()`](../c-api/veryhigh.html#c.PyRun_SimpleString \"PyRun_SimpleString\") which takes\na single string argument to be executed in the context of the module\n`\\_\\_main\\_\\_` and returns `0` for success and `-1` when an exception occurred\n(including [`SyntaxError`](../library/exceptions.html#SyntaxError \"SyntaxError\")). If you want more control, use\n[`PyRun\\_String()`](../c-api/veryhigh.html#c.PyRun_String \"PyRun_String\"); see the source for [`PyRun\\_SimpleString()`](../c-api/veryhigh.html#c.PyRun_SimpleString \"PyRun_SimpleString\") in\n`Python/pythonrun.c`.\n\n\n"}
{"instruction": "How can I evaluate an arbitrary Python expression from C?\u00b6", "context": "python extending", "output": "\nCall the function [`PyRun\\_String()`](../c-api/veryhigh.html#c.PyRun_String \"PyRun_String\") from the previous question with the\nstart symbol [`Py\\_eval\\_input`](../c-api/veryhigh.html#c.Py_eval_input \"Py_eval_input\"); it parses an expression, evaluates it and\nreturns its value.\n\n\n"}
{"instruction": "How do I extract C values from a Python object?\u00b6", "context": "python extending", "output": "\nThat depends on the object\u2019s type. If it\u2019s a tuple, [`PyTuple\\_Size()`](../c-api/tuple.html#c.PyTuple_Size \"PyTuple_Size\")\nreturns its length and [`PyTuple\\_GetItem()`](../c-api/tuple.html#c.PyTuple_GetItem \"PyTuple_GetItem\") returns the item at a specified\nindex. Lists have similar functions, `PyListSize()` and\n[`PyList\\_GetItem()`](../c-api/list.html#c.PyList_GetItem \"PyList_GetItem\").\n\n\nFor bytes, [`PyBytes\\_Size()`](../c-api/bytes.html#c.PyBytes_Size \"PyBytes_Size\") returns its length and\n[`PyBytes\\_AsStringAndSize()`](../c-api/bytes.html#c.PyBytes_AsStringAndSize \"PyBytes_AsStringAndSize\") provides a pointer to its value and its\nlength. Note that Python bytes objects may contain null bytes so C\u2019s\n`strlen()` should not be used.\n\n\nTo test the type of an object, first make sure it isn\u2019t `NULL`, and then use\n[`PyBytes\\_Check()`](../c-api/bytes.html#c.PyBytes_Check \"PyBytes_Check\"), [`PyTuple\\_Check()`](../c-api/tuple.html#c.PyTuple_Check \"PyTuple_Check\"), [`PyList\\_Check()`](../c-api/list.html#c.PyList_Check \"PyList_Check\"), etc.\n\n\nThere is also a high-level API to Python objects which is provided by the\nso-called \u2018abstract\u2019 interface \u2013 read `Include/abstract.h` for further\ndetails. It allows interfacing with any kind of Python sequence using calls\nlike [`PySequence\\_Length()`](../c-api/sequence.html#c.PySequence_Length \"PySequence_Length\"), [`PySequence\\_GetItem()`](../c-api/sequence.html#c.PySequence_GetItem \"PySequence_GetItem\"), etc. as well\nas many other useful protocols such as numbers ([`PyNumber\\_Index()`](../c-api/number.html#c.PyNumber_Index \"PyNumber_Index\") et\nal.) and mappings in the PyMapping APIs.\n\n\n"}
{"instruction": "How do I use Py_BuildValue() to create a tuple of arbitrary length?\u00b6", "context": "python extending", "output": "\nYou can\u2019t. Use [`PyTuple\\_Pack()`](../c-api/tuple.html#c.PyTuple_Pack \"PyTuple_Pack\") instead.\n\n\n"}
{"instruction": "How do I call an object\u2019s method from C?\u00b6", "context": "python extending", "output": "\nThe [`PyObject\\_CallMethod()`](../c-api/call.html#c.PyObject_CallMethod \"PyObject_CallMethod\") function can be used to call an arbitrary\nmethod of an object. The parameters are the object, the name of the method to\ncall, a format string like that used with [`Py\\_BuildValue()`](../c-api/arg.html#c.Py_BuildValue \"Py_BuildValue\"), and the\nargument values:\n\n\n\n```\nPyObject \\*\nPyObject\\_CallMethod(PyObject \\*object, const char \\*method\\_name,\n const char \\*arg\\_format, ...);\n\n```\n\n\nThis works for any object that has methods \u2013 whether built-in or user-defined.\nYou are responsible for eventually [`Py\\_DECREF()`](../c-api/refcounting.html#c.Py_DECREF \"Py_DECREF\")\u2018ing the return value.\n\n\nTo call, e.g., a file object\u2019s \u201cseek\u201d method with arguments 10, 0 (assuming the\nfile object pointer is \u201cf\u201d):\n\n\n\n```\nres = PyObject\\_CallMethod(f, \"seek\", \"(ii)\", 10, 0);\nif (res == NULL) {\n ... an exception occurred ...\n}\nelse {\n Py\\_DECREF(res);\n}\n\n```\n\n\nNote that since [`PyObject\\_CallObject()`](../c-api/call.html#c.PyObject_CallObject \"PyObject_CallObject\") *always* wants a tuple for the\nargument list, to call a function without arguments, pass \u201c()\u201d for the format,\nand to call a function with one argument, surround the argument in parentheses,\ne.g. \u201c(i)\u201d.\n\n\n"}
{"instruction": "How do I catch the output from PyErr_Print() (or anything that prints to stdout/stderr)?\u00b6", "context": "python extending", "output": "\nIn Python code, define an object that supports the `write()` method. Assign\nthis object to [`sys.stdout`](../library/sys.html#sys.stdout \"sys.stdout\") and [`sys.stderr`](../library/sys.html#sys.stderr \"sys.stderr\"). Call print\\_error, or\njust allow the standard traceback mechanism to work. Then, the output will go\nwherever your `write()` method sends it.\n\n\nThe easiest way to do this is to use the [`io.StringIO`](../library/io.html#io.StringIO \"io.StringIO\") class:\n\n\n\n```\n>>> import io, sys\n>>> sys.stdout = io.StringIO()\n>>> print('foo')\n>>> print('hello world!')\n>>> sys.stderr.write(sys.stdout.getvalue())\nfoo\nhello world!\n\n```\n\n\nA custom object to do the same would look like this:\n\n\n\n```\n>>> import io, sys\n>>> class StdoutCatcher(io.TextIOBase):\n...     def \\_\\_init\\_\\_(self):\n...         self.data = []\n...     def write(self, stuff):\n...         self.data.append(stuff)\n...\n>>> import sys\n>>> sys.stdout = StdoutCatcher()\n>>> print('foo')\n>>> print('hello world!')\n>>> sys.stderr.write(''.join(sys.stdout.data))\nfoo\nhello world!\n\n```\n\n\n"}
{"instruction": "How do I access a module written in Python from C?\u00b6", "context": "python extending", "output": "\nYou can get a pointer to the module object as follows:\n\n\n\n```\nmodule = PyImport\\_ImportModule(\"<modulename>\");\n\n```\n\n\nIf the module hasn\u2019t been imported yet (i.e. it is not yet present in\n[`sys.modules`](../library/sys.html#sys.modules \"sys.modules\")), this initializes the module; otherwise it simply returns\nthe value of `sys.modules[\"<modulename>\"]`. Note that it doesn\u2019t enter the\nmodule into any namespace \u2013 it only ensures it has been initialized and is\nstored in [`sys.modules`](../library/sys.html#sys.modules \"sys.modules\").\n\n\nYou can then access the module\u2019s attributes (i.e. any name defined in the\nmodule) as follows:\n\n\n\n```\nattr = PyObject\\_GetAttrString(module, \"<attrname>\");\n\n```\n\n\nCalling [`PyObject\\_SetAttrString()`](../c-api/object.html#c.PyObject_SetAttrString \"PyObject_SetAttrString\") to assign to variables in the module\nalso works.\n\n\n"}
{"instruction": "How do I interface to C++ objects from Python?\u00b6", "context": "python extending", "output": "\nDepending on your requirements, there are many approaches. To do this manually,\nbegin by reading [the \u201cExtending and Embedding\u201d document](../extending/index.html#extending-index). Realize that for the Python run-time system, there isn\u2019t a\nwhole lot of difference between C and C++ \u2013 so the strategy of building a new\nPython type around a C structure (pointer) type will also work for C++ objects.\n\n\nFor C++ libraries, see [Writing C is hard; are there any alternatives?](#c-wrapper-software).\n\n\n"}
{"instruction": "I added a module using the Setup file and the make fails; why?\u00b6", "context": "python extending", "output": "\nSetup must end in a newline, if there is no newline there, the build process\nfails. (Fixing this requires some ugly shell script hackery, and this bug is so\nminor that it doesn\u2019t seem worth the effort.)\n\n\n"}
{"instruction": "How do I debug an extension?\u00b6", "context": "python extending", "output": "\nWhen using GDB with dynamically loaded extensions, you can\u2019t set a breakpoint in\nyour extension until your extension is loaded.\n\n\nIn your `.gdbinit` file (or interactively), add the command:\n\n\n\n```\nbr _PyImport_LoadDynamicModule\n\n```\n\n\nThen, when you run GDB:\n\n\n\n```\n$ gdb /local/bin/python\ngdb) run myscript.py\ngdb) continue # repeat until your extension is loaded\ngdb) finish # so that your extension is loaded\ngdb) br myfunction.c:50\ngdb) continue\n\n```\n\n\n"}
{"instruction": "I want to compile a Python module on my Linux system, but some files are missing. Why?\u00b6", "context": "python extending", "output": "\nMost packaged versions of Python don\u2019t include the\n`/usr/lib/python2.*x*/config/` directory, which contains various files\nrequired for compiling Python extensions.\n\n\nFor Red Hat, install the python-devel RPM to get the necessary files.\n\n\nFor Debian, run `apt-get install python-dev`.\n\n\n"}
{"instruction": "How do I tell \u201cincomplete input\u201d from \u201cinvalid input\u201d?\u00b6", "context": "python extending", "output": "\nSometimes you want to emulate the Python interactive interpreter\u2019s behavior,\nwhere it gives you a continuation prompt when the input is incomplete (e.g. you\ntyped the start of an \u201cif\u201d statement or you didn\u2019t close your parentheses or\ntriple string quotes), but it gives you a syntax error message immediately when\nthe input is invalid.\n\n\nIn Python you can use the [`codeop`](../library/codeop.html#module-codeop \"codeop: Compile (possibly incomplete) Python code.\") module, which approximates the parser\u2019s\nbehavior sufficiently. IDLE uses this, for example.\n\n\nThe easiest way to do it in C is to call [`PyRun\\_InteractiveLoop()`](../c-api/veryhigh.html#c.PyRun_InteractiveLoop \"PyRun_InteractiveLoop\") (perhaps\nin a separate thread) and let the Python interpreter handle the input for\nyou. You can also set the [`PyOS\\_ReadlineFunctionPointer()`](../c-api/veryhigh.html#c.PyOS_ReadlineFunctionPointer \"PyOS_ReadlineFunctionPointer\") to point at your\ncustom input function. See `Modules/readline.c` and `Parser/myreadline.c`\nfor more hints.\n\n\n"}
{"instruction": "How do I find undefined g++ symbols __builtin_new or __pure_virtual?\u00b6", "context": "python extending", "output": "\nTo dynamically load g++ extension modules, you must recompile Python, relink it\nusing g++ (change LINKCC in the Python Modules Makefile), and link your\nextension module using g++ (e.g., `g++ -shared -o mymodule.so mymodule.o`).\n\n\n"}
{"instruction": "Can I create an object class with some methods implemented in C and others in Python (e.g. through inheritance)?\u00b6", "context": "python extending", "output": "\nYes, you can inherit from built-in classes such as [`int`](../library/functions.html#int \"int\"), [`list`](../library/stdtypes.html#list \"list\"),\n[`dict`](../library/stdtypes.html#dict \"dict\"), etc.\n\n\nThe Boost Python Library (BPL, <https://www.boost.org/libs/python/doc/index.html>)\nprovides a way of doing this from C++ (i.e. you can inherit from an extension\nclass written in C++ using the BPL).\n\n\n"}
